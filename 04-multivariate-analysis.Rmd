# Multivariate Analysis

The following section illustrates the different methods in multivariate analyses. These methods are not to be confused with the more simple multivariable analyses.

## Principal component analysis

Principal component analysis (PCA) is a data dimension reduction method which can be applied to a large dataset to determine the latent variables (principal components) which best represent that set of data. A brief description of the method is described here and a more detailed description of the method can be found in review [@pmid10703049]. The usual approach to PCA involves eigen analysis of a covariance matrix or singular value decomposition of a data matrix. 

PCA estimates an orthogonal transformation (variance maximising) to convert a set of observations of correlated variables into a set of values of uncorrelated (orthogonal) variables called principal components. The first extracted principle component aligns in the direction that contains most of the variance of observed variables. The next principal component is orthogonal to the first principle component and contains the second most of spread of variance. The next component contains the third most of spread, and so on.  The latter principal components are likely to represent noise and are discarded. Expressing this in terms of our imaging data, each component yields a linear combination of ‘ischemic’ voxels that covary with each other. These components can be interpreted as patterns of ischemic injury. The unit of measurement in PCA images is the covariance of the data. 

In the case of MR images, each voxel is a variable, leading to tens of thousands of variables with relatively small numbers of samples. Specialised methods are required to compute principle components. 

Based on cosine rule, principle components from different data are similar if values approach 1 and dissimilar if values approach 0 [@pmid22551679].

## Independent component analysis

Independent component analysis is different from PCA in that it seeks components which are statistically independent. 

## Partial least squares

There are several versions of partial least squares (PLS). A detailed mathematical exposition of the PLS-PLR technique used here can be found in the paper by Fort and Lambert-Lacroix [@pmid15531609]. For the purposes of exposition we will describe the individual components of the method. PLS is a multiple regression method that is suited to datasets comprising large sets of independent predictor variables (voxels in an image) and smaller sets of dependent variables (neurological outcome scores). Each voxel can take on a value of 1 (representing involvement by infarction) or 0 (representing absence of involvement) in the MR image of each patient. PLS employs a data reduction method which generates latent variables, linear combinations of independent and dependent variables which explain as much of their covariance as possible. Linear least squares regression of the latent variables produces coefﬁcients or beta weights for the latent variables at each voxel location in the brain in stereotaxic coordinate space.[@pmid19660556]

The colon dataset containing microarray data comes with the _plsgenomics_ library [@pmid28968879]. The analysis involves partitioning the data into training and test set. the classification data is in the Y column.

```{r pls}
library(plsgenomics)
data("Colon")
class(Colon) #list
#62 samples 2000 genes
dim(Colon$X) 
#heatmap
matrix.heatmap(cbind(Colon$X,Colon$y))
#
IndexLearn <- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8))
Xtrain <- Colon$X[IndexLearn,] 
Ytrain <- Colon$Y[IndexLearn] 
Xtest <- Colon$X[-IndexLearn,]
# preprocess data 
resP <- preprocess(Xtrain= Xtrain, Xtest=Xtest,Threshold = c(100,16000),Filtering=c(5,500), log10.scale=TRUE,row.stand=TRUE)
# perform prediction by GSIM 
res <- gsim(Xtrain=resP$pXtrain,Ytrain= Ytrain,Xtest=resP$pXtest,Lambda=10,hA=50,hB=NULL)
res$Cvg 
sum(res$Ytest!=Colon$Y[-IndexLearn])
```

