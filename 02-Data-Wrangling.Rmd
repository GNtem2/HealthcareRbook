# Data Wrangling

This section deals with the nitty gritty of data analysis. There's no nice plots like the previous chapter. In fact, this data wrangling is the major aspect of data science. Sometimes, it's worth spending time on _stack overflow_ to look at similar problems that others have and the approaches to solving them. Useful blogs are available at https://www.r-bloggers.com/.

## Data 

This section on atomic vector, arrays, matrix and list is often considered boring and ignored. 

### Vector, Arrays, Matrix

#### Vector and list

All elements of an atomic vector and arrays are the same. Vector can be numbers, words etc. However, vectors should not be a mixture of words and numbers. List can contain different types of data. A complex example of a structured list is the _json_ format shown below. In base R, _c_ is a function for creating a vector or list. The function _list_ can also be used to create list. It is best to avoid using _c_ when assigning a name to a dataframe or vector [@Wickham2019].

```{r vector}
a<-c(1,2,3)
is.vector(a)
class(a) #numeric vector
b<-c("apple","orange","banana")
is.vector(b) #character vector
class(b)
d<-c(1,2,"banana")
is.list(d) #character vector
class(d) #FALSE
e<-list(a,b,c(TRUE,FALSE,FALSE))
is.list(e) #TRUE
```    

#### Matrix and Arrays

Arrays are arranged in matrix of rows and columns of a single data type. Later in this chapter, we will illustrate the importance of arrays for manipulating MRI scans. A volumetric mri scan, there are 3 dimenions _[,,]_. The first column is the sagittal sequence, second column is the coronal sequence and the third column is the axial sequence. In the example below, this knowledge of arrays can be used to reverse the ordering of information on MRI scan (flip on it axis between left and right)

```{r matrix and arrays}
#vector
vector1<-c(1,2,3)
vector2<-c(4,5,6,7,8,9)
array1<-array(c(vector1,vector2),dim = c(3,3,2))
array1
array2<-array(c(vector1,vector2),dim = c(2,2,3))
array2
#check if array or matrix
is.matrix(array1)
is.array(array1)
```

Arrays can be accessed by indexing its structure.

```{r indexing arrays}
#first row of array1
array1[,,1]
#second row of array1
array1[,,2]
#second row of array2
array2[,,2]
```

The _apply_ function works on data in array format.

```{r array maths}
#sum data across rows ie c(1)
apply(array1,c(1),sum)
#sum data across columns(2)
apply(array1,c(2),sum)
#sum data across rows and columns c(1,2)
apply(array1,c(1,2),sum)
```

#### Data frame

Data frame is a convenient way of formatting data in table format. It is worth checking the structure of data. Some libraries preder to work with data in data frame while others prefer matrix or array structure.

```{r dataframe}
a<-c(1,2,3)
b<-c("apple","orange","banana")
e<-data.frame(a,b)
rownames(e)
```

#### Simple function

a function is written by creating name of function, calling on _function_ (x) and brackets to define function.

```{r simple function}
# function sun
ArithSum<-function (x) {
  sum(x)
}
vector1<-c(1,2,3)
ArithSum(vector1)
```

This is an example of using loop to remove na. There is an elegant function _replace_na_ within _tidyr_ to perform this task.

```{r loop}
# data
x=c(3,2,NA,6,9,NA,NA,5,NA,NA,11)
# vector of missing values
y <- NULL
# find missing value indices
for(i in 1:length(x)){
  if(is.na(x[i])==TRUE)
    y <- append(y, i)
}
# replace missing values with a random integer between 1 and 5
x.no.na <- replace(x, y, sample(5, length(y), replace = FALSE))
x.no.na
#tidyr version
x.no.na2<-tidyr::replace_na(x,round(runif(1,1,5),0))
x.no.na2
```

#### sapply

The call _sapply_ applies a function to a vector, matrix or list. It returns the results in the form of a matrix.

```{r eval=FALSE}
a<-list.files(pattern="100*")
#SopenNii<-sapply(a, readNIfTI )[1] 902629      4
#class(SopenNii) #matrix
```
#### lapply

The call _lapply_ applies a function to a list. The example below creates a list of nifti files which are opened painlessly using _lapply_ call to _readNIfTI_.

```{r eval=FALSE}
library(oro.nifti)
a<-list.files(pattern="100*")
#LopenNii<-lapply(a, readNIfTI)
#class(LopenNii) #list
```

## Data storage

Often one assumes that opening Rstudio is sufficient to locate the file and run the analysis. One way of doing this at the console is to click on _Session_ tab, then _Set Working Directory_ to location of file. Another way of doing this seemlessly is to use the library _here_. It is easy to find the files in your directory using the _list.files()_ call.One way of storing data in R format is to save the file as .Rda. This format will ensure that no one can accidentally rewrite or delete a number. For very large data, it's quicker to save as .Rda file than as csv file.

```{r list files}
#list all files in a directory
#list.files()
#list files matching pattern
list.files(pattern=".Rmd")
```


### Excel data

Excel data are stored as csv, xls and xlsx.  Csv files can be open in base R using _read.csv_ function or _read_csv_ using _readr_ library, or _fread_ using _data.table_. One issue with _read.csv_ is that it converts string to factors and it is advisable to follow up with a statement (stringsAsFactors = FALSE) to avoid this issue. Both _read_csv_ and _fread_ are deemed to be faster for large files. _fread_ convert date to character and requires further conversion using base R or _fasttime_. 

I would urge you to get used to manipulating data in R as the codes serve as a way to keep track with the change in data. The original xcel data should not be touched outside of R.

```{r eval=FALSE}
File<-read.csv("File.csv", stringsAsFactors = FALSE)

File<-readr::read_csv ("File.csv")

File<-data.table::fread("File.csv")

File<-readxl::read_xlsx("File.xlsx",skip=1) #skip first row 

File<-readxl::read_xlsx("File.xlsx",sheet=2) #read data from sheet 2
```

#### Date and time

Date and time can be handle in base R. The library _lubridate_ is useful for parsing date data. One warning, errors with parsing can occur if there are characters in the column containing date data. It is possible to get an overview of the functions of the library by typing _help(package=lubridate)_.

```{r date}
library(dplyr)
dfdate<-data.frame("DateofEvent"=c("12/03/2005","12/04/2006",NA),"Result"=c(4,5,6))
class(dfdate$DateofEvent)
dfdate$DateofEvent
#remove NA
dfdate %>% filter(!is.na(DateofEvent))
#convert to Date using base R
dfdate$DateofEvent2<-as.POSIXct(dfdate$DateofEvent)
class(dfdate$DateofEvent2)
dfdate$DateofEvent2
```

Partitioning date time data to weekend or week day is simple. Below, a simple data frame is created containing Date and Time. A simple check using _class_ call show that the data are factors. The data can be converted to date using _as.POSIXct_ call with base R or _ymd_ or _ymd_hms_ with _lubridate_. The day of the week can be parsed with _wday_ call. This call _wday_ return a numerical vector of days. Sunday is day 1 and Saturday is day 7.

```{r weekend}
library(lubridate)
library(dplyr)
#help(package="lubridate")
#create an example for practice
dfw<-data.frame(
Location=c("MMC","Casey","DDH","DDH","MMC"),
Date=c("2020-04-22","2020-04-23","2020-04-24","2020-04-25","2020-04-26"),Time=c("15:33:21","20:32:34","12:12:43","09:40:32","18:23:50"))
dfw1<-dfw %>% 
        #merge Date Time columns
        mutate(DateTime=paste0(Date,Time),
        #Date and Time data are factors
        DateTime=ymd_hms(DateTime),
        #parse day of week
        Wday=wday(DateTime),
        #return logical value for days 1 and 7
        Wkend=(Wday==1 | Wday==7),
        #hour returns a value between 0-24
        Ohour=hour(DateTime),
        #return logical value for time between 8-17
        Officehour=(Ohour>=8 & Ohour<=17))
```

### Foreign data

The foreign library is traditionally use to handle data from _SPSS_ (.sav), _Stata_ (.dta) and _SAS_ (.sas). One should look at the ending in the file to determine the necessary library. 

```{r eval=FALSE}
library(foreign)
#write and read Stata
write.dta(dfdate,file="./Data-Use/dfdate_temp.dta")
a<-read.dta("./Data-Use/dfdate_temp.dta")
a
```

This dataset is available for download from https://www1.nyc.gov/site/doh/data/data-sets/data-sets-and-tables.page. It contains data on a survey of health status on New Yorkers in 2018. The data is available for download is _SAS_ format. The _.sas7bdat_ is the latest version of _SAS_ and requires this _sas7bdat_ library.

```{r eval=FALSE}
library(sas7bdat)
chc2018<-read.sas7bdat("./Ext-Data/chs2018_public.sas7bdat")
```

The _foreign_ library can handle older _SAS_ files but not the current version. The current version of _SAS_ file requires _sas7bdat_. The clue is the file ends in .sas7bdat.

### json format

Json is short for JavaScript object Notification. These files have a hierarchical (nested) structured format.  The json file is in text format and can also be examined using _Notepad_. These files can be read using the _RJSONIO_, _rjson_ or _tidyjson_ libraries in R.

There are 2 sample json files. One is the address book for kommune 0411 in Denmark. The other json data is the hospital bed per 1000 people data is obtained from  World Bank at
https://data.worldbank.org/indicator/SH.MED.BEDS.ZS.

```{r address RJSONIO}
library(RJSONIO)
j<-fromJSON("./Data-Use/0411.geojson")
j<-lapply(j, function(x) {
  x[sapply(x,is.null)]<-NA
  unlist(x)
})
k<-as.data.frame(do.call("cbind",j)) #list to data frame

j1<-fromJSON("./Data-Use/WB-Beds/SH.MED.BEDS.json")
j1<-lapply(j1, function(x) {
  x[sapply(x,is.null)]<-NA
  unlist(x)
})
k1<-as.data.frame(do.call("cbind",j)) #list to data frame
```


```{r rjson}
library(rjson)
setwd("./Data-Use/WB-Beds/")
j2<-rjson::fromJSON(file="SH.MED.BEDS.json")
```
The _tidyjson_ library contains the world bank data in _json_ format. 

```{r address tidyjson}
library(tidyjson)
library(dplyr)
SH.MED.BEDS<-read_json("./Data-Use/WB-Beds/SH.MED.BEDS.json")

```

### XML

XML refers to extensible markup language. Extracting XML object to data frame requires conversion to a list and then unlist the document.

```{r xml}
library(XML)
library(methods)
#parse data into XML document
result<-xmlParse("./Data-Use/WB-Beds/API_SH.MED.BEDS.ZS_DS2_en_xml_v2_994608.xml")
#explore data structure of xml document
#find number of root node
rootnode <- xmlRoot(result)
rootsize <- xmlSize(rootnode)
#print(rootsize)
#print(rootnode[1])
#create data frame directly-not recommended
Bedsdf<-xmlToDataFrame(result) # 1 15840
#a different way to convert to data frame
Bedsdfdoc <- xmlToDataFrame(nodes = getNodeSet(result, "//record"))
#View(Bedsdfdoc)
```

The XML library is no longer supported. 

```{r xml2}
library(xml2)
library(tidyverse)
t<-read_xml("./Data-Use/WB-Beds/API_SH.MED.BEDS.ZS_DS2_en_xml_v2_994608.xml")
recs <- xml_find_all(t, "//record")
# extract and clean all the columns
vals <- trimws(xml_text(recs))
```

### DICOM, nifti and minc format

R can handle a variety of different data format. Medical images are stored as Digital Imaging and Communications in Medicine or _DICOM_ files for handling and converted to  Neuroimaging Informatics Technology Initiative or _nifti_ files for analysis. DICOM files have x-axis arranged right to left, y-axis arranged anterior to posterior and z-axis arranged inferior to superior. NIFTI files are adapted from ANALYZE 7.5 format and integrate the seaprate header and image files from ANALYZE format. The workhorses are the _oro.dicom_ and _oro.nifti_ libraries. The package _oro.nifti_ treat _nifti_ as an S4 class object with multiple slots for data type. These slots can be accessed by typing the @ after the handle of the file. The MINC 1.0 refers to Medical Imaging (Network Common Data Format (NetCDF) [@10.3389/fninf.2016.00035]. This file format was developed so that the metadata of the images including processing history can be stored; this was not possible with the ANALYZE format. The current version of MINC 2.0 uses HDF5 (The HDF Group, 1997â€“2015). This format has a hierarchical structure. 

The standard brain template is MNI152. The MNI template was created usinng 152 male right handed 23 year old medical students at MNI [@pmid8126267]. A high resolution brain known as Big Brain was created on 3T at 20 micrometers [@Amunts1472]

```{r create nifti}
library(oro.nifti)
set.seed(1234)
dims = rep(5, 3)
SimArr = array(rnorm(5*5*5), dim = dims)
SimIm = oro.nifti::nifti(SimArr)
print(SimIm)
neurobase::ortho2(SimIm)
```

```{r nifti}
library(oro.nifti)
mca<-readNIfTI("./Data-Use/mca_notpa.nii.gz", reorient = FALSE) 
print(mca)
#plot mca 
image(mca[50,,]) #sagittal
image(mca[,70,]) #coronal
image(mca[,,35]) #axial
```

#### Manipulating array

The imaging data are stored as arrays within the _.Data_ slot in nifti. 

```{r image array}
library(oro.nifti)
#extract data as array using @ function
img<-readNIfTI("./Data-Use/mca_notpa.nii.gz", reorient = FALSE) 
k<-img@.Data
#channge x orientation to right to left 91*109*91
k1<-k[91:1,,] 
image(k1[,,35])
img2<-img
#find available of slots
slotNames(img)
img2@.Data <- k1
#write image to file
#writeNIfTI(img2, "tt")
#unsigned angle between 2 vectors k and k1
Morpho::angle.calc(k,k1)

#centre of gravity
neurobase::cog(img)
```

#### Math operations

```{r math nifti}
#stack arrays
k2<-cbind(k,k1) #cbind combines columns
dim(k2) #902629 2
ak2<-t(rbind(k,k1)) #rbind combines rows, t transpose the rows
dim(ak2) # 902629      2
#abind arrays
ab<-abind::abind(k,k1)
dim(ab) #91 109 182
neurobase::ortho2(ab)
dim(ab)
#sum array
k3<-k+k1
dim(k3) #91 109 91
neurobase::ortho2(k3)
```

The MNI brain template is the standard image for registration. It is obtained from 152 mainly right-handed male medical students.

```{r mri rendering}
library(rgl)
library(misc3d)
library(neurobase)
library(aal)#devtools::install_github("muschellij2/aal")
library(MNITemplate) #devtools::install_github("jfortin1/MNITemplate")
img = aal_image()
template = readMNI(res = "2mm")
cut <- 4500
dtemp <- dim(template)
# All of the sections you can label
labs = aal_get_labels()
# Pick the region of the brain you would like to highlight - in this case the hippocamus_L
#hippocampus = labs$index[grep("Hippocampus_L", labs$name)]
#load mca
mca<-readNIfTI("./Data-Use/mca_notpa.nii.gz", reorient = FALSE)
mca2<-(mca>0.05)*mca
mask = remake_img(vec = img %in% mca2, img = img)
### this would be the ``activation'' or surface you want to render 
contour3d(template, x=1:dtemp[1], y=1:dtemp[2], z=1:dtemp[3], level = cut, alpha = 0.1, draw = TRUE)
contour3d(mask, level = c(0.5), alpha = c(0.5), add = TRUE, color=c("red") )
### add text
text3d(x=dtemp[1]/2, y=dtemp[2]/2, z = dtemp[3]*0.98, text="Top")
text3d(x=-0.98, y=dtemp[2]/2, z = dtemp[3]/2, text="Right")
#rglwidget()
```

Up to now we have discussed handling of MRI scans. The library for segmenting hemorrhage on CT scan is _ichseg_ [@muschelli2017pitchperfect]. Its use require installation of _fsl_.

```{r ichseg}
```

#### tar file

Image files can be large and are often stored as tar files. The _tar_ (tgz file), _untar_, _zip_ (gz file) and _unzip_ function are from the _utils_ library. The _readNIfTI_ call can open gz file without the need to call _unzip_ function.

```{r tar file}
colin_1mm<-untar("./Data-Use/colin_1mm.tgz")
colinIm<-readNIfTI("colin_1mm") #1 x 1 x 1
class(colinIm)
# look at 2 images side by side
neurobase::double_ortho(img, template)
#combining colin and mca map
img2<-(img*1000)+template/10000
neurobase::ortho2(img2,col.y = alpha("red",.3))

library(RNiftyReg)
epi_t2<- readNIfTI(system.file("extdata", "epi_t2.nii.gz", package="RNiftyReg"))
class(epi_t2)
neurobase::ortho2(epi_t2) #2.5 2.5 2.5
```

#### Image registration

There are many different libraries for performing registration.

```{r registration}
library(RNiftyReg)
source <- readNifti(system.file("extdata", "epi_t2.nii.gz", package="RNiftyReg"))
target <- readNifti(system.file("extdata", "flash_t1.nii.gz", package="RNiftyReg"))
result <- niftyreg(source, target)
#affine transformation
result$forwardTransforms
```

### Mesh generation

Generate 3D mesh from brain

```{r Rvcg}
library(Rvcg)
library(oro.nifti)
library(rgl)
library(misc3d)
###load image
mca <- readNIfTI("./Data-Use/mca_notpa.nii.gz")
mca2=(mca*1000)
#array
k<-mca2@.Data

### Generate surface
kmesh<-vcgIsosurface(k, threshold = 0.1)

### decimate it
kmesh <- vcgQEdecim(kmesh,edgeLength = 1)

### smooth it
kmesh <- vcgSmooth(kmesh)

### save it to disk 
vcgPlyWrite(kmesh)

shade3d(vcgSmooth(kmesh,"HC",iteration=3),col="pink") 

kbp <- vcgBallPivoting(kmesh, radius = 0.0022, clustering = 0.2, angle = pi/2)
shade3d(kbp, col = rainbow(1000), specular = "black")
```

## Tidy data
Attention to collection of data is important as it shows the way for performing analysis. In general each row represents an observation (ie patient 1) and each column represents an attribute of that observation (ie patient 1 having CT abdomen). Avoid the temptation to embed 2 types of attributes into a column (avoid patient 1 having CT abdomen positive). 

```{r variables}
df2<-data.frame(Var=c("test positive","test negative"), Result=c(5, 10))
df2
```

The above example should be entered this way. This change allows one to group variables by Test status: 'positive' or 'negative'. One can easily perform a t test here (not recommend in this case as the data contains only 2 rows).

```{r variables2}
df2<-data.frame(Test=c("positive","negative"), Result=c(5, 10))
df2
```
The below example is about collapsing of column using the base R. 
```{r collapsing column}
dfa<-data.frame(City=c("Melbourne","Sydney","Adelaide"),State=c("Victoria","NSW","South Australia"))
#collapsing City and State columns and generate new column address
dfa$addresses<-paste0(dfa$City,",", dfa$State) #separate by comma
dfa$addresses2<-paste0(dfa$City,",", dfa$State,", Australia")
dfa
```

This exanple is same as above but uses verbs from _tidyr_. This is useful for collapsing address for geocoding. 

```{r unite}
library(tidyr)
dfa1<-dfa %>% unite ("new_address",City:State,sep = ",")
dfa1
```

Using the data above, let's split the column address
```{r separate}
library(tidyr)
dfa2<-dfa %>% separate(addresses, c("City2", "State2"))
dfa2
```

### Factors

There are several types of factors in R: ordered and not ordered. It is important to pay attention to how factors are coded. Sometimes, male is represented as 1 and female as 0. Sometimes, female is represented as 2. This discussion may seems trivial but a paper had been retracted in a high impact factor journal Jama because of miscoding of the trial assignment 1 and 2 rather than the assignment of 0 and 1. This error led to reversing the results with logistic regression [@pmid31593277]. This error led to report that an outpatient management program for chronic obstructive pulmonary disease resulted in fewer admissions. Below is an example which can occur when data is transformed into factor and back to number. Note that the coding goes from 0 and 1 to 2 and 1.

In certain analyses, the libraries prefer to use the dependent  or outcome variable as binary coding in numeric format eg logistic regression and random forest. The library _e1071_ for performing supprt vector machine prefers the outcome variable as factor.

```{r factors}
library(Stat2Data)
data("Leukemia") #treatment of leukemia
Leukemia$Resp
Leukemia$Response.factor<-as.factor(Leukemia$Resp)
Leukemia$Response.factor
Leukemia$Response.numeric<-as.numeric(Leukemia$Response.factor)
Leukemia$Response.numeric
```

This illustration describes conversion of a continuous variable into orderly factors.

```{r ordered factors}
library(Stat2Data)
data("Leukemia") #treatment of leukemia
#partition Age into 8 ordered factors
Leukemia$AgeCat<-ggplot2::cut_interval(Leukemia$Age, n=8, ordered_result=TRUE)
class(Leukemia$AgeCat)
```

### Multiple files
Merging of files can be done using _dplyr_ to perform _inner_join_, _outer_join_, _left_join_ and _right_join_. Note that this can also be done in base R or using syntax of _data.table_.

### Pivot
A variety of different expressions are used to describe data format such as wide and long formats. In some case the distinction between such formats is not clear. The verbs for performing these operations are _pivot_wide_, _pivot_long_. Again _data.table_ uses different verbs such as _cast_ and _melt_. In general, most regression analyses are performed with data in wide format. In this case each row represents a unique ID.  Longitudinal analyses are performed with data in long format. In this format, there are several rows with the same ID. In the next Chapter on Statistics, an example of data generated in wide format and coverted to long format using _plyr_. Here we will demonstrate the use of _tidyr_ to pivot loner or wider. 

The best way to think about how data should be presented is that data is analyzed according to columns not rows. The data below is extracted from CDC COVID website. Details are given below under Web scraping on how this task was performed.

```{r pivot longer long format}
library(dplyr)
library(tidyr)
library(stringr)
usa<-read.csv("./Data-Use/Covid_bystate_Table130420.csv")
# for demonstration we will select 3 columns of interest
usa_long <-usa %>% select(Jurisdiction,NumberCases31.03.20,NumberCases07.04.20) %>% pivot_longer(-Jurisdiction,names_to = "Date",values_to = "Number.Cases")  
usa_long$Date <- str_replace(usa_long$Date,"NumberCases","")
head(usa %>%select(Jurisdiction,NumberCases31.03.20,NumberCases07.04.20),6) #data in wide format
head(usa_long,6) #data in long format
```

This second illustrates that sometimes the _pivot_long_ call helps to organise data in wide format. This dataset is taken from article in Jama on variations in death in New York.

```{r pivot longer wide format}
library(tidyverse)
dfj<-data.frame(
Borough=c("Bronx","Brooklyn","Manhattan","Queens","Staten Island"),
Pop=c(1432132,2582830,1628701,2278906,476179),
Age65=c(12.8,13.9,16.5,15.7,16.2),
White=c(25.1,46.6,59.2,39.6,75.1),
Hispanic=c(56.4,19.1,25.9,28.1,18.7),
Afro.American=c(38.3,33.5,16.9,19.9,11.5),
Asian=c(4.6,13.4,14,27.5,11),
Others=c(36.8,10.4,15.4,17,5.2),
Income=c(38467,61220,85066,69320,82166),
Beds=c(336,214,534,144,234),
COVIDtest=c(4599,2970,2844,3800,5603),
COVIDhosp=c(634,400,331,560,370),
COVIDdeath=c(224,181,122,200,143),
COVIDdeathlab=c(173,132,91,154,117)
  ) %>% 
  #reverse prevalence per 100000 to raw
  mutate(Age65raw=round(Age65/100*Pop,0),
               Bedsraw=round(Beds/100000*Pop,0),
               COVIDtestraw=round(COVIDtest/100000*Pop,0),
               COVIDhospraw=round(COVIDhosp/100000*Pop,0),
               COVIDdeathraw=round(COVIDdeath/100000*Pop),0) %>%
#move all ethnic types under Ethnicity and percentage to Proportion
pivot_longer(names_to = "Ethnicity", values_to = "Proportion",cols=c(White,Afro.American,Asian,Hispanic,Others))
```

## Regular Expressions

Here is a short tutorial on regular expression. We will begin using base R. This section is based on experience trying to clean a data frame containing many words used to describe one disease or one drug.

### base R
```{r base-R-example}
#create example dataframe
df<-data.frame(
  drug=c("valium 1mg","verapamil sr","betaloc zoc","tramadol","valium (diazepam)"),
  infection=c("pneumonia","aspiration pneumonia","tracheobronchitis","respiratory tract infection","respiratory.tract.infection"))
df
```

Now that we have a data frame, we can use pattern matching to replace part of phrase. This step can be done simply using _gsub_ command. First create a list so that the computer searches the phrases in the list.

```{r gsub}
#create list to remove phrase
redun=c("1mg", "zoc", "sr")
pat=paste0("\\b(",paste0(redun,collapse = "|"),")\\b")
df$drug1<-gsub(pat,"",df$drug)
df$drug1
#create list to remove phrase
redunc1=c("respiratory tract infection", "tracheobronchitis", "aspiration")
pat=paste0("\\b(",paste0(redunc1,collapse = "|"),")\\b")
df$infection1<-gsub(pat,"",df$infection)
df$infection1
```

This section deals with meta-characterers. Examples of meta-characters include $ . + * ? ^ () {} []. These meta-characters requires the double back slashes \\.

```{r metacharacter}
#create list to remove phrase
redun=c("1mg", "zoc", "sr")
pat=paste0("\\b(",paste0(redun, collapse = "|"),")\\b")   
df$drug2<-gsub(pat,"",df$drug)

#[a-z] indicates any letter
#[a-z]+ indicates any letter and those that follow the intial letter
df$drug2<-gsub("\\(|[a-z]+\\)","",df$drug2)
df$drug2
```

Back to our data frame df, we want to remove the change the different words accounting for pneumonia.

```{r list}
redunc=c("\\.")
redunc1=c("respiratory tract infection", "tracheobronchitis", "aspiration")
pat=paste0("\\b(",paste0(redunc,collapse = "|"),")\\b")
df$infection2<-gsub(pat," ",df$infection)
pat=paste0("\\b(",paste0(redunc1,collapse = "|"),")\\b")
df$infection2<-gsub(pat," ",df$infection2)
df$infection2
```

### stringr

The following examples are taken from excel after conversion from pdf. In the process of conversion errors were introduced in the conversion from pdf to excel. 

```{r bracket}
library(stringr)
#error introduced by double space 
a<-c("8396 (7890 to 8920)","6 301 113(6 085 757 to 6 517 308)","4 841 208 (4 533 619 to 5 141 654)","1 407 701 (127 445 922 to 138 273 863)","4 841 208\n(4 533 619 to\n5 141 654)")

b<-str_replace (a, "\\(c.*\\)","")

#this is a complex example to clean and requires several steps. Note that the original data in the list a is now assigned to b. 
b<-str_replace(a,"\n","") %>% 
  str_replace("\\(.*","") %>%
  str_replace("\n.*","") %>%
  str_replace("\\)","") %>%
str_replace("\\s","") %>%
  str_replace("\\s","")%>% as.numeric()
b
```

Another example. This time the 2 numbers in the column are separated by a slash sign. Supposed you want to keep the denominator. The first remove the number before the slash sign. The * metacharacter denotes the action occurs at the end.

```{r slash}
df.d<-data.frame(seizure.rate=c("59/90", "90/100", "3/23"))
df.d$seizure.number<-str_replace(df.d$seizure.rate,"[0-9]*","") 
df.d$seizure.number
```

Now combine with the next step to remove the slash sign. 

```{r slash2}
#We used [0-9] to denote any number from 0 to 9. For text, one can use [A-Z].
df.d$seizure.number<-str_replace(df.d$seizure.rate,"^[0-9]*","")%>%
  str_replace("/","\\")
df.d$seizure.number
```

Removing the denominator requires a different approach. First remove the last number then the slash sign. 

```{r slash3}
df.d$case<-str_replace(df.d$seizure.rate,"/[0-9]*"," ")
df.d$case
```

## PDF to xcel

Sometimes data from public governement sites come in PDF form instead of excel. Conversion from pdf to excel or text can be difficult especially with special character eg Danish. There are several libraries for doing this: _pdftables_ (require API key) and _pdftools_. The example below uses _pdftools_. Documentation for _pdftools_ is available at https://docs.ropensci.org/pdftools/. The document is the 2018 Danish Stroke Registry report. The _tabulizer_ package is excellent for converting table data. However, _tabulizer_ package depends on _rJava_ and requires deft handling. 

```{r pdf Demark}
library(pdftools)
txtDK<-pdf_text("./Data-Use/4669_dap_aarsrapport-2018_24062019final.pdf")
cat(txtDK[17]) #browse data page 13+4 filler pages
#render page for screen shot
screenshot13<-pdf_render_page("./Data-Use/4669_dap_aarsrapport-2018_24062019final.pdf", page =17)
#take screen shot
png::writePNG(screenshot13, "./Data-Use/Danish-Stroke-page13.png")
#knit screenshot
knitr::include_graphics("./Data-Use/Danish-Stroke-page13.png")
```

An alternatively dataset is from New York City Health Department of Health and Mental Hygiene available at   https://www1.nyc.gov/assets/doh/downloads/pdf/vs/2017sum.pdf. This table is illustrated as the data for life expectanct and age adjusted deathrate is not tidy.


```{r pdf NY}
library(pdftools)
txtNY<-pdf_text("./Ext-Data/2017nyc.pdf")
cat(txtNY[10]) #life expectancy
cat(txtNY[12]) # age adjusted death
```
### Scanned text or picture

Importing data from scanned text will require use of Optical Character Recognition (OCR). The _tesseract_ library provides an R interface for OCR. In the example below, a picture is taken from same CDC website containing mortality data  (https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/04102020/nchs-data.html). The screenshot of this website was then cleaned in _paint_. The data is available in the Data-Use folder.
```{r tesseract}
library(tesseract)
eng <- tesseract("eng") #english
text <- tesseract::ocr("./Data-Use/Covid_PNG100420.png", engine = eng)
cat(text)
```

## Web scraping
The readers may ask why web scraping for healthcare. A pertinent example related to COVID-19 data is provided below. The library _rvest_ is helpful at scraping data from an internet page. The _rvest_ library assumes that web contents have xml document-tree representation. The different options available for web scraping with _rvest_ are available at the website https://rvest.tidyverse.org/reference/. The user can use CSS selectors to scrape content. The library _Rselenium_ is also useful for web scraping. For dynamic web page, the library _CasperJS_ library does a better job especially if the data contain embedded java script. 

The library _cdccovidview_ provides access to the CDC website on COVID-19. In the example below, we will try to this manually. Data from CDC website on COVID-19 is downloaded, cleaned and saved in csv format. It is important to pay attention to the data. The first row contains header and is removed. There are several columns with commas. These commas can be removed using the exercises above. Further the data is updated on weekly basis. As such the data needs to be converted into a date time format using _lubridate_. 

```{r scrape}
library(rvest)
library(tidyverse)
#assign handle to web page accessed 12/4/20
cdc<-read_html("https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/04102020/nchs-data.html")#"xml_document" "xml_node"  
# scrape all div tags
html_tag <- cdc %>% html_nodes("div")
# scrape header h1 tags
html_list<-html_tag %>% html_nodes("h1") %>% html_text()
#there is only one table on this web page
Table1<- cdc %>% html_node("table") %>% html_table(fill = TRUE)
#Table1 has a header row
Table1<-Table1[-1,]
#The data in the Total Deaths column has a comma 
Table1$Total.Deaths<-as.numeric(gsub(",","",Table1$`Total Deaths`))
#now combine the year and week column to Date
Table1$Date<-lubridate::parse_date_time(paste(Table1$Year, Table1$Week, 'Mon', sep="/"),'Y/W/a')
#there are still commas remaining in some columns. This is a useful exercise for the reader. A solution is provided in the next example.  
#write.csv(Table1,file="./Data-Use/Covid_Table100420.csv")
```

The next example is from the CDC COVID-19 website. It poses a different challenges as there are several columns with the same names. In this case we will rename the column by index. There are several columns containing commas. Rather than removing column by column we will write a function with lapply to do it over the table. the apply function returns a matrix whereas lapply returns a dataframe. There is one column containing percentage enclosed in a bracket. This can be removed using the example above on metacharacter ie using doule back slash in front of bracket and again at close of bracket.

```{r covid-us}
library(rvest)
library(tidyverse)
cdc<-read_html("https://www.cdc.gov/mmwr/volumes/69/wr/mm6915e4.htm?s_cid=mm6915e4_w") #"xml_document" "xml_node"   
# scrape all div tags
html_tag <- cdc %>% html_nodes("div")
# scrape header h1 tags
html_list<-html_tag %>% html_nodes("h1") %>% html_text()
#there is only one table on this web page
Table2<- cdc %>% html_node("table") %>% html_table(fill = TRUE)
#first row is header
names(Table2) <- as.matrix(Table2[1, ])
Table2<-Table2[-c(1:2,55),]#rows 1 and 2 are redundant
#rename the columns by index 
names(Table2)[2] <-"NumberCases31.03.20"
names(Table2)[3]<-"CumulativeIncidence31.03.20"
names(Table2)[4]<-"NumberCases07.04.20"
names(Table2)[5]<-"NumberDeath07.04.20"
names(Table2)[6]<-"CumulativeIncidence07.04.20"
#rather than removing column by column we will write a function with lapply to remove commas over the table. the apply function returns a matrix whereas lapply returns a dataframe.
Table2<-as.data.frame(lapply(Table2, function(y) gsub(",", "", y))) 
Table2<-as.data.frame(lapply(Table2, function(x)
  gsub("\\(|[0-9]+\\)","",x)))
#write.csv(Table2,file="./Data-Use/Covid_bystate_Table130420.csv")
```

The script above can be used to extract data from any open access article from medical journals. It does not work if the article has not provided full access. For example full access articles from Stroke or Circulation can easily be accessed except more recent articles.

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  df, caption = 'Table containing uncleaned data above',
  booktabs = TRUE
)
```

```{r nice-tab2, tidy=FALSE}
knitr::kable(
  Table2, caption = 'Table containing cleaned data above',
  booktabs = TRUE
)
```