[
["Statistics.html", "Chapter 4 Statistics 4.1 Univariable analyses 4.2 Regression 4.3 Special types of regression 4.4 Non-linear regression 4.5 Sample size estimation 4.6 Metaanalysis 4.7 Data simulation", " Chapter 4 Statistics This section is not intended as a textbook on statistics. Rather it demonstrates different regression approaches that can be used such as Poisson 4.3.6, survival 4.3.3, quantile 4.3.4, penalised 4.3.5, and non-linear regressions 4.4. It also includes sections on Sample size estimation, Metaanalysis and Data simulation. R codes are provided. 4.1 Univariable analyses 4.1.1 Parametric tests T-test is the workhorse for comparing if 2 datasets are have the same distribution. Performing t-test in R requires data from 2 columns: one containing the variables for comparison and one to label the group. There are different forms of t-test depending on whether the two samples are paired or unpaired. In general, the analysis takes the form of \\(t=\\frac{\\mu_1 - \\mu_2}{variance}\\). It is recommended to check the distribution of the data by using histogram. For this exercise, we will use the simulated data from ECR trials. The grouping variable is the trial assignment. #comparison of early neurological recovery (ENI) by tral (T) dtTrial&lt;-read.csv(&quot;./Data-Use/dtTrial_simulated.csv&quot;) t.test(dtTrial$ENI~dtTrial$T) 4.1.2 Non-parametric tests Chi-squared and Fisher-exact tests can be done by using the table function for setting up the count data into 2 x 2 contingency table or confusion matrix. The formula for the Chi-squared test takes on a familiar form \\(\\chi^2=\\frac{(observed-expected)^2}{expected}\\). In this example we will use the data above. table(dtTrial$HT,dtTrial$T) chisq.test(dtTrial$HT,dtTrial$T) The Wilcoxon rank sum test is performed with continuous data organised in the same way as the t-test. There are several different approaches to performing Wilcoxon rank sum test. The coin package allows handling of ties. library(coin) wilcox.test(ENI~T, data=dtTrial) 4.2 Regression There are many different form of regression methods: Linear (least square) regression and Non-linear regression. A key principle is that the predictors are independent of each others. This issue will be expand on in the later in the section on collinearity. Special methods are required when the predictors are collinear. 4.2.1 Linear (least square) regression Least square regression uses the geometric properties of Euclidean geometry to identify the line of best. The sum of squares \\(SSE\\) is \\(\\sum(observed-expected)^2\\). The \\(R^2\\) is a measure of the fit of the model. It is given by \\(1-\\frac{SS_(res)}{SS_(total)}\\). Low \\(R^2\\) indicates a poorly fitted model and high \\(R^2\\) indicates excellent fitting. The assumption here is that the outcome variable is a continuous variable. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.1 load(&quot;./Data-Use/world_stroke.Rda&quot;) ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+geom_smooth(method=&quot;lm&quot;, aes(Group=Income, linetype=Income))+geom_point()+xlab(&quot;Life Expectancy&quot;) ## Warning: Ignoring unknown aesthetics: Group ## Warning: Removed 71 rows containing non-finite values (stat_smooth). ## Warning: Removed 71 rows containing missing values (geom_point). 4.2.2 Logistic regression For outcome that are binary in nature such as yes or no, then least square regression is not appropriate. There are no close form solution for this analysis and a numerical approach using maximum likelihood approach is needed. When examining the results of logistic regression one is often enchanted by the large odds ratio. It is important to look at the metrics of model calibration (discussed below). A clue to a poorly calibrated model is the observation that the width of the confidence interval for odds ratio is wide. 4.2.2.1 Discrimination and Calibration A high _\\(R^2\\) suggests that the linear regression model is well calibrated. This metric is often not displayed but should be sought when interpreting the data. The areas under the receiver operating characteristic curve (AUC) is used to assess how well the models discriminate between those who have the disease and those who do not have the disease of interest. An AUC of 0.5 is classified as no better than by chance; 0.8 to 0.89 provides good (excellent) discrimination, and 0.9 to 1.0 provides outstanding discrimination. This rule of thumb about interpreting AUC when reading the literature is language the authors used to describe the AUC. This test of discrimination is not synonymous with calibration. It is possible to have a model with high discrimination but poor calibration (Diamond 1992). Calibration of logistic regression model is performed using the Hosmer–Lemeshow goodness-of-ﬁt test and the Nagelkerke generalized R2. A model is well calibrated when the Hosmer–Lemeshow goodness-of-ﬁt test shows no difference between observed and expected outcome or P value approaching 1. A high generalized R2 value suggests a well-calibrated regression model. 4.2.2.2 Measuring Improvement in Regression Models The net reclassification improvement (NRI) and integrated discrimination improvement (IDI) have been proposed as more sensitive metrics of improvement in model discrimination.The NRI can be considered as a percentage reclassiﬁcation for the risk categories and the IDI is the mean difference in predicted probabilities between 2 models (constructed from cases with disease and without disease). The NRI and IDI scores are expressed as fractions and can be converted to percentage by multiplying 100.The continuous NRI and IDI were performed using PredictABEL . (Phan et al. 2017)(Phan et al. 2016) 4.2.2.3 Shapley value We can use ideas from game theory relating to fair distribution of proﬁt in coalition games; the coalition (co-operative) game in this case can be interpreted as contribution of the covariates to the model. The Shapley value regression method calculates the marginal contribution of each covariate as the average of all permutations of the coalition of the covariates containing the covariate of interest minus the coalition without the covariate of interest. The advantage of this approach is that it can handle multicollinearity (relatedness) among the covariates. library(iml) 4.2.2.3.1 Interaction When describing interaction terms it is recommended that the results be expressed as β coefficients rather than as odds ratio. 4.2.2.4 Propensity matching A common misconception is that the multiple regression adjust for imbalance in covariates. This issue was observed in the pivotal NINDS alteplase trial. The results of the trial has since been accepted with re-analysis of this trial. Propensity matchine is an important technique to adjust for imbalance in covariates between 2 arms. There are concerns with mis-use of this technique such as difference in placebo arms from multiple sclerosis trials (Signori et al. 2020). It is proposed that this technique should be used only if all the confounders are measurable. This situation may not be satisfied if the data were accrued at different period, in different continent etc. 4.3 Special types of regression 4.3.1 Multinomial modelling Multinomial modelling is used when the outcome categorical variables are not ordered. This situation can occur when analysis involves choice outcome (choices of fruit: apple, orange or pear). In this case, the log odds of each of the categorical outcomes are analysed as a linear combination of the predictor variables. The nnet library have functions for performing this analysis. 4.3.2 Ordinal regression Ordinal regression is appropriate when the outcome variable is in the form of ordered categorical values. For example, the Rankin scale of disability is bounded between the values of 0 and 6. This type of analysis uses the proportional odds model and the requirement for this model is stringent. When examining results of ordinal regression check that the authors provide this metric, the Brant test. The Brant test assesses the parallel regression assumption. Ordinal regression is performed using polr function in MASS library. The Brant test is available in the Brant library. 4.3.3 Survival analysis Survival analysis is useful when dealing with time to event data. The Cox model assesses the hazard of outcome between two groups. The assumption of this model is that the hazard between each arm is proportional (Stensrud and Hernan 2020). The proportional hazard model can be tested (Grambsch and Therneau 1994) library(survival) data(&quot;lung&quot;) #data from survival package on NCCTG lung cancer trial #https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html #time in days #status censored=1, dead=2sex: #sex:Male=1 Female=2 sfit&lt;- coxph(Surv(time, status) ~ age+sex+ph.ecog, data = lung) summary(sfit) ## Call: ## coxph(formula = Surv(time, status) ~ age + sex + ph.ecog, data = lung) ## ## n= 227, number of events= 164 ## (1 observation deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.011067 1.011128 0.009267 1.194 0.232416 ## sex -0.552612 0.575445 0.167739 -3.294 0.000986 *** ## ph.ecog 0.463728 1.589991 0.113577 4.083 4.45e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.0111 0.9890 0.9929 1.0297 ## sex 0.5754 1.7378 0.4142 0.7994 ## ph.ecog 1.5900 0.6289 1.2727 1.9864 ## ## Concordance= 0.637 (se = 0.025 ) ## Likelihood ratio test= 30.5 on 3 df, p=1e-06 ## Wald test = 29.93 on 3 df, p=1e-06 ## Score (logrank) test = 30.5 on 3 df, p=1e-06 4.3.4 Quantile regression Quantile regression is appropriate when the distribution of the data is non-normal and it is more appropriate to look at the conditional median of the dependent variable. There are several libraries for this task quantreg and bayesian libraries. In the example below, the life time risk of stroke is regresed against life expectancy using lest square and quantile regression. library(quantreg) ## Loading required package: SparseM ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve ## ## Attaching package: &#39;quantreg&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## untangle.specials load(&quot;./Data-Use/world_stroke.Rda&quot;) #quantile rqfit &lt;- rq( MeanLifetimeRisk~ LifeExpectancy, data = world_sfdf) rqfit_sum&lt;-summary(rqfit) #least square lsfit&lt;-lm(MeanLifetimeRisk~LifeExpectancy,data=world_sfdf) lsfit_sum&lt;-summary(lsfit) #plot ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ #add fitted line for least square geom_abline(intercept =lsfit_sum$coefficients[1], slope=lsfit_sum$coefficients[2],color=&quot;red&quot;)+ #add fitted line for quantile regression geom_point()+xlab(&quot;Life Expectancy&quot;)+ geom_abline(intercept =rqfit_sum$coefficients[1], slope=rqfit_sum$coefficients[2],color=&quot;blue&quot;)+ #annotate least square annotate(&quot;text&quot;,x=60, y=27,label=paste0(&quot;least square =&quot;,round(lsfit_sum$coefficients[1],2),&quot; + &quot;,round(lsfit_sum$coefficients[2],2),&quot; x &quot;,&quot;Life Expectancy&quot;),color=&quot;red&quot;)+ #annotae quantile regression annotate(&quot;text&quot;,x=75, y=12,label=paste0(&quot;quantile =&quot;,round(rqfit_sum$coefficients[1],2), &quot; + &quot;, round(rqfit_sum$coefficients[2],2),&quot; x &quot;,&quot;Life Expectancy&quot;),color=&quot;blue&quot;) ## Warning: Removed 71 rows containing missing values (geom_point). 4.3.5 Penalised regression We used penalised logistic regression (PLR) to assess the relationship between the ASPECTS regions and stroke disability (binary outcome) (Phan et al. 2013). PLR can be conceptualized as a modification of logistic regression. In logistic regression, there is no algebraic solution to determine the parameter estimate (β coefficient) and a numerical method (trial and error approach) such as maximum likelihood estimate is used to determine the parameter estimate. In certain situations overfitting of the model may occur with the maximum likelihood method. This situation occurs when there is collinearity (relatedness) of the data. To circumvent this, a bias factor is introduced into the calculation to prevent overfitting of the model. The tuning (regularization) parameter for the bias factor is chosen from the quadratic of the norms of the parameter estimate. This method is known as PLR. This method also allows handling of a large number of interaction terms in the model. We employed a forward and backward stepwise PLR that used all the ASPECTS regions in the analysis, calling on the penalized function in R programming environment. This program automatically assessed the interaction of factors in the regression model in the following manner. The choice of factors to be added/deleted to the stepwise regression was based on the cost complexity statistic. The asymmetric hierarchy principle was used to determine the choice of interaction of factors. In this case, any factor retained in the model can form interactions with others that are already in the model and those that are not yet in the model. In this analysis, we have specified a maximum of 5 terms to be added to the selection procedure. The significance of the interactions was plotted using a previously described method. We regressed the dichotomized mRS score against ASPECTS regions, demographic variables (such as age and sex), physiological variables (such as blood pressure and serum glucose level) and treatment (rt-PA). The results are expressed as β coefficients rather than as odds ratio for consistency due to the presence of interaction terms. library(stepPlr) 4.3.5.0.1 Non-negative regression In certain situations, it is necessary to constrain the analysis so that the regression coefifcients are non-negative. For example, when regressing brain regions against infarct volume, there is no reason believe that a negative coefficient attributable to a brain region is possible. Non-negative regression can be performed in R using nnls. library(nnls) 4.3.6 Poisson regression Poisson regression is used when dealing with number of event over time or distance such as number of new admissions or new cases of hepatitis or TIA over time. An assumption of the Poisson distribution is that the mean λ and variance λ are the same. Fit&lt;-glm(OUtcome~Age+Sex+Event,data=Imaginary, family=poisson) print(summary(Fit)) A special case of Poisson regression is the negative binomial regression. This latter method is used when the variance is greater than the mean pf the data or over-dispersed data. Negative binomial regression can be applied to number of ‘failure’ event over time. Here ‘failure’ has a lose definition and can be stroke recurrence after TIA or cirrhosis after hepatitis C infection. library(MASS) Fit&lt;-glm.nb(Outcome~Age+Sex+Event,data=Imaginary,init.theta = 1.032713156,link=logit) Zero-inflated data occurs when there is an abundance of zeroes in the data (true and excess zeroes). 4.3.7 Conditional logistic regression This special form of logistic regression is used when handling matched data or hierarchical data. An example would be paired carotid arteries or identical twins. 4.3.8 Multinomial modelling Multinomial modelling is used when the outcome categorical variables are not ordered. This situation can occur when analysis involves choice outcome (choices of fruit: apple, orange or pear). In this case, the log odds of each of the categorical outcomes are analysed as a linear combination of the predictor variables. The nnet library have functions for performing this analysis. 4.3.9 Mixed modelling Mixed modeling is a useful technique for handling multilevel or group data. There are several R packages for performing mixed modling such as lme4. An example of mixed modeling in metaregression is illustrated below in the section on Metaanalysis. library(lme4) 4.3.10 Trajectory modelling Trajectory analysis attempts to group the behaviour of the subject of interest over time. There are several different approaches to trajectory analysis: data in raw form or after orthonal transformation of the data in principal component analysis. Trajectory analysis is different from mixed modelling in that it examines group behaviour. The output of trajectory analysis is only the beginning of the modeling analysis. For example, the analysis may identify that there are 3 groups. These groups are labelled as group A, B and C. The next step would be to use the results in a modelling analysis of your choice. A useful library for performing trajectory analysis is akmedoids. This library anchored the analysis around the median value. The analysis requires the data in long format. The traj library is similar to the one in Stata. It uses several steps including factor and cluster analyses to idetify groups. The traj model prefers data in wide format. library(akmedoids) 4.4 Non-linear regression In this section, several different methods are illustrated such as polynomial, MARS, kernel. MARS and Supported vector machine (SVM) will be discussed in the chapters 6.2 and 6.5 on machine learning and manifold regression will be discussed in the chapter on machine learning 5.2.1.2. 4.4.0.1 Polynomial 4.4.0.2 Kernel regression Kernel regression is a non-linear regression method which uses Gaussian kernel to model the curvature of the data. It is equivalent to radial basis function (neural) network. library(np) ## Warning: package &#39;np&#39; was built under R version 3.6.3 ## Nonparametric Kernel Methods for Mixed Datatypes (version 0.60-10) ## [vignette(&quot;np_faq&quot;,package=&quot;np&quot;) provides answers to frequently asked questions] ## [vignette(&quot;np&quot;,package=&quot;np&quot;) an overview] ## [vignette(&quot;entropy_np&quot;,package=&quot;np&quot;) an overview of entropy-based methods] 4.5 Sample size estimation Clinicians are often frustrated about sample size and power estimation for a study, grant or clinical trial. This aspect is scrutinised by ethics committee and in peer review process for journals. Luckily, R provides several packages for sample size amd power estimation: pwr library. Cohen has written reference textbook on this subject (Cohen 1977). 4.5.1 Proportion library(pwr) ## Warning: package &#39;pwr&#39; was built under R version 3.6.1 #ttest-d is effect size #d = )mean group1 -mean group2)/variance pwr.t.test(n=300,d=0.2,sig.level=.05,alternative=&quot;greater&quot;) ## ## Two-sample t test power calculation ## ## n = 300 ## d = 0.2 ## sig.level = 0.05 ## power = 0.7886842 ## alternative = greater ## ## NOTE: n is number in *each* group We provided an example below for generating power of clinical trial. Examples are taken from a paper on sample size estimation for phase II trials (Phan et al. 2006). library(pwr) #h is effect size. effect size of 0.5 is very large #sample size pwr.2p.test(h=0.5,n=50,sig.level=0.05,alternative=&quot;two.sided&quot;) ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.5 ## n = 50 ## sig.level = 0.05 ## power = 0.705418 ## alternative = two.sided ## ## NOTE: same sample sizes #medium effect size pwr.2p.test(h=0.1,n=50,sig.level=0.05,alternative=&quot;two.sided&quot;) ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.1 ## n = 50 ## sig.level = 0.05 ## power = 0.07909753 ## alternative = two.sided ## ## NOTE: same sample sizes The ouput of the sample size calculation can be put into a table or plot. library(pwr) #pwr.2p.test(h=0.3,n=80,sig.level=0.05,alternative=&quot;two.sided&quot;) h &lt;- seq(.1,.5,.1) #from 0.1 to 0.3 by 0.05 nh &lt;- length(h) #5 p &lt;- seq(.3,.9,.1)# power from 0.5 to 0.9 by 0.1 np &lt;- length(p) #9 # create an empty array 9 x 5 samplesize &lt;- array(numeric(nh*np), dim=c(nh,np)) for (i in 1:np){ for (j in 1:nh){ result &lt;- pwr.2p.test(n = NULL, h = h[j], #result &lt;- pwr.r.test(n = NULL, h = h[j], sig.level = .05, power = p[i], alternative = &quot;two.sided&quot;) samplesize[j,i] &lt;- ceiling(result$n) } } samplesize ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 412 583 769 980 1235 1570 2102 ## [2,] 103 146 193 245 309 393 526 ## [3,] 46 65 86 109 138 175 234 ## [4,] 26 37 49 62 78 99 132 ## [5,] 17 24 31 40 50 63 85 #graph xrange &lt;- range(h) yrange &lt;- round(range(samplesize)) colors &lt;- rainbow(length(p)) plot(xrange, yrange, type=&quot;n&quot;, xlab=&quot;Effect size (h)&quot;, ylab=&quot;Sample Size (n)&quot; ) # add power curves for (i in 1:np){ lines(h, samplesize[,i], type=&quot;l&quot;, lwd=2, col=colors[i]) } # add annotation (grid lines, title, legend) abline(v=0, h=seq(0,yrange[2],50), lty=2, col=&quot;grey89&quot;) abline(h=0, v=seq(xrange[1],xrange[2],.02), lty=2, col=&quot;grey89&quot;) title(&quot;Sample Size Estimation\\n Difference in Proportion&quot;) legend(&quot;topright&quot;, title=&quot;Power&quot;, as.character(p), fill=colors) 4.5.1.1 Non-inferiority Non-inferiority trials may offer information in a way that a traditional superiority design do not. The design may be interested in other aspect of the treatment such as cost and lower toxicity (Kaji and Lewis 2015). Examples of non-inferiority trial designs include antibiotics versus surgery for appendicitis (Salminen et al. 2015). There are concerns with reporting of noninferiority trial. Justification for the margin provided in 27.6% (Gopal et al. 2015). The following describes a trial design where it’s expected that drug will result in a certain outcome p1 and the control arm p2 and the ratio of subject in treatment to control arm is k. The difference in outcome is delta. The margin is defined as non-inferior if &lt;0. library(TrialSize) TwoSampleProportion.NIS(alpha=.05, beta=.8, p1=.6, p2=.7, k=1, delta = .1, margin=-.2 ) ## [1] 3.225911 4.5.2 Logistic regression library(powerMediation) ## Warning: package &#39;powerMediation&#39; was built under R version 3.6.3 #continuous predictor #p1=event rate powerLogisticCon(n=317, p1=0.5, OR=exp(0.405), alpha=0.05) ## [1] 0.9500611 4.5.3 Survival studies Sample size for survival studies can be performed using powerSurvEpi or gsDesign. library(powerSurvEpi) ## Warning: package &#39;powerSurvEpi&#39; was built under R version 3.6.2 #sample size ssizeEpi.default(power = 0.80, theta = 2, p = 0.408 , psi = 0.15, rho2 = 0.344^2, alpha = 0.05) ## [1] 512 #power powerEpi.default(n = 2691, theta = 2, p = 0.408, psi = 0.250, rho2 = 0.344^2, alpha = 0.05) ## [1] 1 #Amarenco NEJM 2020 #equal sample size k=1 ssizeCT.default(power = 0.8, k = .8, pE = 0.085, pC = 0.109, RR = 0.78, alpha = 0.05) ## nE nC ## 2417 3021 library(gsDesign) ## Warning: package &#39;gsDesign&#39; was built under R version 3.6.3 ## Loading required package: xtable 4.5.4 Multiple regression The power for general linear model can be calculated using the pwr.f2.test function. library(pwr) #u=degrees of freedom for numerator #v=degrees of freedomfor denominator #f2=effect size 4.6 Metaanalysis During journal club, junior doctors are often taught about the importance of metaanalysis. It is worth knowing how to perform a metaanalysis in order to critique the metaanalysis. It is not well known outside of statistics journal that the bivariate analysis is the preferred method of metaanalysis of diagnostic study (Reitsma et al. 2005). By contrast, the majority of metaanalysis of diagnostic study uses the univariate method of Moses and Littenberg (Moses, Shapiro, and Littenberg 1993). This issue will be expanded below. 4.6.1 PRISMA The PRISMA statement is useful for understanding the search strategy and the papers removed and retained in the metaanalysis. An example of generating the statement is provided below in R. The example given here is from a paper on the use of spot sign to predict enlargment of intracerebral hemorrhage (Phan et al. 2019). library(PRISMAstatement) ## Warning: package &#39;PRISMAstatement&#39; was built under R version 3.6.3 #example from Spot sign paper. Stroke 2019 prisma(found = 193, found_other = 27, no_dupes = 141, screened = 141, screen_exclusions = 3, full_text = 138, full_text_exclusions = 112, qualitative = 26, quantitative = 26, width = 800, height = 800) 4.6.2 Metaanalysis of proportion This is an example of metaanalysis of stroke recurrence following management in rapid TIA clinic. A variety of different methods for calculating the 95% confidence interval of the binomial distribution. The mean of the binomial distribution is given by p and the variance by \\(\\frac{p \\times (1-p)}{n}\\). The term \\(z\\) is given by \\(1-\\frac{\\alpha}{2}\\) quantile of normal distribution. A standard way of calculating the confidence interval is the Wald method \\(p\\pm z\\times \\sqrt{\\frac{p \\times(1-p)}{n}}\\). The Freeman-Tukey double arcsine transformation tries to transform the data to a normal distribution. This approach is useful for handling when occurence of event is rare. The exact or Clopper-Pearson method is suggested as the most conservative of the methods for calculating confidence interval for proportion. It is based on cumulative properties of the binomial distribution. The Wilson method has similarities to the Wald method. It has an extra term \\(z^2/n\\). There are many different methods for calculating the confidence interval for proportions. Investigators such as Agresti proposed that approximate methods are better than exact method (Agresti and Coull 1998). Brown and colleagues proposed the use of the Wilson method (Brown, Cai, and DasGupta 2001) library(metafor) #open software metafor ## Warning: package &#39;metafor&#39; was built under R version 3.6.3 ## Loading required package: Matrix ## Loading &#39;metafor&#39; package (version 2.4-0). For an overview ## and introduction to the package please type: help(metafor). #create data frame dat #xi is numerator #ni is denominator dat &lt;- data.frame(model=c(&quot;melbourne&quot;,&quot;paris&quot;,&quot;oxford&quot;,&quot;stanford&quot;,&quot;ottawa&quot;,&quot;new zealand&quot;), xi=c(7,7,6,2,31,2), ni=c(468,296, 281,223,982,172)) #calculate new variable pi base on ratio xi/ni dat$pi &lt;- with(dat, xi/ni) #Freeman-Tukey double arcsine trasformation dat &lt;- escalc(measure=&quot;PFT&quot;, xi=xi, ni=ni, data=dat, add=0) res &lt;- rma(yi, vi, method=&quot;REML&quot;, data=dat, slab=paste(model)) #create forest plot with labels forest(res, transf=transf.ipft.hm, targs=list(ni=dat$ni), xlim=c(-1,1.5),refline=res$beta[1],cex=.8, ilab=cbind(dat$xi, dat$ni), ilab.xpos=c(-.6,-.4),digits=3) op &lt;- par(cex=.75, font=2) text(-1.0, 7.5, &quot;model &quot;,pos=4) text(c(-.55,-.2), 7.5, c(&quot;recurrence&quot;, &quot; total subjects&quot;)) text(1.4,7.5, &quot;frequency [95% CI]&quot;, pos=2) Figure 4.1: Stroke recurrence after TIA clinic par(op) Exact 95% confidence interval is provided below using the data above. This solution was provided on stack overflow. tmp &lt;- t(sapply(split(dat, dat$model), function(x) binom.test(x$xi, x$ni)$conf.int)) dat$ci.lb &lt;- tmp[,1] #adding column to data frame dat dat$ci.ub &lt;- tmp[,2] #adding column to data frame dat res &lt;- rma.glmm(measure=&quot;PLO&quot;, xi=xi, ni=ni, data=dat) par(mar=c(5,4,1,2)) with(dat, forest(yi, ci.lb=ci.lb, ci.ub=ci.ub, ylim=c(-1.5,8), xlim=c(-.5,1), refline=predict(res, transf=transf.ilogit)$pred)) addpoly(res, row=-1, transf=transf.ilogit) abline(h=0) text(-1.0, 7.5, &quot;Model&quot;, pos=4) text(c(-.8,-.2), 7.5, c(&quot;recurrence&quot;, &quot; total subjects&quot;)) text( 1, 7.5, &quot;Proportion [95% CI]&quot;, pos=2) Figure 4.2: Stroke recurrence after TIA clinic-exact 95% CI 4.6.3 Bivariate Metaanalysis The univariate method of Moses-Shapiro-Littenberg combines these measures (sensitivity and specificity) into a single measure of accuracy (diagnostic odds ratio)(Moses, Shapiro, and Littenberg 1993) . This approach has been criticized for losing data on sensitivity and specificity of the test. Similar to the univariate method, the bivariate method employs a random effect to take into account the withinstudy correlation (Reitsma et al. 2005). Additionally, the bivariate method also accounts for the between-study correlation in sensitivity and specificity. Bivariate analysi is performed using mada package. Bayesian bivariate metaanalysis will be discussed under Bayesian Analysis chapter 8.3.1. The example below is taken from a metaanalysis of spot sign as predictor expansion of intracerebral hemorrhage (Phan et al. 2019). The data for this analysis is available in the Data-Use sub-folder. SROC for different subgroups to explore their effects on the results. library(mada) ## Loading required package: mvtnorm ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs ## Loading required package: mvmeta ## This is mvmeta 0.4.11. For an overview type: help(&#39;mvmeta-package&#39;). ## ## Attaching package: &#39;mvmeta&#39; ## The following object is masked from &#39;package:metafor&#39;: ## ## blup ## ## Attaching package: &#39;mada&#39; ## The following object is masked from &#39;package:metafor&#39;: ## ## forest Dat&lt;-read.csv(&quot;./Data-Use/ss150718.csv&quot;) #remove duplicates dat&lt;-subset(Dat, Dat$retain==&quot;yes&quot;) (ss&lt;-reitsma(dat)) ## Call: reitsma.default(data = dat) ## ## Fixed-effects coefficients: ## tsens tfpr ## (Intercept) 0.2548 -1.9989 ## ## 27 studies, 2 fixed and 3 random-effects parameters ## logLik AIC BIC ## 52.0325 -94.0650 -84.1201 summary(ss) ## Call: reitsma.default(data = dat) ## ## Bivariate diagnostic random-effects meta-analysis ## Estimation method: REML ## ## Fixed-effects coefficients ## Estimate Std. Error z Pr(&gt;|z|) 95%ci.lb 95%ci.ub ## tsens.(Intercept) 0.255 0.152 1.676 0.094 -0.043 0.553 ## tfpr.(Intercept) -1.999 0.097 -20.664 0.000 -2.189 -1.809 ## sensitivity 0.563 - - - 0.489 0.635 ## false pos. rate 0.119 - - - 0.101 0.141 ## ## tsens.(Intercept) . ## tfpr.(Intercept) *** ## sensitivity ## false pos. rate ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Variance components: between-studies Std. Dev and correlation matrix ## Std. Dev tsens tfpr ## tsens 0.692 1.000 . ## tfpr 0.400 -0.003 1.000 ## ## logLik AIC BIC ## 52.033 -94.065 -84.120 ## ## AUC: 0.858 ## Partial AUC (restricted to observed FPRs and normalized): 0.547 ## ## HSROC parameters ## Theta Lambda beta sigma2theta sigma2alpha ## -1.217 2.823 -0.548 0.138 0.556 AUC(reitsma(data = dat)) ## $AUC ## [1] 0.8576619 ## ## $pAUC ## [1] 0.5474316 ## ## attr(,&quot;sroc.type&quot;) ## [1] &quot;ruttergatsonis&quot; sumss&lt;-SummaryPts(ss,n.iter = 10^3) #bivariate pooled LR summary(sumss) ## Mean Median 2.5% 97.5% ## posLR 4.740 4.720 3.820 5.76 ## negLR 0.496 0.495 0.415 0.58 ## invnegLR 2.030 2.020 1.720 2.41 ## DOR 9.680 9.600 6.740 13.50 #compare retro vs prospective study datpros&lt;-subset(dat,dat$Study.type==&quot;Prospective&quot;) #retrospective vs prospective datretro&lt;-subset(dat, dat$Study.type==&quot;Retro&quot;) #compare study with clinical data datclin&lt;-subset(dat, dat$clinical==1) #compare CTA&lt;6hr datlt6h&lt;-subset(dat, dat$CTA6hrs==&quot;yes&quot;) datgt6h&lt;-subset(dat,dat$CTA6hrs==&quot;no&quot;) #subset Du&#39;s data DuSS&lt;-subset(Dat,Dat$metaDu==&quot;yes&quot;) #14 #subset Xu data XuSS&lt;-subset(Dat,Dat$metaXu==&quot;yes&quot;) #14 #################################### srocretro&lt;-reitsma(datretro) AUC(srocretro)$AUC ## [1] 0.868405 sr1&lt;-SummaryPts(srocretro,n.iter = 1000) #mc summary(sr1) ## Mean Median 2.5% 97.5% ## posLR 5.210 5.130 3.760 7.11 ## negLR 0.441 0.436 0.319 0.57 ## invnegLR 2.320 2.290 1.750 3.14 ## DOR 12.300 11.800 6.930 20.70 #talpha fit.retro &lt;- reitsma(datretro, alphasens = 0, alphafpr = 2, method = &quot;ml&quot;) summary(fit.retro) ## Call: reitsma.default(data = datretro, alphasens = 0, alphafpr = 2, ## method = &quot;ml&quot;) ## ## Bivariate diagnostic random-effects meta-analysis ## Estimation method: ML ## ## Fixed-effects coefficients ## Estimate Std. Error z Pr(&gt;|z|) 95%ci.lb 95%ci.ub ## tsens.(Intercept) 1.909 0.254 7.511 0.000 1.411 2.407 ## tfpr.(Intercept) -4.263 0.261 -16.335 0.000 -4.775 -3.752 ## sensitivity 0.615 - - - 0.506 0.700 ## false pos. rate 0.119 - - - 0.092 0.153 ## ## tsens.(Intercept) *** ## tfpr.(Intercept) *** ## sensitivity ## false pos. rate ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Variance components: between-studies Std. Dev and correlation matrix ## Std. Dev tsens tfpr ## tsens 0.801 1.000 . ## tfpr 0.799 0.039 1.000 ## ## logLik AIC BIC ## 26.619 -43.238 -36.577 ## ## AUC: 0.812 ## Partial AUC (restricted to observed FPRs and normalized): 0.539 ## ## HSROC parameters ## Theta Lambda beta sigma2theta sigma2alpha ## -1.181 6.175 -0.002 0.332 1.230 # srocpros&lt;-reitsma(datpros) AUC(srocpros)$AUC ## [1] 0.8501042 plot(srocretro, xlim = c(0,.5), ylim = c(.5,1)) lines(sroc(srocpros), lty = 2) ROCellipse(srocpros, lty = 2, pch = 2, add = TRUE) points(fpr(datretro), sens(datretro), cex = .5) points(fpr(datpros), sens(datpros), pch = 2, cex = 0.5) legend(&quot;bottomright&quot;,c(&quot;AUC Retrospective=0.87&quot;,&quot;AUC Prospective=0.85&quot;),pch = 1:2, lty = 1:2) ################################## #compare clinical datclinno&lt;-subset(dat,dat$clinical==0) srocclin&lt;-reitsma(datclin) AUC(srocclin)$AUC ## [1] 0.8649725 srocclinno&lt;-reitsma(datclinno) AUC(srocclinno)$AUC ## [1] 0.8595582 plot(srocclin, xlim = c(0,.5), ylim = c(.5,1)) lines(sroc(srocclinno), lty = 2) ROCellipse(srocclinno, lty = 2, pch = 2, add = TRUE) points(fpr(datclin), sens(datclin), cex = .5) points(fpr(datclinno), sens(datclinno), pch = 2, cex = 0.5) legend(&quot;bottomright&quot;, c(&quot;AUC Clinical data=0.86&quot;,&quot;AUC Without clinical=0.86&quot;), pch = 1:2, lty = 1:2) ################################# #compare CTA&lt;6hr sroclt6h&lt;-reitsma(datlt6h) AUC(sroclt6h)$AUC ## [1] 0.8716562 srocgt6h&lt;-reitsma(datgt6h) AUC(srocgt6h)$AUC ## [1] 0.844387 plot(sroclt6h, xlim = c(0,.5), ylim = c(.5,1)) lines(sroc(srocgt6h), lty = 2) ROCellipse(srocgt6h, lty = 2, pch = 2, add = TRUE) points(fpr(datlt6h), sens(datlt6h), cex = .5) points(fpr(datgt6h), sens(datgt6h), pch = 2, cex = 0.5) legend(&quot;bottomright&quot;, c(&quot;AUC CTA=&lt;6 hrs=0.88&quot;, &quot;AUC CTA&gt; 6 hrs=0.84&quot;), pch = 1:2, lty = 1:2) ################################ #original Du summary(reitsma((DuSS))) ## Call: reitsma.default(data = (DuSS)) ## ## Bivariate diagnostic random-effects meta-analysis ## Estimation method: REML ## ## Fixed-effects coefficients ## Estimate Std. Error z Pr(&gt;|z|) 95%ci.lb 95%ci.ub ## tsens.(Intercept) 0.567 0.198 2.868 0.004 0.179 0.954 ## tfpr.(Intercept) -1.923 0.183 -10.489 0.000 -2.283 -1.564 ## sensitivity 0.638 - - - 0.545 0.722 ## false pos. rate 0.127 - - - 0.093 0.173 ## ## tsens.(Intercept) ** ## tfpr.(Intercept) *** ## sensitivity ## false pos. rate ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Variance components: between-studies Std. Dev and correlation matrix ## Std. Dev tsens tfpr ## tsens 0.638 1.000 . ## tfpr 0.626 -0.170 1.000 ## ## logLik AIC BIC ## 25.076 -40.151 -33.145 ## ## AUC: 0.844 ## Partial AUC (restricted to observed FPRs and normalized): 0.728 ## ## HSROC parameters ## Theta Lambda beta sigma2theta sigma2alpha ## -0.689 2.502 -0.018 0.166 0.935 #compare new Du datMetaDu&lt;-subset(dat, dat$metaDu==&quot;yes&quot;) datnotDu&lt;-subset(dat, dat$metaDu==&quot;no&quot;) srocMetaDu&lt;-reitsma(datMetaDu) a1=round(AUC(srocMetaDu)$AUC,2) srocnotDu&lt;-reitsma(datnotDu) a2=round(AUC(srocnotDu)$AUC,2) plot(srocMetaDu, xlim = c(0,.5), ylim = c(.5,1)) lines(sroc(srocnotDu), lty = 2) ROCellipse(srocnotDu, lty = 2, pch = 2, add = TRUE) points(fpr(datMetaDu), sens(datMetaDu), cex = .5) points(fpr(datnotDu), sens(datnotDu), pch = 2, cex = 0.5) legend(&quot;bottomright&quot;, c(&quot;AUC Du=0.89&quot;, &quot;AUC Later=0.83&quot;), pch = 1:2, lty = 1:2) 4.6.4 Metaregression #plot year against tsens library(ggplot2) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date ssr&lt;-as.data.frame(ss$residuals) ssr$Year&lt;-as.Date(as.character(dat$PubYear),&quot;%Y&quot;) ssr$Quality&lt;-dat$Quality.assessment ggplot(ssr, aes(x=ssr$Year,y=ssr$tsens))+geom_point()+scale_x_date()+geom_smooth(method=&quot;lm&quot;)+ ggtitle(&quot;Relationship between transformed sensitivity and Publication Year&quot;)+ labs(x=&quot;Year&quot;,y=&quot;transformed sensitivity&quot;) 4.6.4.1 Inconsistency I2 The inconsistency \\(I^2\\) index is the sum of the squared deviations from the overall effect and weighted by the study size. Value &lt;25% is classified as low and greater than 75% as high heterogeneity. This test can be performed using metafor package . The presence of high \\(I^2\\) suggests a need to proceed to meta-regression on the data to understand the source of heterogeneity. The fixed component were the covariates which were being tested for their effect on heterogeneity. The random effect components were the sensitivity and FPR. 4.6.4.2 summary Positive and Negative Likelihood Ratio Positive likelihood ratio (PLR) is the ratio of sensitivity to false positive rate (FPR); the negative (NLR) likelihood ratio is the ratio of 1-sensitivity to specificity. A PLR indicates the likelihood that a positive spot sign (test) would be expected in a patient with target disorder compared with the likelihood that the same result would be expected in a patient without target disorder. Using the recommendation by Jaeschke et al(Jaeschke, Guyatt, and Sackett 1994) a high PLR (&gt;5) and low NLR (&lt;0.2) indicate that the test results would make moderate changes in the likelihood of hematoma growth from baseline risk. PLRs of &gt;10 and NLRs of &lt;0.1 would confer very large changes from baseline risk. The pooled likelihood ratios were used to calculate post-test odds according to Bayes’ Theorem and post-test probabilities of outcome after a positive test result for a range of possible values of baseline risk. 4.6.5 Network metaanalysis Traditionally metaanalysis studies of clinical trials evaluate one intervention. Network metaanalysis is increasingly used to indirectly evaluate efficacy of different interevention (Shim et al. 2019). The Parkinson dataset is used to illustrate network metaanalysis. The dataset contains 3 different treatments. There are 801 patients in group 1 and 731 patients in group 2. library(netmeta) ## Warning: package &#39;netmeta&#39; was built under R version 3.6.3 ## Loading required package: meta ## Warning: package &#39;meta&#39; was built under R version 3.6.3 ## Loading &#39;meta&#39; package (version 4.11-0). ## Type &#39;help(meta)&#39; for a brief overview. ## ## Attaching package: &#39;meta&#39; ## The following object is masked from &#39;package:mada&#39;: ## ## forest ## The following objects are masked from &#39;package:metafor&#39;: ## ## baujat, forest, funnel, funnel.default, labbe, radial, ## trimfill ## Loading &#39;netmeta&#39; package (version 1.2-1). ## Type &#39;help(&quot;netmeta-package&quot;)&#39; for a brief overview. data(&quot;parkinson&quot;, package = &quot;netmeta&quot;) colnames(parkinson) ## [1] &quot;Study&quot; &quot;Treatment1&quot; &quot;y1&quot; &quot;sd1&quot; &quot;n1&quot; ## [6] &quot;Treatment2&quot; &quot;y2&quot; &quot;sd2&quot; &quot;n2&quot; &quot;Treatment3&quot; ## [11] &quot;y3&quot; &quot;sd3&quot; &quot;n3&quot; net1 &lt;- netmeta(y1, sd1, Treatment1, Treatment2, Study, data = parkinson, sm = &quot;MD&quot;, reference = &quot;2&quot;) # Network graph with default settings netgraph(net1) 4.7 Data simulation Data simulation is an important aspects of data science. The example below is taken from our experience trying to simulate data from recent clot retrieval trials in stroke (Berkhemer et al. 2015, @pmid25671797). Simulation is performed using simstudy library. library (simstudy) ## Loading required package: data.table ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, ## week, yday, year library(tidyverse) ## Registered S3 method overwritten by &#39;rvest&#39;: ## method from ## read_xml.response xml2 ## -- Attaching packages ------------------------------------- tidyverse 1.2.1 -- ## v tibble 3.0.1 v purrr 0.3.2 ## v tidyr 1.0.0 v dplyr 0.8.3 ## v readr 1.3.1 v stringr 1.4.0 ## v tibble 3.0.1 v forcats 0.4.0 ## Warning: package &#39;tibble&#39; was built under R version 3.6.3 ## Warning: package &#39;tidyr&#39; was built under R version 3.6.1 ## Warning: package &#39;dplyr&#39; was built under R version 3.6.1 ## -- Conflicts ---------------------------------------- tidyverse_conflicts() -- ## x lubridate::as.difftime() masks base::as.difftime() ## x dplyr::between() masks data.table::between() ## x lubridate::date() masks base::date() ## x tidyr::expand() masks Matrix::expand() ## x dplyr::filter() masks stats::filter() ## x dplyr::first() masks data.table::first() ## x data.table::hour() masks lubridate::hour() ## x lubridate::intersect() masks base::intersect() ## x data.table::isoweek() masks lubridate::isoweek() ## x dplyr::lag() masks stats::lag() ## x dplyr::last() masks data.table::last() ## x data.table::mday() masks lubridate::mday() ## x data.table::minute() masks lubridate::minute() ## x data.table::month() masks lubridate::month() ## x tidyr::pack() masks Matrix::pack() ## x data.table::quarter() masks lubridate::quarter() ## x data.table::second() masks lubridate::second() ## x lubridate::setdiff() masks base::setdiff() ## x readr::spec() masks mada::spec() ## x purrr::transpose() masks data.table::transpose() ## x lubridate::union() masks base::union() ## x tidyr::unpack() masks Matrix::unpack() ## x data.table::wday() masks lubridate::wday() ## x data.table::week() masks lubridate::week() ## x data.table::yday() masks lubridate::yday() ## x data.table::year() masks lubridate::year() #T is Trial tdef &lt;- defData(varname = &quot;T&quot;, dist = &quot;binary&quot;, formula = 0.5) #early neurological improvement (ENI) .37 in TPA and .8 in ECR #baseline NIHSS 13 in TPA and 17 in ECR tdef &lt;- defData(tdef, varname = &quot;ENI&quot;, dist = &quot;normal&quot;, formula = .8-.52*T, variance = .1) #baseline NIHSS 13 in TPA and 17 in ECR tdef &lt;- defData(tdef, varname = &quot;Y0&quot;, dist = &quot;normal&quot;, formula = 13, variance = 1) tdef &lt;- defData(tdef, varname = &quot;Y1&quot;, dist = &quot;normal&quot;, formula = &quot;Y0- 5 - 5 * T &gt;5&quot;,variance = 1) tdef &lt;- defData(tdef, varname = &quot;Y2&quot;, dist = &quot;normal&quot;, formula = &quot;Y0 - 5 - 5- 9* T&gt;0&quot;,variance = 1) tdef &lt;- defData(tdef, varname = &quot;Y3&quot;, dist = &quot;normal&quot;, formula = &quot;Y0 - 5 - 5 -2- 12 * T&gt;0&quot;, variance = 1) #male tdef &lt;- defData(tdef,varname = &quot;Male&quot;, dist = &quot;binary&quot;, formula = 0.49*T) #diabetes .23 in TPA and .06 in ECR tdef &lt;- defData(tdef,varname = &quot;Diabetes&quot;, dist = &quot;binary&quot;, formula = .23-.17*T) #HT .66 TPA vs .6 ECR tdef &lt;- defData(tdef,varname = &quot;HT&quot;, dist = &quot;binary&quot;, formula = .66-.06*T) #generate data frame dtTrial &lt;- genData(500, tdef) dtTime &lt;- addPeriods(dtTrial, nPeriods = 4, idvars = &quot;id&quot;, timevars = c(&quot;Y0&quot;, &quot;Y1&quot;, &quot;Y2&quot;,&quot;Y3&quot;), timevarName = &quot;Y&quot;) dtTime ## id period T ENI Male Diabetes HT Y timeID ## 1: 1 0 1 0.35320233 1 1 1 13.73195658 1 ## 2: 1 1 1 0.35320233 1 1 1 0.67603046 2 ## 3: 1 2 1 0.35320233 1 1 1 1.91860804 3 ## 4: 1 3 1 0.35320233 1 1 1 1.03072801 4 ## 5: 2 0 0 -0.03156351 1 0 0 13.03043429 5 ## --- ## 1996: 499 3 1 -0.07201004 0 0 1 -0.03498031 1996 ## 1997: 500 0 1 0.30876925 1 0 1 11.41679918 1997 ## 1998: 500 1 1 0.30876925 1 0 1 1.01021292 1998 ## 1999: 500 2 1 0.30876925 1 0 1 0.77090336 1999 ## 2000: 500 3 1 0.30876925 1 0 1 -0.30082826 2000 #check that 2 groups are similar at start but not at finish t.test(Y0~T,data=dtTrial) ## ## Welch Two Sample t-test ## ## data: Y0 by T ## t = -0.69711, df = 497.82, p-value = 0.4861 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2516075 0.1198213 ## sample estimates: ## mean in group 0 mean in group 1 ## 12.97239 13.03829 t.test(Y3~T,data=dtTrial) ## ## Welch Two Sample t-test ## ## data: Y3 by T ## t = 11.266, df = 496.89, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8192978 1.1654155 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.8797044 -0.1126523 t.test(Male~T,data=dtTrial) ## ## Welch Two Sample t-test ## ## data: Male by T ## t = 1.0699, df = 495.39, p-value = 0.2852 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.04008157 0.13591777 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.5228216 0.4749035 #putting the 4 time periods together dtTime &lt;- addPeriods(dtTrial, nPeriods = 3, idvars = &quot;id&quot;, timevars = c(&quot;Y0&quot;, &quot;Y1&quot;, &quot;Y2&quot;,&quot;Y3&quot;), timevarName = &quot;Y&quot;) ## Warning in addPeriods(dtTrial, nPeriods = 3, idvars = &quot;id&quot;, timevars = c(&quot;Y0&quot;, : Number of periods &lt;&gt; number of time dependent variables: ## periods based on time-dependent variables #summarise data using group_by dtTime2&lt;-dtTime %&gt;% group_by(period, T) %&gt;% summarise(meanY=mean(Y), sdY=sd(Y), upperY=meanY+sdY, lowerY=meanY-sdY) #write.csv(dtTime, file=&quot;./Data-Use/dtTime_simulated.csv&quot;) #write.csv(dtTrial, file=&quot;./Data-Use/dtTrial_simulated.csv&quot;) References "]
]
