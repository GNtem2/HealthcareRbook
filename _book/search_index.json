[["index.html", "Applications of R in Healthcare Chapter 1 Preamble", " Applications of R in Healthcare Thanh Phan 2024-02-10 Chapter 1 Preamble The book is written in the hope that clinicians and junior doctors will take a new approach towards understanding medicine and data. One of the key aspect in the journey to becoming a doctor is leaning about medical diagnosis. This is a complex process and should appropriately take into account the history and corroborating history, examination and investigations (Centor, Geha, and Manesh 2019). However, early on in medical school, students are taught list of associations and frequency of signs and symptoms related to a disease, rather than recognition of patterns. Medical students later become junior doctors and would pride themselves on the ability to generate these lists. An analogy to these lists is performing univariable regression to explore their occurrences in the condition compare to an alternative diagnosis. However, the findings from univariable regression do not convey a meaning about relationship of the variables. Similarly, the teaching that vasculitis is associated with stroke. This has led many junior doctors to search for rare association such vasculitis among young patients with stroke. In an audit over 10 years at Monash Health, there was only one case of vasculitis presenting as stroke (Kempster, McLean, and Phan 2016). Another example would be searching for temporal arteritis among elderly patients presenting with stroke. This action is performing without realising that few patients with arteritis have stroke and few patients with stroke gave arteritis. Often patients developed arteritis first and their associated symptoms. Stroke may present later in the course of arteritis and sometimes even after treatment for arteritis. By contrast, a senior clinician would discuss why certain diagnoses and not others were considered in the differential diagnoses given the occurrence of selected symptoms and signs. In statistical analysis, patterns can be found using a variety of multivariate methods (Hastie, Tibshirani, and Friedman 2009). These methods are not taught except in advance Statistics or in machine learning courses. Consequently, there is a lack of appreciations of these methods and how they can be applied in Health care. In this book a wide variety of multivariate methods will be briefly demonstrated to give the clinicians a glimpse of the possibilities. Statistics is taught in at rudimentary level in school and during specialty training. Given this lack of emphasis of an important subject, it is not surprising that students and junior doctors do not embrace statistics. When it is done it is through the use of commercial statistical software such as SPSS which has a graphical user interface (GUI) and encourage the user to perform analysis by clicking. R is a statistical program which takes a completely opposite direction. It requires the user to be able to code and understand why a task should be done. This drawback means that many students, junior doctors and clinicians do not appreciate the advantage of R: free open source software, large online community who are willing to share their codes and direct access to statisticians and bioinformaticians who write the softwares. A lot of the new ideas in statistics have libraries available in R. By contrast, it would take several years for commercial software to catch-up. R can be used to scrape data from the internet or interface with data platform such as Google Maps application programming interface (API), Youtube, Twitter. Rstudio, the integrated development environment (IDE) of R, provides platform for Shiny app development, creation of web document and writing of book (such as this one). This book takes on a non-traditional approach to teaching R. It emphasises learning R by examples. Often data science course spend time explaining how R treat data as vector and manipulate data symbolically. Data manipulation is the foundation of data science but can bore those new to R. That aspect is left to the next chapter on data wrangling. This chapter is an introduction to ggplot2. Another aspect of learning R is that the libraries come with many free dataset. Clinicians do not find the diamond or car or gapminder datasets useful as they are not related to medicine. On the other hand, the actual Titanic data, with passenger list and their fate, may be of interest. In this book we will try to use dataset which are directly related to medicine or topics of high interest such as the COVID-19 data. Some of the data provided here comes from publications by the Neurology Department, some are simulated and some have come from the internet (COVID-19) and some dataset are provided by R (eg fertility, cancer (lung, breast), leukemia, lymphoma, coronary artery disease, diabetes, hepatitis and microbiome). The dataset available in R in the following packages datasets, Stat2Data and mlbench. Additional datasets are available from external website such as Kaggle and UCI Machine learning Databases. For example, the heart disease data are available from https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/. It is encouraged that the reader visit these websites to obtain data for learning R. Dataset from these websites are labelled in this book as coming from the ExtData folder. Unless indicated, the data use in this book can be found in the Data-Use folder. Researchers working on animal dataset may find the principles of analysis described here useful for animal research. This is a book adapted from Yihui Xie’s package bookdown. The files are written in Markdown. Markdown files enable embedding of R script, the main aim of writing this book. The repository for this book is https://github.com/GNtem2/HealthcareRbook. Once completed the book will be available on Netlify. R can be installed from The Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/. It is free. The user can choose between three different platforms such as Windows, Mac (OS X) and Linux. Following installation of R, you should install Rstudio, available at https://rstudio.com/. Some of the libraries are available at Bioconductor website and not CRAN. References "],["intro.html", "Chapter 2 Introduction to R 2.1 Plot using base R 2.2 ggplot2 2.3 ggplot2 extra 2.4 plotly", " Chapter 2 Introduction to R It is often said that the majority of time is spent on data cleaning and 20% on analysis. A common trap when start using R is that the library have not been installed or the data are in different folder to the working directory. Installing library is easy with the command install.packages. The command getwd() will tell you which directory you are in. Sometimes, the user encounter issues with R. This is not an uncommon problem and often not due to serious errors but forgetting that R is case-sensitive. Also when copying codes from the net across, R does not recognise the “inverted comma” but has its own “inverted comma”. Unlike Python, R does not tolerate space between variables. R also does not like variables to be named starting with a number. R treats the variables in certain ways depend on their class. Some function requires that the variables are numeric. The Breast Cancer data from mlbench has 11 columns with the 9 covariates being factors and ordered factors. These issues will be dealt further in the chapter on data wrangling. One way to get an overview of the structure of the data is to use the glimpse function from dplyr. Another issue is that some libraries have assign certain names to their function and which is also used by other libraries. In the course of opening multiple libraries, R may be confused and use the last library open and which may lead to error result. We will illustrate this with the forest function in the Statistics chapter. A selling point of R is that there is a large community of users who often post their solutions online. it’s worth spending time on stack overflow to look at similar problems that others have and the approaches to solving them. A clue to pose questions is to look at the error. If you wish to post questions for others to help then it’s encouraged that the question include some data so that the person providing the solution can understand the nature of the errors. Useful blogs are also available at https://www.r-bloggers.com/. This chapter focusses on the ggplot2 and its related libraries for plotting (Wickham 2016). Base R also has plotting function but lacks the flexibility of ggplot2. Plotting is introduced early to enage clinicians who may not have the patience read the following chapter on data wrangling prior. The book is not intended to be used in a sequential fashion as the reader may find elements of this chapter relevant to them and jump to another chapter such as chapter 3 on statistics. 2.1 Plot using base R Below we illustrate bar plot using base R. data(&quot;Leukemia&quot;, package=&quot;Stat2Data&quot;) #AML dataset-treatment colnames(Leukemia) ## [1] &quot;Age&quot; &quot;Smear&quot; &quot;Infil&quot; &quot;Index&quot; &quot;Blasts&quot; &quot;Temp&quot; &quot;Resp&quot; &quot;Time&quot; &quot;Status&quot; #base R hist(Leukemia$Age, 8, xlab = &quot;Age&quot;,main=&quot;Acute Myeloid Leukemia Dataset&quot;) Line plot can be performed in base R using abline argument. The font is defined by cex argument and colour defined by col argument. The title is defined by main argument. The X and Y axes can be labelled using xlab and ylab arguments within the plot function. Below we illustrate example of drawing a line to a point and in the segment below we illustrate example of drawing a line. #set parameters of plot X=seq(0,1,by=.1) #define a set of point from 0 to 1, separated by 0.1. Y=seq(0,1,by=.1) #define A and B A=.065; B=.44 #location of point plot(X,Y, main=&quot;ROC curve&quot;) points(A,B,pch=8,col=&quot;red&quot;,cex=2) #add point abline(coef = c(0,1)) #add diagonal line #draw line to a point segments(x0=0,y0=0,x1=A,y1=B,col=&quot;blue&quot;) segments(x0=A,y0=B,x1=1,y=1,col=&quot;blue&quot;) This is an illustration of using base R to plot regression line. Later, we will illustrate using geom_smooth call from ggplot2. TPR_1=.44; FPR_1=.065 plot(X,Y,main=&quot;Likelihood ratio graph&quot;, xlab=&quot;1-Specificity&quot;,ylab=&quot;Sensitivity&quot;,cex=.25) points(.02,.8,pch=8,col=&quot;red&quot;,cex=2) #add point df1&lt;-data.frame(c1=c(0,TPR_1),c2=c(0,FPR_1)) reg1&lt;-lm(c1~c2,data=df1) df2&lt;-data.frame(c1=c(TPR_1,1),c2=c(FPR_1,1)) reg2&lt;-lm(c1~c2,data=df2) abline(reg1) #draw line using coefficient reg1 abline(reg2) #draw line using coefficient reg2 text(x=FPR_1,y=TPR_1+.3,label=&quot;Superior&quot;,cex=.7) text(x=FPR_1+.2,y=TPR_1+.2,label=&quot;Absence&quot;,cex=.7) text(x=.0125,y=TPR_1-.1,label=&quot;Presence&quot;,cex=.7) text(x=FPR_1+.1,y=TPR_1,label=&quot;Inferior&quot;,cex=.7) Function can be plotted using variations of the above. This requires a formula to describe variable Y. Shading in base R is performed with the polygon function. AUC_Logistic&lt;-function (A,B,C,D){ #binary data #A=True pos %B=False positive %C=False negative %D=True negative TPR=A/(A+C) FPR=1-(D/(D+B)) #binomial distribution sqrt(np(1-p)) #Statist. Med. 2002; 21:1237-1256 (DOI: 10.1002/sim.1099) STDa=sqrt((A+C)*TPR*(1-TPR)); STDn=sqrt((B+D)*FPR*(1-FPR)); a=STDn/STDa; theta=log((TPR/(1-TPR))/((FPR/(1-FPR))^a)); #define a set of point from 0 to 1, separated by 0.001. X=seq(0,1,by=0.001) #logistic regression model Y1=(X^a)/(X^a+(((1-X)^a)*exp(-1*theta))); AUC=round(pracma::trapz(X,Y1),2) AUC #SE using Hanley &amp; McNeil #Preferred method if more than one TPR,FPR data point known #Hanley is less conservative than Bamber Nn=B+D; Na=A+C; Q1=AUC/(2-AUC); Q2=2*AUC^2/(1+AUC); SEhanley=sqrt(((AUC*(1-AUC))+((Na-1)*(Q1-AUC^2))+((Nn-1)*(Q2-AUC^2)))/(Na*Nn)) #SE using Bamber #Ns is the number of patients in the smallest group if (A+C&gt;B+D) { Ns=B+D } else { Ns=A+C } SEbamber=sqrt((AUC*(1-AUC))/(Ns-1)) # plot smoothed ROC plot(X,Y1,main=&quot;ROC curve&quot;, xlab=&quot;1-Specificity&quot;,ylab=&quot;Sensitivity&quot;,cex=.25) points(FPR,TPR,pch=8,col=&quot;red&quot;,cex=2) #add point Y2=0 polygon(c(X[X &gt;= 0 &amp; X &lt;= 1], rev(X[X &gt;= 0 &amp; X &lt;= 1])), c(Y1[X &gt;= 0 &amp; X &lt;= 1], rev(Y2[X &gt;= 0 &amp; X &lt;= 1])), col = &quot;#6BD7AF&quot;) print(paste(&quot;The Area under the ROC curve using the logistic function is&quot;, AUC,&quot;. The Area under the ROC curve using rank sum method is&quot;, round(.5*(TPR+(1-FPR)),2))) } AUC_Logistic(10,20,2,68) ## [1] &quot;The Area under the ROC curve using the logistic function is 0.84 . The Area under the ROC curve using rank sum method is 0.8&quot; 2.2 ggplot2 The plot below uses ggplot2 or grammar of graphics. The plot is built layer by layer like constructing a sentence. Plotting is a distinct advantage of R over commercial software with GUI (graphical user interface) like SPSS. A wide variety of media organisations (BBC, Economist) are using ggplot2 with their own stylised theme. The plot has a certain structure such the name of the data and aesthetics for x and y axes. For illustration purpose the aesthetics are labelled with x and y statements. The fill argument in the aesthetics indicate the variables for coloring. The colors chosen for this graph were imposed by the journal Epilepsia (Seneviratne et al. 2019). To run the examples, check that you have install the libraries. an error can occurred if you don’t have the required library. The meta-character # is used to signal that the line is meant to comment the code ie R will not read it. The install.packages command only need to be run once. 2.2.1 Histogram The flexibility of ggplot2 is shown here in this histogram. The legend can be altered using the scale_fill_manual function. If other colours are preferred then under values add the preferred colours. There are different ways to use ggplot2: quick plot or qplot with limited options and full ggplot2 with all the options. The choice of the method depends on individual preference and as well as reason for plotting. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.2 #qplot qplot(Age, data=Leukemia, bins=8) ## Warning: `qplot()` was deprecated in ggplot2 3.4.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Now a more complex version of histogram with ggplot with color added. #ggplot2 ggplot(data=Leukemia,aes(Age,fill=as.factor(Resp)))+ geom_histogram(bins=8) Note the legend is a bit untidy. The legend can be changed using scale_fill_manual. The color can be specified using rgb argument. This is important as some journals prefer certain color. #adding legend, changing the values 0 and 1 to treatment response ggplot(data=Leukemia,aes(Age,fill=as.factor(Resp)))+ geom_histogram(bins=8)+ scale_fill_manual(name=&quot;Response&quot;,values=c(&quot;#999999&quot;,&quot;#56B4E9&quot;), breaks=c(0,1),labels=c(&quot;No Treatment&quot;,&quot;Treatment&quot;)) Adding title is easy with ggtitle. #adding title ggplot(data=Leukemia,aes(Age,fill=as.factor(Resp)))+ geom_histogram(bins=8)+ scale_fill_manual(name=&quot;Response&quot;, values=c(&quot;#555555&quot;,&quot;#56B4E9&quot;), breaks=c(0,1), labels=c(&quot;No Treatment&quot;,&quot;Treatment&quot;))+ ggtitle(&quot;Acute Myeloid Leukemia Treatment dataset&quot;) 2.2.2 Bar plot Previously, we had used base R for bar plot. Here we use the geom_bar argument in ggplot. library(ggplot2) library(extrafont) ## Registering fonts with R #this data came from a paper published in Epilespia 2019 on cost of looking #after patients with pseudoseizure ## Lower Upper ## percentage 0.95 0.95 ## Duration_PNES.lmg 0.0974 0.0057 0.2906 ## pseudostatus.lmg 0.2283 0.0687 0.3753 ## Hx_anxiety.lmg 0.0471 0.0057 0.1457 ## Hx_depression.lmg 0.0059 0.0041 0.1082 ## DSP.lmg 0.0582 0.0071 0.1500 ## seizure_burden.lmg 0.0179 0.0041 0.1058 ## sex.lmg 0.0413 0.0030 0.1519 df&lt;-data.frame(&quot;Predictors&quot;=c(&quot;Duration_PNES&quot;,&quot;pseudostatus&quot;,&quot;Hx_anxiety&quot;, &quot;Hx_depression&quot;,&quot;DSP&quot;,&quot;seizure_burden&quot;,&quot;sex&quot;), &quot;Importance&quot;=c(0.09737,0.22825, 0.047137,0.00487,0.058153,0.01786,0.04131), &quot;Lower.95&quot;=c(0.0057,0.0687,0.0057,0.0041,0.0071,0.0041,0.0030), &quot;Upper.95&quot;=c(0.2906,0.3753,0.1457,0.1082,0.1500,0.1058,0.1519)) #check dimensions of data frame dim(df) ## [1] 7 4 #check variables in data frame colnames(df) ## [1] &quot;Predictors&quot; &quot;Importance&quot; &quot;Lower.95&quot; &quot;Upper.95&quot; #bar plot uses geom_bar ggplot(df, aes(x=Predictors,y=Importance))+ geom_bar(stat=&quot;identity&quot;) This bar plot may be considered as untidy as the variables have not been sorted. Reordering the data requires ordering the factors. The colors were requested by the journal Epilesia in order to avoid recognitin of the bar from color blindness. #reordering the data df3&lt;-df2&lt;-df df3$Predictors&lt;-factor(df2$Predictors, levels=df2[order(df$Importance),&quot;Predictors&quot;]) #adding color p&lt;-ggplot(df3, aes(x=Predictors,y=Importance,fill=Predictors))+ geom_bar(colour=&quot;black&quot;, stat=&quot;identity&quot;, fill= c(&quot;#e4b84b&quot;,&quot;#ce8080&quot;,&quot;#511c23&quot;,&quot;#e37c1d&quot;,&quot;#ffde75&quot;,&quot;#abb47d&quot;,&quot;#a1c5cb&quot;)) p This bar plot is now ordered but the labels on the axis seem to run into each other. One solution is to tile the axis title using element_text. Note that the text size can also be specified within this argument. #rotate legend on x axis label by 45 p+theme(axis.title.y = element_text(face=&quot;bold&quot;, size=12), axis.title.x = element_text(face=&quot;bold&quot;, size=12), axis.text.x = element_text(angle=45, vjust=0.5, size=10)) The title can be broken up using the backward slash. #adding break in title p1&lt;-p+geom_errorbar(aes(ymin=Lower.95,ymax=Upper.95,width=0.2))+ labs(y=&quot;R2 exaplained (%)&quot;)+ theme(text=element_text(size=10))+ ggtitle(&quot; Relative Importance of Regressors \\n Cost among patients with non-epileptic seizure&quot;) p1 2.2.3 Pie chart This example uses the data above on contribution of non-epileptic seizure variables to hospitalised cost. library(ggplot2) df3$Percentage=round(df3$Importance/sum(df3$Importance)*100,0) ggplot(df3, aes(x=&quot;&quot; ,y=Percentage,fill=Predictors))+ geom_bar(stat=&quot;identity&quot;, width=1, color=&quot;white&quot;) + coord_polar(&quot;y&quot;, start=0) + theme_void() 2.2.4 Scatter plot The above data is used here to illustrate scatter plot. We can denote the color difference among the Predictors by adding color argument in the aesthetics. #color in qplot qplot(data=df3, Predictors, Importance,color=Predictors) Adding color in ggplot is the same as in qplot. #color ggplot ggplot(df3, aes(x=Predictors,y=Importance,color=Predictors))+ geom_point()+ theme(axis.title.y = element_text(face=&quot;bold&quot;, size=10), axis.title.x = element_text(face=&quot;bold&quot;, size=10), axis.text.x = element_text(angle=45, vjust=0.5, size=10)) The size argument within aes can be used like color. In this case, it’s used to denote the importance of the predictors. #size ggplot(df3, aes(x=Predictors,y=Importance,color=Predictors,size=Predictors))+ geom_point()+ theme(axis.title.y = element_text(face=&quot;bold&quot;, size=12), axis.title.x = element_text(face=&quot;bold&quot;, size=12), axis.text.x = element_text(angle=45, vjust=0.5, size=12)) ## Warning: Using size for a discrete variable is not advised. This is a more complicated example of scatter plot combined with formula of regression line. The paste0 function is used to add the equation to the plot. The data comes from GBD 2016 publication on lifetime risk of stroke. A comparison with plotting from base R is also provided. library(tidyverse) ## ── Attaching core tidyverse packages ────────────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ lubridate 1.9.2 ✔ tibble 3.2.1 ## ✔ purrr 1.0.1 ✔ tidyr 1.3.0 ## ── Conflicts ──────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::combine() masks Biobase::combine(), BiocGenerics::combine() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ ggplot2::Position() masks BiocGenerics::Position(), base::Position() ## ✖ dplyr::slice() masks oro.nifti::slice() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors load(&quot;./Data-Use/world_stroke.Rda&quot;) #fitting a regression model fit&lt;-lm(MeanLifetimeRisk~LifeExpectancy,data=world_sfdf) fitsum&lt;-summary(fit) #base R scatter plot with fitted line x=world_sfdf$LifeExpectancy #define x y=world_sfdf$MeanLifetimeRisk #define y plot(x,y, data=world_sfdf, main = &quot;Lifetime Stroke Risk&quot;, xlab = &quot;Life Expectancy&quot;, ylab = &quot;Life time Risk&quot;, pch = 19) abline(lm(y ~ x, data = world_sfdf), col = &quot;blue&quot;) The ggplot version is now provided. Note that the line is fitted in the geom_smooth argument. An interesting aspect of this plot is that the data can be describe as heterosecadic in which the variance changes throughout the plot. #ggplot2 scatter plot with fitted line SR&lt;-ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ geom_smooth(method=&quot;lm&quot;)+geom_point()+ xlab(&quot;Life Expectancy&quot;)+ ggtitle(paste0(&quot;Life time Risk&quot;, &quot;=&quot;, round(fitsum$coefficients[1],2),&quot;+&quot;, round(fitsum$coefficients[2],2),&quot; x &quot;,&quot;Life Expectancy&quot;)) SR ## `geom_smooth()` using formula = &#39;y ~ x&#39; To use the name of the country as label, we use geom_text. The world_sfdf data is now partitioned to show only data from Europe. An interesting pattern emerge. There is clumping of the data around Belgium and Portugal. The nudge_x and nudge_y function are used to adjust the labels and the size argument adjust the label. library(tidyverse) library(sf) ## Registered S3 methods overwritten by &#39;proxy&#39;: ## method from ## print.registry_field registry ## print.registry_entry registry ## Linking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE world_sfdf %&gt;% filter(Continent==&quot;EUROPE&quot;) %&gt;% ggplot( aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ geom_smooth(method=&quot;lm&quot;)+ xlab(&quot;Life Expectancy&quot;)+ geom_text(aes(label=Country, nudge_x=.35, nudge_y=.5, avoid_overlap=T), size=2) ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## `geom_smooth()` using formula = &#39;y ~ x&#39; In order to understand the reason for deviations from the fitted line above, it is possible possible to add additional step to explore the relationship for each income group. This graph illustrates that the high income countries have a ceiling in the relationship between lifetime risk and life expectancy from age of 70 onward. SRIncome&lt;-ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ geom_smooth(method=&quot;lm&quot;, aes(group=Income, linetype=Income, colour= Income))+ geom_point()+ xlab(&quot;Life Expectancy&quot;) SRIncome ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.2.5 arrange plot in grids Plots can be arrange in tabular format for presentation or journal submission.In base R multiple plots can be combined using par function and specify the number of columns by mfrow. The number of columns can be specified with ncol call when using gridExtra library. #Leukemia data par(mfrow=c(1,2)) #row of 1 and 2 columns x=Leukemia$Age #define x y=Leukemia$Blasts #define y plot(x,y, data=Leukemia, main = &quot;Leukemia data&quot;, xlab = &quot;Age&quot;, ylab = &quot;Blasts&quot;, pch = 19) abline(lm(y ~ x, data = Leukemia), col = &quot;blue&quot;) y1=Leukemia$Smear plot(x,y1, data=Leukemia, main = &quot;Leukemia data&quot;, xlab = &quot;Age&quot;, ylab = &quot;Smear&quot;, pch = 19) abline(lm(y1 ~ x, data = Leukemia), col = &quot;blue&quot;) library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:Biobase&#39;: ## ## combine ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## combine SRContinent&lt;-ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ geom_smooth(method=&quot;lm&quot;, aes(group=Continent, linetype=Continent, colour= Continent))+ geom_point()+ xlab(&quot;Life Expectancy&quot;)+ ylim(c(0,50)) grid.arrange(SRIncome, SRContinent, ncol=1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; An alternative way to display multiple plots is to use patchwork library. library(patchwork) SRIncome/SRContinent ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; 2.2.6 Line plot The data below is generated in the section on data simulation. The data were simulated using summary data from recent clot retrieval trials in stroke Campbell et al. (2015) library(ggplot2) library(dplyr) dtTime&lt;-read.csv(&quot;./Data-Use/dtTime_simulated.csv&quot;) dtTrial&lt;-read.csv(&quot;./Data-Use/dtTrial_simulated.csv&quot;) #summarise data using group_by dtTime2&lt;-dtTime %&gt;% group_by(period, T) %&gt;% summarise(meanY=mean(Y), sdY=sd(Y), upperY=meanY+sdY, lowerY=meanY-sdY) ## `summarise()` has grouped output by &#39;period&#39;. You can override using the `.groups` ## argument. #individual line plot ggplot(dtTime,aes(x=as.factor(period),y=Y))+ geom_line(aes(color=as.factor(T),group=id))+ scale_color_manual(values = c(&quot;#e38e17&quot;, &quot;#8e17e3&quot;)) + xlab(&quot;Time&quot;)+ylab(&quot;NIHSS&quot;) The line plot can also be represented as boxplot without the connecting lines. #box plot gg&lt;-ggplot(dtTime,aes(x=as.factor(period),y=Y))+ geom_boxplot(aes(color=as.factor(T)))+ xlab(&quot;Time&quot;)+ylab(&quot;NIHSS&quot;) gg+scale_fill_discrete(name=&quot;Treatment&quot;) To perform line plot on the grouped data, first fit the regression line to the grouped data. #linear regression Y1 predict Y2 where Y1 and Y2 are grouped data #from the simulated data above. fit&lt;-lm(Y1~Y0+T, data=dtTrial) dtTrial2&lt;-filter(dtTrial, T==1) fit2&lt;-lm(Y2~Y1, data=dtTrial2) #line plot by group pd &lt;- position_dodge2(width = 0.2) # move them .2 to the left and right gbase = ggplot(dtTime2, aes(y=meanY, colour=as.factor(T))) + geom_errorbar(aes(ymin=lowerY, ymax=upperY), width=.3, position=pd)+ geom_point(position=pd) gline = gbase + geom_line(position=pd) dtTime2$period=as.numeric(dtTime2$period) unique((dtTime2$period)) ## [1] 0 1 2 3 gline = gline %+% dtTime2 print(gline + aes(x=period)) 2.2.7 Facet wrap Facet wrap is a good way to visually explore different aspects pf the data. Using the dtTime data above, the plots are separated by trial assignment. ggplot(dtTime,aes(x=as.factor(period),y=Y))+ geom_line(aes(color=as.factor(T),group=id))+ scale_color_manual(values = c(&quot;#e38e17&quot;, &quot;#8e17e3&quot;))+ facet_wrap(~T)+ xlab(&quot;Time&quot;)+ylab(&quot;NIHSS&quot;) 2.2.8 Polygons The geom_polygon is often used in thematic plot of maps. It can be used to show polygons outside of map. It requires one data frame for coordinate and another for the values. #simulate data ids &lt;- factor(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;,&quot;5&quot;,&quot;6&quot;)) values &lt;- data.frame( id = ids, value = c(3, 3.1, 3.2, 3.3,3.4,3.5) ) a=seq(0,1.5,by=0.5) x=c(a,a-.5,a+.5,a+.2, a-.3,a+.1) positions &lt;- data.frame( id = rep(ids, each = 4), x=c(a,a-.5,a+.5,a+.2, a-.3,a+.1), y=sample(x) ) # Currently we need to manually merge the two together comb &lt;- merge(values, positions, by = c(&quot;id&quot;)) p &lt;- ggplot(comb, aes(x = x, y = y)) + geom_polygon(aes(fill = value, group = id)) p 2.2.9 Gantt chart Gantt chart can be used to illustrate project timeline. It needs a minimum of 4 data columns: Activity, Project, a start date and end date. This example below is meant as a template. If you have 6 rows of data then add 2 extra rows of data including colours. library(tidyverse) gantt_df&lt;-data.frame(item=seq(1:4), activity=c(&quot;Ethics submission&quot;,&quot;Length&quot;,&quot;Recruitment&quot;,&quot;Follow up&quot;), category=c(&quot;Ethics&quot;,&quot;Duration&quot;,&quot;Recruitment&quot;,&quot;Follow up&quot;), Start=c(&quot;2020-06-01&quot;,&quot;2021-01-01&quot;,&quot;2021-01-01&quot;,&quot;2022-01-01&quot;), End=c(&quot;2021-01-01&quot;,&quot;2023-01-01&quot;,&quot;2022-01-01&quot;,&quot;2023-01-01&quot;)) #Set factor level to order the activities on the plot gantt_df &lt;- mutate(gantt_df, activity=factor(activity,levels=activity[1:nrow(gantt_df)]), category=factor(category,levels=category[1:nrow(gantt_df)])) plot_gantt &lt;- qplot(ymin = Start, ymax = End, x = activity, colour = category, geom = &quot;linerange&quot;, data = gantt_df, size = I(10)) + #width of line scale_colour_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;yellow&quot;)) + coord_flip() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Project plan&quot;) plot_gantt 2.2.10 Heatmap The ggplot2 library can also be used for creating heatmap. This plot uses the geom_tile function. library(ggplot2) library(plyr) ## ------------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact library(reshape) ## ## Attaching package: &#39;reshape&#39; ## The following objects are masked from &#39;package:plyr&#39;: ## ## rename, round_any ## The following object is masked from &#39;package:lubridate&#39;: ## ## stamp ## The following object is masked from &#39;package:dplyr&#39;: ## ## rename ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, smiths library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor #swiss fertility dataset from 1888 data(swiss, package = &quot;datasets&quot;) swiss$swiss_canton&lt;-row.names(swiss) #assign column name to row name rownames(swiss)&lt;-NULL #remove row name data.m &lt;- melt(swiss) ## Using swiss_canton as id variables data.m &lt;- ddply(data.m, .(variable), transform, rescale = rescale(value)) q &lt;- ggplot(data.m, aes(variable, swiss_canton)) + geom_tile(aes(fill = rescale), colour = &quot;white&quot;)+ scale_fill_gradient(low = &quot;white&quot;, high = &quot;steelblue&quot;)+ ggtitle(&quot;Swiss Fertility Data 1888&quot;) q 2.3 ggplot2 extra The following plots retains the framework of ggplot2. Their uses require installing additional libraries. 2.3.1 Alluvial and Sankey diagram The Sankey flow diagram uses the width of the arrow used to indicate the flow rate. It is often used to indicate energy dissipation in the system. There are several libraries providing Sankey plot such as networkD3 library. Alluvial plot is a subset of Sankey diagram but differs in having a structured workflow. The ggalluvial library is chosen here for demonstration as it forms part of the ggplot2 framework. library(ggalluvial) library(Stat2Data) data(&quot;Leukemia&quot;) #treatment of leukemia #partition Age into 8 ordered factors Leukemia$AgeCat&lt;-cut_interval(Leukemia$Age, n=8, ordered_result=TRUE) #axis1 ggplot(data=Leukemia, aes (y=Smear,axis1=AgeCat, axis2=Resp,axis3=Status))+ geom_alluvium(aes(fill=AgeCat),width = 1/12)+ geom_label(stat = &quot;stratum&quot;, infer.label = TRUE) + geom_label(stat = &quot;stratum&quot;, infer.label = TRUE)+ scale_x_discrete(limits = c(&quot;AgeCat&quot;,&quot;Resp&quot;, &quot;Status&quot;),label=c(&quot;Age Category&quot;,&quot;Treatment Response&quot;,&quot;Mortality&quot;), expand = c(.1, .1)) + scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Set1&quot;) + ggtitle(&quot;Outcome after Leukemia Treatment&quot;) 2.3.2 Survival plot The survminer library extends ggplot2 style to survival plot. It requires several libraries such as survival for survival analysis and lubridate to parse time. A description of survival analysis is provided in the Statistics section. library(survminer) ## Loading required package: ggpubr ## ## Attaching package: &#39;ggpubr&#39; ## The following object is masked from &#39;package:plyr&#39;: ## ## mutate library(lubridate) library(survival) ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:survminer&#39;: ## ## myeloma data(cancer, package=&quot;survival&quot;) #data from survival package on NCCTG lung cancer trial #https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html #time in days #status cesored=1, dead=2sex: #sex:Male=1 Female=2 sfit&lt;- survfit(Surv(time, status) ~ sex, data = cancer) ggsurvplot(sfit, data=cancer, surv.median.line = &quot;hv&quot;, pval=TRUE, pval.size=3, conf.int = TRUE, legend.labs=c(&quot;Male&quot;,&quot;Female&quot;),xlab=&quot;Time (Days)&quot;, break.time.by=50, font.x=5, font.y=5, ggtheme = theme_bw(), risk.table=T, risk.table.font=2, #adjust font risk.table.height=.3 #adjust table height ) 2.3.3 ggraph and tidygraph The igraph library does the heavy lifting in graph theory analysis. This aspect will be expanded on in the chapter on Graph Theory. However, the plotting function with igraph is still not optimal. The ggraph and tidygraph libraries extend the ggplot2 style to graph. library(tidygraph) ## ## Attaching package: &#39;tidygraph&#39; ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following objects are masked from &#39;package:plyr&#39;: ## ## arrange, mutate, rename ## The following object is masked from &#39;package:oro.nifti&#39;: ## ## slice ## The following object is masked from &#39;package:stats&#39;: ## ## filter library(ggraph) #relationship among members of acute stroke team tpa&lt;-readr::read_csv(&quot;./Data-Use/TPA_edge010817.csv&quot;)%&gt;% rename(from=V1, to=V2) ## Rows: 25 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): V1, V2 ## dbl (2): time.line, time.capacity ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #node by degree centrality graph&lt;-as_tbl_graph(tpa) %&gt;% mutate(degree = centrality_degree()) ggraph(graph, layout = &#39;fr&#39;) + geom_edge_link() + #label size by degree centrality geom_node_point(aes(size=degree))+ #label node geom_node_text(aes(label=name),repel=T)+ ggtitle(&quot;Acute Stroke Team Network&quot;) ## Warning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` in the `default_aes` field and elsewhere instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. 2.3.4 ggparty-decision tree Decision tree can be plotted using ggplot2 and ggparty framework. This example uses data from the Breast Cancer dataset. It explores the effect of different histological features on class of breast tissue type. library(ggparty) ## Loading required package: partykit ## Loading required package: grid ## Loading required package: libcoin ## Loading required package: mvtnorm ## ## Attaching package: &#39;partykit&#39; ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## width ## ## Attaching package: &#39;ggparty&#39; ## The following object is masked from &#39;package:ggraph&#39;: ## ## geom_node_label library(partykit) library(tidyverse) data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) #check data type str(BreastCancer) ## &#39;data.frame&#39;: 699 obs. of 11 variables: ## $ Id : chr &quot;1000025&quot; &quot;1002945&quot; &quot;1015425&quot; &quot;1016277&quot; ... ## $ Cl.thickness : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 5 5 3 6 4 8 1 2 2 4 ... ## $ Cell.size : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 4 1 8 1 10 1 1 1 2 ... ## $ Cell.shape : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 4 1 8 1 10 1 2 1 1 ... ## $ Marg.adhesion : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 1 5 1 1 3 8 1 1 1 1 ... ## $ Epith.c.size : Ord.factor w/ 10 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 2 7 2 3 2 7 2 2 2 2 ... ## $ Bare.nuclei : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 10 2 4 1 10 10 1 1 1 ... ## $ Bl.cromatin : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 3 3 3 3 3 9 3 3 1 2 ... ## $ Normal.nucleoli: Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 1 7 1 7 1 1 1 1 ... ## $ Mitoses : Factor w/ 9 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 5 1 ... ## $ Class : Factor w/ 2 levels &quot;benign&quot;,&quot;malignant&quot;: 1 1 1 1 1 2 1 1 1 1 ... BC&lt;- BreastCancer %&gt;% select(-Id) treeBC&lt;-ctree(Class~., data=BC, control = ctree_control(testtype = &quot;Teststatistic&quot;)) #plot tree using plot from base R plot(treeBC) #plot tree using ggparty ggparty(treeBC) + geom_edge() + geom_edge_label() + geom_node_label(aes(label = splitvar), ids = &quot;inner&quot;) + geom_node_label(aes(label = info), ids = &quot;terminal&quot;) ggparty(treeBC) + geom_edge() + geom_edge_label() + # map color to level and size to nodesize for all nodes geom_node_splitvar(aes(col = factor(level), size = nodesize)) + geom_node_info(aes(col = factor(level), size = nodesize)) 2.3.5 Map Several simple examples are provided here. They illustrate the different plotting methods used according to the type of data. It is important to check the structure of the data using class() function. 2.3.5.1 Thematic map The ggplot2 library can also be used to generate thematic (choropleth) map. Here we use map_data function from ggplot2 to obtain a map of USA. Geocoded data are contained in the long and lat columns. The US map data is in standard dataframe format. In this case, the geom_map function is used for mapping. The USArrests data contains a column for murder, assault, rape and urban population. The assault data presented here is normalised by the population data. This section will be expanded further in the Geospatial Analysis chapter. library(dplyr) library(ggplot2) arrest&lt;-data(&quot;USArrests&quot;) arrest&lt;-USArrests%&gt;% add_rownames(&quot;region&quot;) %&gt;% mutate(region=tolower(region)) ## Warning: `add_rownames()` was deprecated in dplyr 1.0.0. ## ℹ Please use `tibble::rownames_to_column()` instead. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. US &lt;- map_data(&quot;state&quot;) map_assault&lt;-ggplot()+ geom_map(data=US, map=US, aes(x=long, y=lat, map_id=region), fill=&quot;#ffffff&quot;, color=&quot;#ffffff&quot;, size=0.15)+ #add USArrests data here geom_map(data=arrest, map=US, aes(fill=Assault/UrbanPop, map_id=region), color=&quot;#ffffff&quot;, size=0.15)+ scale_fill_continuous(low=&#39;thistle2&#39;, high=&#39;darkred&#39;, guide=&#39;colorbar&#39;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## Warning in geom_map(data = US, map = US, aes(x = long, y = lat, map_id = region), : ## Ignoring unknown aesthetics: x and y map_assault This is a more complex example and uses state by state COVID-19 data CDC website. Steps to extract the COVID-10 is shown in the next chapter. The shapefile for USA can also be extracted from tigris library. A challenge with plotting a map of US is that the country extends far North to Alaska and East to pacific islands. library(ggplot2) library(dplyr) covid&lt;-read.csv(&quot;./Data-Use/Covid_bystate_Table130420.csv&quot;) %&gt;% mutate(region=str_to_lower(Jurisdiction)) map_covid&lt;-ggplot()+ geom_map(data=US, map=US, aes(x=long, y=lat, map_id=region), fill=&quot;#ffffff&quot;, color=&quot;#ffffff&quot;, size=0.15)+ #add covid data here geom_map(data=covid, map=US, aes(fill=CumulativeIncidence31.03.20, map_id=region), color=&quot;#ffffff&quot;, size=0.15)+ scale_fill_continuous(low=&#39;thistle2&#39;, high=&#39;darkred&#39;, guide=&#39;colorbar&#39;) ## Warning in geom_map(data = US, map = US, aes(x = long, y = lat, map_id = region), : ## Ignoring unknown aesthetics: x and y map_covid In the simple example below we will generate a map of Australian State territories color by size of area. The ggplot2 combines with sf library and uses the shape file data in the geom_sf call. library(ggplot2) library(sf) #shape file Aust&lt;-st_read(&quot;./Data-Use/GCCSA_2016_AUST.shp&quot;) ## Reading layer `GCCSA_2016_AUST&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\GCCSA_2016_AUST.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 34 features and 5 fields (with 18 geometries empty) ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 96.81694 ymin: -43.74051 xmax: 167.998 ymax: -9.142176 ## Geodetic CRS: GDA94 colnames(Aust) #find out column variables ## [1] &quot;GCC_CODE16&quot; &quot;GCC_NAME16&quot; &quot;STE_CODE16&quot; &quot;STE_NAME16&quot; &quot;AREASQKM16&quot; &quot;geometry&quot; ggplot() + geom_sf(data=Aust,aes(fill=AREASQKM16))+ labs(x=&quot;Longitude (WGS84)&quot;, y=&quot;Latitude&quot;, title=&quot;Map of Australia&quot;) + theme_bw() 2.3.5.2 tmap The tmap library works in conjunction with ggplot2 and sf. The tm_shape function takes in the shape data. The tm_polygon function color the shape file with the column data of interest. library(tmap) ## The legacy packages maptools, rgdal, and rgeos, underpinning the sp package, ## which was just loaded, will retire in October 2023. ## Please refer to R-spatial evolution reports for details, especially ## https://r-spatial.org/r/2023/05/15/evolution4.html. ## It may be desirable to make the sf package available; ## package maintainers should consider adding sf to Suggests:. ## The sp package is now running under evolution status 2 ## (status 2 uses the sf package in place of rgdal) load(&quot;./Data-Use/world_stroke.Rda&quot;) #data from GBD 2016 investigators colnames(world_sfdf) ## [1] &quot;FIPS&quot; &quot;ISO2&quot; ## [3] &quot;ISO3&quot; &quot;UN&quot; ## [5] &quot;NAME&quot; &quot;AREA&quot; ## [7] &quot;POP2005&quot; &quot;REGION&quot; ## [9] &quot;SUBREGION&quot; &quot;LON&quot; ## [11] &quot;LAT&quot; &quot;MeanLifetimeRisk&quot; ## [13] &quot;Country code&quot; &quot;y2015&quot; ## [15] &quot;Continent&quot; &quot;Region, subregion, country or area *&quot; ## [17] &quot;18+&quot; &quot;Code&quot; ## [19] &quot;Region&quot; &quot;Income&quot; ## [21] &quot;LendingCategory&quot; &quot;Other&quot; ## [23] &quot;LifeExpectancy&quot; &quot;MeanLifetimeRiskH&quot; ## [25] &quot;MeanLifetimeRiskI&quot; &quot;geometry&quot; ## [27] &quot;Country&quot; class(world_sfdf) #contains simple features ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; #map of country income m&lt;-tm_shape(world_sfdf, projection = &quot;+proj=eck4&quot;)+tm_polygons(&quot;Income&quot;) m ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() #save object as png #tmap_save(m,file=&quot;world_income.png&quot;&quot;) #save as leaflet object #tmap_save(m,file=&quot;world_income.html&quot;&quot;) map of lifetime stroke risk n&lt;-tm_shape(world_sfdf, projection = &quot;+proj=eck4&quot;)+tm_polygons(&quot;MeanLifetimeRisk&quot;) n ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() 2.3.5.3 Voronoi The ggplot2 and sf libraries are extended to include drawing of voronoi. Voronoi is a special type of polygon. It can be seen as a mathematical approach to partition regions such that all points within the polygons are closest (depending on distance metrics) to the seed points. Voronoi has been used in disease mapping (John Snow mapping of Broad Street Cholera outbreak) and meteorology (Alfred Thiessen polygon method for measuring rainfall in basin). This is a more complex coding task. It uses the geom_voronoi call from ggvoronoi library. Some libraries have vignettes to help you implement the codes. The vignette in the ggvoronoi library can be called using vignette(“ggvoronoi”). The osmdata library will be used to provide map from OpenStreetMap. A related library is OpenStreetMap. The latter library uses raster file whereas osmdata provides vectorised map data. In the chapter on Geospatial Analysis we will expand on this theme with interactive map. One issue with the use of voronoi is that there are infinite sets and so a boundary needs to set. In the example below, the boundary was set for Greater Melbourne. library(dplyr) library(ggvoronoi) ## rgeos version: 0.6-3, (SVN revision 696) ## GEOS runtime version: 3.11.2-CAPI-1.17.2 ## Please note that rgeos will be retired during October 2023, ## plan transition to sf or terra functions using GEOS at your earliest convenience. ## See https://r-spatial.org/r/2023/05/15/evolution4.html for details. ## GEOS using OverlayNG ## Linking to sp version: 2.0-0 ## Polygon checking: TRUE library(ggplot2) library(sf) #subset data with dplyr for metropolitan melbourne msclinic&lt;-read.csv(&quot;./Data-Use/msclinic.csv&quot;) %&gt;% filter(clinic==1, metropolitan==1) Victoria&lt;-st_read(&quot;./Data-Use/GCCSA_2016_AUST.shp&quot;) %&gt;% filter(STE_NAME16==&quot;Victoria&quot;) ## Reading layer `GCCSA_2016_AUST&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\GCCSA_2016_AUST.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 34 features and 5 fields (with 18 geometries empty) ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 96.81694 ymin: -43.74051 xmax: 167.998 ymax: -9.142176 ## Geodetic CRS: GDA94 m&lt;-ggplot(msclinic)+ geom_point(aes(x=lon,y=lat))+ #add hospital location geom_voronoi(aes(x=lon,y=lat,fill=distance),fill=NA, color=&quot;black&quot;)+ #create voronoi from hospital location geom_sf(data=Victoria,aes(fill=AREASQKM16)) + labs(x=&quot;Longitude (WGS84)&quot;, y=&quot;Latitude&quot;, title=&quot;Voronoi Map of MS Clinics in Melbourne&quot;) m ## Warning: `fortify(&lt;SpatialPolygonsDataFrame&gt;)` was deprecated in ggplot2 3.4.4. ## ℹ Please migrate to sf. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. This map is not so useful as the features of Victoria overwhelm the features of Greater Melbourne. 2.3.6 ggwordcloud The ggwordcloud library extend the ggplot2 family to creating wordcloud. The following is an illustration of wordcloud created from comments on Youtube video “Stroke Heroes Act Fast”. library(ggwordcloud) library(tm) ## Loading required package: NLP ## ## Attaching package: &#39;NLP&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## annotate ## The following objects are masked from &#39;package:Biobase&#39;: ## ## annotation, content ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## annotation library(tidyverse) heroes_df&lt;-read.csv(&quot;./Data-Use/Feb_01_1_49_59 PM_2018_AEDT_YoutubeData.csv&quot;,stringsAsFactors = FALSE) #cleaning data keywords &lt;- heroes_df$Comment keywords &lt;- iconv(keywords, to = &#39;utf-8&#39;) #create corpus myCorpus &lt;- VCorpus(VectorSource(keywords)) #lower case myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower)) #remove numer myCorpus &lt;- tm_map(myCorpus, removeNumbers) #remove punctuation myCorpus &lt;- tm_map(myCorpus, removePunctuation) #remove stopwords myCorpus &lt;- tm_map(myCorpus, removeWords, stopwords(&quot;english&quot;),lazy=TRUE) #remove white space myCorpus &lt;- tm_map(myCorpus, stripWhitespace, lazy=TRUE) #term document matrix dtm &lt;- DocumentTermMatrix(myCorpus,control = list(wordLengths=c(3, 20))) #remove sparse terms dtm&lt;-removeSparseTerms(dtm, 0.95) #remove words of length &lt;=3 tdm=TermDocumentMatrix(myCorpus, control = list(minWordLength=4,maxWordLength=20) ) m &lt;- as.matrix(tdm) v &lt;- sort(rowSums(m),decreasing=TRUE) #remove words with frequency &lt;=1 d &lt;- data.frame(word = names(v),freq=v) %&gt;% filter(freq&gt;1) #wordcloud ggplot(data = d, aes(label = word, size = freq, col = as.character(freq))) + geom_text_wordcloud(rm_outside = TRUE, max_steps = 1, grid_size = 1, eccentricity = .8)+ scale_size_area(max_size = 12)+ scale_color_brewer(palette = &quot;Paired&quot;, direction = -1)+ theme_void() 2.3.7 gganimate The library gganimate can be used to generate videos in the form of gif file. The data needs to be collated from wide to long format. For the purpose of this book, the code has been turned off. library(ggplot2) library(gganimate) #ensure data in long format, Y =NIHSS u &lt;- ggplot(dtTime2, aes(period, meanY , color = T, frame = period)) + geom_bar(stat=&quot;identity&quot;) + geom_text(aes(x = 11.9, label = period), hjust = 0) + xlim(0,13)+ coord_cartesian(clip = &#39;off&#39;) + facet_wrap(~meanY)+ labs(title = &#39;ECR Trials&#39;, y = &#39;Number&#39;) + transition_reveal(period) u #create gif #animate(u,fps=15,duration=15) #anim_save(u,file=&quot;./Data-Use/simulated_ECR_trial.gif&quot;, width=800, height=800) 2.3.8 ggneuro There are several ways of plotting mri scans. The ggneuro library is illustrated here as it relates to the ggplot2 family. The Colin \\(T_1\\) scan is a high resolution scan from MNI. #devtools::install_github(&quot;muschellij2/ggneuro&quot;) library(ggneuro) library(neurobase) colin_1mm&lt;-untar(&quot;./Data-Use/colin_1mm.tgz&quot;) colinIm&lt;-readNIfTI(&quot;colin_1mm&quot;) ggortho(colinIm) 2.4 plotly Plotly has its own API and uses Dash to upload the figure on the web. It has additional ability for interaction as well as create a video. Examples are provided with calling plotly directly or via ggplot2. In the examples below, the plots are performed using ggplot2 and then pass onto plotly using ggplotly function. The example uses the Leukemia dataset from Stat2Data library. 2.4.1 Scatter plot with plotly The plotly syntax uses a ~ after the = symbol to identify a variable for plotting. library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:neurobase&#39;: ## ## colorbar ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following objects are masked from &#39;package:plyr&#39;: ## ## arrange, mutate, rename, summarise ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:oro.nifti&#39;: ## ## slice ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(Stat2Data) data(&quot;Leukemia&quot;) #treatment of leukemia #scatter plot directly from plotly plot_ly(x=~Age,y=~Smear, #percentage of blast color=~as.factor(Status), #dead or alive symbol=~Resp, symbols=c(&#39;circle&#39; ,&#39;square&#39;), #Response to treatment data=Leukemia) ## No trace type specified: ## Based on info supplied, a &#39;scatter&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter ## No scatter mode specifed: ## Setting the mode to markers ## Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels ## Warning in RColorBrewer::brewer.pal(N, &quot;Set2&quot;): minimal value for n is 3, returning requested palette with 3 different levels 2.4.2 Bar plot with plotly Plotly can uses a ggplot object directly. #using the epilepsy data in p generated above for bar plot ggplotly(p) 2.4.3 Heatmap #using swiss data above to create heatmap ggplotly(q) 2.4.4 map library(ggplot2) library(plotly) library(dplyr) covid&lt;-read.csv(&quot;./Data-Use/Covid_bystate_Table130420.csv&quot;) %&gt;% mutate(region=str_to_lower(Jurisdiction)) #consistent ggplotly( ggplot()+ geom_map(data=US, map=US, aes(x=long, y=lat, map_id=region), fill=&quot;#ffffff&quot;, color=&quot;#ffffff&quot;, size=0.15)+ #add covid data here geom_map(data=covid, map=US, aes(fill=NumberCases31.03.20, map_id=region), color=&quot;#ffffff&quot;, size=0.15)+ scale_fill_continuous(low=&#39;thistle2&#39;, high=&#39;darkred&#39;, guide=&#39;colorbar&#39;) ) ## Warning in geom_map(data = US, map = US, aes(x = long, y = lat, map_id = region), : ## Ignoring unknown aesthetics: x and y You can write citations, too. For example, we are using the bookdown package (Xie 2023) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["data-wrangling.html", "Chapter 3 Data Wrangling 3.1 Data 3.2 Data storage 3.3 Tidy data 3.4 Regular Expressions 3.5 PDF to xcel 3.6 Web scraping 3.7 Medical Images 3.8 ECG Signal processing 3.9 EEG signal processing 3.10 Python 3.11 Matlab 3.12 Stata", " Chapter 3 Data Wrangling This section deals with the nitty gritty of data analysis. There’s no nice plots like the previous chapter. In fact, this data wrangling is the major aspect of data science. 3.1 Data This section on atomic vector, arrays, matrix and list is often considered boring and ignored. 3.1.1 Vector, Arrays, Matrix 3.1.1.1 Vector and list All elements of an atomic vector and arrays are the same. A vector is an array with one dimension . List can contain different types of data. A complex example of a structured list is the json format shown below. In base R, c is a function for creating a vector or list. The function list can also be used to create list. It is best to avoid using c when assigning a name to a dataframe or vector (Wickham 2019). a&lt;-c(1,2,3) is.vector(a) ## [1] TRUE class(a) #numeric vector ## [1] &quot;numeric&quot; b&lt;-c(&quot;apple&quot;,&quot;orange&quot;,&quot;banana&quot;) is.vector(b) #character vector ## [1] TRUE class(b) ## [1] &quot;character&quot; d&lt;-c(1,2,&quot;banana&quot;) is.list(d) #character vector ## [1] FALSE class(d) #FALSE ## [1] &quot;character&quot; e&lt;-list(a,b,c(TRUE,FALSE,FALSE)) is.list(e) #TRUE ## [1] TRUE 3.1.1.2 Matrix and Arrays In R, an array is a vector organised with attributes such as dimensions. It is of a single data type. It contains a description of the number of dimension. Array can also be accessed by its indexing. Later in this chapter, we will illustrate the importance of arrays for manipulating MRI scans. A volumetric mri scan, there are 3 dimenions [,,]. The first column is the sagittal sequence, second column is the coronal sequence and the third column is the axial sequence. In the example below, this knowledge of arrays can be used to reverse the ordering of information on MRI scan (flip on it axis between left and right). #vector vector1&lt;-c(1,2,3) vector2&lt;-c(4,5,6,7,8,9) # 2 dimensions array1&lt;-array(c(vector1,vector2),dim = c(3,3,2)) array1 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # 3 dimensions array2&lt;-array(c(vector1,vector2),dim = c(2,2,3)) array2 ## , , 1 ## ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## , , 2 ## ## [,1] [,2] ## [1,] 5 7 ## [2,] 6 8 ## ## , , 3 ## ## [,1] [,2] ## [1,] 9 2 ## [2,] 1 3 #check if array or matrix is.matrix(array1) ## [1] FALSE is.array(array1) ## [1] TRUE Arrays can be accessed by indexing its structure. array1[,,1] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 The second array of array1 array1[,,2] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 First row of array1 array1[1,,] ## [,1] [,2] ## [1,] 1 1 ## [2,] 4 4 ## [3,] 7 7 First column of array1 array1[,1,] ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 A matrix is a rectangular array of a single data type arranged in rows and columns or a 2 dimensional array. Data type can be number or character. Matrix can be considered a linear combination of vector into a linear map. In R, the function is.matrix returns true only if there is a dimension argument in the data. This definition also means that a dat frame is not a matrix. M1&lt;-matrix(c(1,2,3, 4,5,6),nrow=2, ncol=3, byrow = T, dimnames = list(c(&quot;row1&quot;, &quot;row2&quot;), c(&quot;Col1&quot;, &quot;Col2&quot;, &quot;Col3&quot;))) M1 ## Col1 Col2 Col3 ## row1 1 2 3 ## row2 4 5 6 A tensor is multidimensional array. Using the analogy before about a matrix being a linear map, a tensor can be viewed as a multilinear map. More on tensor later. 3.1.1.3 array operations Array operations are performed element wise. arrayA&lt;-array(c(1,2,3),dim=c(1,3)) #1 row 3 columns arrayA ## [,1] [,2] [,3] ## [1,] 1 2 3 arrayB&lt;-array(c(4,5,6),dim=c(1,3)) arrayB ## [,1] [,2] [,3] ## [1,] 4 5 6 #addition arrayC=arrayA+arrayB arrayC ## [,1] [,2] [,3] ## [1,] 5 7 9 Now we illustrate multiplication of arrays arrayD=arrayA*arrayB arrayD ## [,1] [,2] [,3] ## [1,] 4 10 18 Division of arrays arrayE&lt;-arrayA/arrayB arrayE ## [,1] [,2] [,3] ## [1,] 0.25 0.4 0.5 3.1.2 Operators 3.1.2.0.1 Relation operators We test if variable A is same as B by using == operator while we test if A is not equal to B by != operator. A is greater than B by &gt; operator and A is lesser than B by &lt; operator. 3.1.2.1 Logical operators The | operator signifies elementwise or relationship and || signifies or relationship. The &amp; operator signifies elementwise and relationship and &amp;&amp; signifies and relationship. 3.1.2.2 %in% The %in% operator helps to evaluate if an element belong to a vector. The package Hmisc contains an operator %nin% which is the opposite of %in%. 3.1.2.3 %&gt;% The %&gt;% operator or pipe function, from magrittr package, is used to pass information to the next function. DF&lt;-data.frame(Disability=c(&quot;Good&quot;,&quot;Moderate&quot;,&quot;Good&quot;,&quot;Poor&quot;),NIHSS=c(2,10,1,20),Sex=c(&quot;M&quot;,&quot;F&quot;,&quot;M&quot;,&quot;M&quot; )) #create new variable outcome DF$outcome=ifelse(DF$Disability %in% c(&quot;Good&quot;,&quot;Moderate&quot;), 0,1) DF ## Disability NIHSS Sex outcome ## 1 Good 2 M 0 ## 2 Moderate 10 F 0 ## 3 Good 1 M 0 ## 4 Poor 20 M 1 3.1.3 Simple function a function is written by creating name of function, calling on function (x) and brackets to define function. # function sun ArithSum&lt;-function (x) { sum(x) } vector1&lt;-c(1,2,3) ArithSum(vector1) ## [1] 6 3.1.4 for loop vector2&lt;-c(4,5,6) for (i in vector2){ print(i) } ## [1] 4 ## [1] 5 ## [1] 6 Perform math operation using for loop for (i in vector2){ m= sum(vector2)/length(vector2) } m ## [1] 5 The next command can be used to modify the loop. This simple example removes even number for(i in vector2){ if(!i %% 2){ next } print(i) } ## [1] 5 Perform a math operation along the columns. A=c(1,2,3) B=c(4,5,6) df=data.frame(A,B) output_col &lt;- vector(&quot;double&quot;, ncol(df)) # 1. output for (i in seq_along(df)) { # 2. sequence output_col[[i]] &lt;- mean(df[[i]]) # 3. body } output_col ## [1] 2 5 Perform a math operation along the rows. output_row &lt;- vector(&quot;double&quot;, nrow(df)) # 1. output for (i in seq_along(df)) { # 2. sequence output_row[[i]] &lt;- mean(df[[i]]) # 3. body } output_row ## [1] 2 5 0 3.1.5 apply, lapply, sapply 3.1.5.1 apply The apply function works on data in array format. #sum data across rows ie c(1) apply(array1,c(1),sum) ## [1] 24 30 36 #sum data across columns(2) apply(array1,c(2),sum) ## [1] 12 30 48 #sum data across rows and columns c(1,2) apply(array1,c(1,2),sum) ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 3.1.5.2 lapply The call lapply applies a function to a list. In the section below of medical images the lapply function will be used to creates a list of nifti files and which can be opened painlessly with additional call to readNIfTI. Here a more simple example is used. a&lt;-c(1,2,3,4,5,6,7,8) lapply(a, function(x) x^3) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 8 ## ## [[3]] ## [1] 27 ## ## [[4]] ## [1] 64 ## ## [[5]] ## [1] 125 ## ## [[6]] ## [1] 216 ## ## [[7]] ## [1] 343 ## ## [[8]] ## [1] 512 3.1.5.3 sapply The call sapply applies a function to a vector, matrix or list. It returns the results in the form of a matrix. a&lt;-c(1,2,3,4) sapply(a, function(x) x^2) ## [1] 1 4 9 16 3.1.5.4 tapply The tapply function applies a function to a subset of the data. dat.array1&lt;-as.data.frame.array(array1) dat.array1 ## V1 V2 V3 V4 V5 V6 ## 1 1 4 7 1 4 7 ## 2 2 5 8 2 5 8 ## 3 3 6 9 3 6 9 dat.array1$V6&lt;- dat.array1$V6 %% 2 dat.array1$V6 ## [1] 1 0 1 tapply(dat.array1$V1, dat.array1$V6, sum) ## 0 1 ## 2 4 The rapply function or recursive apply is applied to a list, contingent on the second argument. In this example, the first function is to multiple elements of the list by 2 contingent on the element being numeric. a&lt;-c(1,2,3.4,D,5, &quot;NA&quot;) rapply(a, function(x) x*2, class=&quot;numeric&quot;) ## [1] 2.0 4.0 6.8 10.0 3.1.6 Functional A functional is a function embedded in a function. Later, we will revisit functional to perform analysis on a list of nifti files. Fou&lt;-lapply(df, function(A) mean(A)/sd(A)) #returns a list Fou ## $A ## [1] 2 ## ## $B ## [1] 5 The unlist function returns a matrix. Fou2&lt;-unlist(lapply(df, function(A) mean(A)/sd(A))) #returns a matrix Fou2 ## A B ## 2 5 3.1.6.1 Mapply Mapply applies the function over elements of the data. MeanFou&lt;-lapply(df, mean) MeanFou[]&lt;-Map(&#39;/&#39;, df, MeanFou) The equivalent code with lapply is provided below. MeanFou2&lt;-lapply(df, function(M) M/mean(M)) MeanFou2 ## $A ## [1] 0.5 1.0 1.5 ## ## $B ## [1] 0.8 1.0 1.2 3.1.7 Iteration This is an extension of the discussion on functional programming. Here we will be using purrr library and map function to apply function to multiple input. First, we use split function to partition data. library(purrr) library(dplyr) ss&lt;-read.csv(&quot;./Data-Use/ss150718.csv&quot;) ss%&gt;% select(duplicate, PubYear, TP,FP,FN,TN) |&gt; split(ss$duplicate) %&gt;% head() ## $no ## duplicate PubYear TP FP FN TN ## 1 no 2007 10 3 1 25 ## 4 no 2009 49 22 6 290 ## 5 no 2010 7 10 1 41 ## 6 no 2010 10 9 6 85 ## 7 no 2010 11 0 4 12 ## 8 no 2011 23 7 9 100 ## 9 no 2011 60 16 17 219 ## 10 no 2011 5 12 8 64 ## 13 no 2013 5 6 3 57 ## 14 no 2013 6 13 2 44 ## 15 no 2013 16 11 16 58 ## 19 no 2014 7 8 4 55 ## 20 no 2014 12 1 3 37 ## 21 no 2014 15 25 29 174 ## 22 no 2014 25 15 53 194 ## 24 no 2014 26 21 31 238 ## 25 no 2014 33 41 123 620 ## 26 no 2014 35 26 12 114 ## 30 no 2016 10 7 6 100 ## 32 no 2016 20 5 12 35 ## 35 no 2017 13 11 40 69 ## ## $yes ## duplicate PubYear TP FP FN TN ## 2 yes 2007 13 45 1 45 ## 3 yes 2009 14 7 4 36 ## 11 yes 2012 33 41 38 279 ## 12 yes 2012 37 24 36 131 ## 16 yes 2013 16 15 9 91 ## 17 yes 2013 17 7 11 77 ## 18 yes 2013 7 3 3 8 ## 23 yes 2014 25 19 2 37 ## 27 yes 2014 37 24 36 131 ## 28 yes 2014 44 43 45 188 ## 29 yes 2016 6 10 12 52 ## 31 yes 2017 16 12 9 78 ## 33 yes 2016 32 37 20 103 ## 34 yes 2017 55 66 67 521 ## 36 yes 2017 19 13 11 86 ## 37 yes 2017 10 14 4 112 ## 38 yes 2016 75 95 81 531 ## 39 yes 2017 NA NA NA NA Following on from the splitting of the data, we perform the regression on the split data with map. ss %&gt;% select(duplicate, PubYear, TP, FP, FN, TN) %&gt;% mutate(Sensitivity=TP/(TP+FN)) |&gt; split(ss$duplicate) |&gt; map(\\(df) lm(Sensitivity ~ PubYear, data = df))|&gt; map(summary) ## $no ## ## Call: ## lm(formula = Sensitivity ~ PubYear, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.32962 -0.11367 0.02946 0.13633 0.25884 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.98638 31.22820 3.202 0.00470 ** ## PubYear -0.04938 0.01552 -3.182 0.00491 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1762 on 19 degrees of freedom ## Multiple R-squared: 0.3477, Adjusted R-squared: 0.3134 ## F-statistic: 10.13 on 1 and 19 DF, p-value: 0.004905 ## ## ## $yes ## ## Call: ## lm(formula = Sensitivity ~ PubYear, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.225540 -0.104559 0.002324 0.100728 0.314517 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.51463 26.03472 2.056 0.0577 . ## PubYear -0.02627 0.01293 -2.032 0.0603 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1491 on 15 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.2158, Adjusted R-squared: 0.1636 ## F-statistic: 4.129 on 1 and 15 DF, p-value: 0.06026 We can extend this function to each element of a vector to get the output for the linear regression on the partition data. ss %&gt;% select(duplicate, PubYear, TP, FP, FN, TN) %&gt;% mutate(Sensitivity=TP/(TP+FN)) |&gt; split(ss$duplicate) |&gt; map(\\(df) lm(Sensitivity ~ PubYear, data = df))|&gt; map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) ## no yes ## 0.3476832 0.2158460 3.1.7.1 map characters The map function can be applied to characters. This can be illustrated using the spot sign data map2_chr(ss$Authors, ss$PubYear, ~paste(.x, .y, sep=&quot;: &quot;)) %&gt;% head() ## [1] &quot;Wada: 2007&quot; &quot;Goldstein: 2007&quot; &quot;Delgado Almand: 2009&quot; ## [4] &quot;Ederies: 2009&quot; &quot;Hallevi: 2010&quot; &quot;Park: 2010&quot; The characters can be mapped to upper case. library(stringr) map2_chr(ss$Authors, ss$PubYear, ~paste(.x, .y, sep=&quot; - &quot;)) %&gt;% map_chr(~str_to_upper(.)) %&gt;% head() ## [1] &quot;WADA - 2007&quot; &quot;GOLDSTEIN - 2007&quot; &quot;DELGADO ALMAND - 2009&quot; ## [4] &quot;EDERIES - 2009&quot; &quot;HALLEVI - 2010&quot; &quot;PARK - 2010&quot; The purrr library has function to pluck items from the list or data frame. map2_chr(ss$Authors, ss$PubYear, ~paste(.x, .y, sep=&quot; - &quot;)) %&gt;% pluck(1) ## [1] &quot;Wada - 2007&quot; 3.2 Data storage Often one assumes that opening Rstudio is sufficient to locate the file and run the analysis. One way of doing this at the console is to click on Session tab, then Set Working Directory to location of file. Another way of doing this seemlessly is to use the library here. It is easy to find the files in your directory using the list.files() call. To list only some files use pattern matching. We can increase the complexity of pattern matching. #list files matching pattern list.files(pattern=&quot;.Rmd|*.stan&quot;) ## [1] &quot;01-intro.Rmd&quot; &quot;02-Data-Wrangling.Rmd&quot; ## [3] &quot;03-Statistics.Rmd&quot; &quot;04-multivariate-analysis.Rmd&quot; ## [5] &quot;05-machinelearning.Rmd&quot; &quot;06-machinelearningpt2.Rmd&quot; ## [7] &quot;07-Bayesian-analysis.Rmd&quot; &quot;08-operational-research.Rmd&quot; ## [9] &quot;09-graph-theory.Rmd&quot; &quot;10-geospatial-analysis.Rmd&quot; ## [11] &quot;11-App.Rmd&quot; &quot;12-Appendix.Rmd&quot; ## [13] &quot;13-references.Rmd&quot; &quot;Applications-of-R-in-Healthcare.Rmd&quot; ## [15] &quot;bym_predictor_plus_offset.stan&quot; &quot;bym2.stan&quot; ## [17] &quot;car.stan&quot; &quot;car_ht.stan&quot; ## [19] &quot;index.Rmd&quot; &quot;lm.stan&quot; ## [21] &quot;lmr.stan&quot; &quot;normal_regression.stan&quot; ## [23] &quot;schools.stan&quot; &quot;svc.stan&quot; 3.2.1 Data frame Data frame is a convenient way of formatting data in table format. It is worth checking the structure of data. Some libraries prefer to work with data in data frame while others prefer matrix or array structure. a&lt;-c(1,2,3) b&lt;-c(&quot;apple&quot;,&quot;orange&quot;,&quot;banana&quot;) e&lt;-data.frame(a,b) rownames(e) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; 3.2.2 Excel data Excel data are stored as csv, xls and xlsx. Csv files can be open in base R using read.csv function or using readr library and read_csv function. I would urge you to get used to manipulating data in R as the codes serve as a way to keep track with the change in data. The original xcel data should not be touched outside of R. A problem with excel data is that its autocorrect function change provenance of genomic data eg SEPT1, MARCH1. SEPT1 is now relabeled as SEPTIN1. A&lt;-read.csv(&quot;File.csv&quot;) B&lt;-readr::read_csv (&quot;File.csv&quot;) The readxl library can be used to open files ending in xls and xlsx. C&lt;-readxl::read_xlsx(&quot;File.xlsx&quot;,skip=1) #skip first row D&lt;-readxl::read_xlsx(&quot;File.xlsx&quot;,sheet=2) #read data from sheet 2 3.2.2.1 Date and time Date and time can be handle in base R. The library lubridate is useful for parsing date data. It is possible to get an overview of the functions of the library by typing help(package=lubridate). Errors with parsing can occur if there are characters in the column containing date data. library(dplyr) dfdate&lt;-data.frame(&quot;DateofEvent&quot;=c(&quot;12/03/2005&quot;,&quot;12/04/2006&quot;,NA), &quot;Result&quot;=c(4,5,6)) class(dfdate$DateofEvent) ## [1] &quot;character&quot; dfdate$DateofEvent ## [1] &quot;12/03/2005&quot; &quot;12/04/2006&quot; NA The date column appears to be listed as character because of NA. This is easily fixed by filtering NA. dfdate$`Date.of.Event`&lt;-dfdate$DateofEvent #remove NA using filter dfdate %&gt;% filter(!is.na(DateofEvent)) ## DateofEvent Result Date.of.Event ## 1 12/03/2005 4 12/03/2005 ## 2 12/04/2006 5 12/04/2006 #re assigning date data type dfdate$DateofEvent2&lt;-as.POSIXct(dfdate$DateofEvent) dfdate$DateofEvent3&lt;-as.POSIXct(dfdate$`Date.of.Event`) class(dfdate$DateofEvent2) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; dfdate$DateofEvent2 ## [1] &quot;0012-03-20 LMT&quot; &quot;0012-04-20 LMT&quot; NA Problem can occur when date and time are located in separate columns. The first issue is that time data is assigned a default date 1899-12-30. This error occurs as the base date in MS Office is 1899-12-30. This issue can be compounded when there are 2 time data eg a patient has stroke onset at 22:00 and is scanned at 02:00. The data become 1899-12-31 22:00 and 1899-12-31 02:00. In this case it implies that scanning occurs before stroke onset. This could have been solved at the data collection stage by having 2 separate date colums. There are several solutions inclusing ifelse but care must be taken with this argument. Note that ifelse argument convert date time data to numeric class. This can be resolved by embedding ifelse statement within as.Date argument. This argument requires origin argument. The logic argument may fail when the default date differ eg 1904-02-08 02:00. In this case 1904-02-08 02:00 is greater than 1899-12-31 22:00. library(lubridate) library(tidyverse) df&lt;-data.frame(&quot;Onset_date&quot;=c(&quot;2014-03-06&quot;,&quot;2013-06-09&quot;), &quot;Onset_Time&quot;=c(&quot;1899-12-31 08:03:00&quot;,&quot;1899-12-31 22:00:00&quot;), &quot;Scan_Time&quot;=c(&quot;1904-02-08 10:00:00&quot;,&quot;1899-12-31 02:00:00&quot;)) %&gt;% mutate( #use update argument to reassign year, month and day Scantime=update(ymd_hms(Scan_Time), year=year(ymd(Onset_date)), month=month(ymd(Onset_date)), mday=day(ymd(Onset_date))), Onsettime=update(ymd_hms(Onset_Time), year=year(ymd(Onset_date)), month=month(ymd(Onset_date)), mday=day(ymd(Onset_date))), Scan_date=ifelse(Onsettime&gt;Scantime,1,0), Scantime=Scantime+Scan_date*hms(&quot;24:00:00&quot;), DiffHour=Scantime-Onsettime #minutes ) df %&gt;% select(-Scan_date) %&gt;% gt::gt() #xtsnsieaxn table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #xtsnsieaxn thead, #xtsnsieaxn tbody, #xtsnsieaxn tfoot, #xtsnsieaxn tr, #xtsnsieaxn td, #xtsnsieaxn th { border-style: none; } #xtsnsieaxn p { margin: 0; padding: 0; } #xtsnsieaxn .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xtsnsieaxn .gt_caption { padding-top: 4px; padding-bottom: 4px; } #xtsnsieaxn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xtsnsieaxn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #xtsnsieaxn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xtsnsieaxn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xtsnsieaxn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xtsnsieaxn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xtsnsieaxn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xtsnsieaxn .gt_column_spanner_outer:first-child { padding-left: 0; } #xtsnsieaxn .gt_column_spanner_outer:last-child { padding-right: 0; } #xtsnsieaxn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #xtsnsieaxn .gt_spanner_row { border-bottom-style: hidden; } #xtsnsieaxn .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #xtsnsieaxn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xtsnsieaxn .gt_from_md > :first-child { margin-top: 0; } #xtsnsieaxn .gt_from_md > :last-child { margin-bottom: 0; } #xtsnsieaxn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xtsnsieaxn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #xtsnsieaxn .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #xtsnsieaxn .gt_row_group_first td { border-top-width: 2px; } #xtsnsieaxn .gt_row_group_first th { border-top-width: 2px; } #xtsnsieaxn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xtsnsieaxn .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #xtsnsieaxn .gt_first_summary_row.thick { border-top-width: 2px; } #xtsnsieaxn .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xtsnsieaxn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xtsnsieaxn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xtsnsieaxn .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #xtsnsieaxn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xtsnsieaxn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xtsnsieaxn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xtsnsieaxn .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #xtsnsieaxn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xtsnsieaxn .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #xtsnsieaxn .gt_left { text-align: left; } #xtsnsieaxn .gt_center { text-align: center; } #xtsnsieaxn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xtsnsieaxn .gt_font_normal { font-weight: normal; } #xtsnsieaxn .gt_font_bold { font-weight: bold; } #xtsnsieaxn .gt_font_italic { font-style: italic; } #xtsnsieaxn .gt_super { font-size: 65%; } #xtsnsieaxn .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #xtsnsieaxn .gt_asterisk { font-size: 100%; vertical-align: 0; } #xtsnsieaxn .gt_indent_1 { text-indent: 5px; } #xtsnsieaxn .gt_indent_2 { text-indent: 10px; } #xtsnsieaxn .gt_indent_3 { text-indent: 15px; } #xtsnsieaxn .gt_indent_4 { text-indent: 20px; } #xtsnsieaxn .gt_indent_5 { text-indent: 25px; } Onset_date Onset_Time Scan_Time Scantime Onsettime DiffHour 2014-03-06 1899-12-31 08:03:00 1904-02-08 10:00:00 2014-03-06 10:00:00 2014-03-06 08:03:00 1.95 2013-06-09 1899-12-31 22:00:00 1899-12-31 02:00:00 2013-06-10 02:00:00 2013-06-09 22:00:00 4 One way of storing data in R format is to save the file as .Rda. This format will ensure that no one can accidentally rewrite or delete a number. For very large data, it’s quicker to save as .Rda file than as csv file. 3.2.3 Foreign data The foreign library is traditionally use to handle data from SPSS (.sav), Stata (.dta) and SAS (.sas). One should look at the ending in the file to determine the necessary library. library(foreign) #write and read Stata write.dta(dfdate,file=&quot;./Data-Use/dfdate_temp.dta&quot;) a&lt;-read.dta(&quot;./Data-Use/dfdate_temp.dta&quot;) a The foreign library can handle older SAS files but not the current version. The current version of SAS file requires sas7bdat. The clue is the file ends in .sas7bdat. 3.2.4 json format Json is short for JavaScript object Notification. These files have a hierarchical structured format. The json file is in text format amd can also be examined using Notepad. These files can be read using the RJSONIO or rjson libraries in R. Geojson is a json format with geographical data. library(RJSONIO) j&lt;-fromJSON(&quot;./Data-Use/0411.geojson&quot;) #Christionso Island j&lt;-lapply(j, function(x) { x[sapply(x,is.null)]&lt;-NA unlist(x) }) k&lt;-as.data.frame(do.call(&quot;cbind&quot;,j)) #list to data frame head(k) ## type crs ## type FeatureCollection name ## geometry.type FeatureCollection EPSG:4326 ## geometry.coordinates1 FeatureCollection name ## geometry.coordinates2 FeatureCollection EPSG:4326 ## properties.id FeatureCollection name ## properties.status FeatureCollection EPSG:4326 ## features ## type Feature ## geometry.type Point ## geometry.coordinates1 15.18657358 ## geometry.coordinates2 55.3200168 ## properties.id 651a5746-735a-4312-b236-3a008e173de9 ## properties.status 1 The geojson file can be converted to sf using geojson_sf function from geojsonsf library. library(geojsonsf) #Christianso Island geojson_sf(&quot;./Data-Use/0411.geojson&quot;) %&gt;% mapview::mapview() 3.3 Tidy data Attention to collection of data is important as it shows the way for performing analysis. In general each row represents on variable and each column represents an attribute of that variables. Sometimes there is a temptation to embed 2 types of attributes into a column. df2&lt;-data.frame(Sex=c(&quot;Male&quot;,&quot;Female&quot;), Test=c(&quot;positive 5 negative 5&quot;, &quot; negative 0 negative 10&quot;)) df2 ## Sex Test ## 1 Male positive 5 negative 5 ## 2 Female negative 0 negative 10 The above example should be entered this way. This change allows one to group variables by Test status: ‘positive’ or ‘negative’. One can easily perform a t-test here (not recommend in this case as the data contains only 2 rows). df2&lt;-data.frame(Sex=c(&quot;Male&quot;,&quot;Female&quot;), `Test Positive` =c(5,0), `Test Negative`=c(5, 10)) df2 ## Sex Test.Positive Test.Negative ## 1 Male 5 5 ## 2 Female 0 10 The below example is illustrate how to collapse columns when using base R. dfa&lt;-data.frame(City=c(&quot;Melbourne&quot;,&quot;Sydney&quot;,&quot;Adelaide&quot;), State=c(&quot;Victoria&quot;,&quot;NSW&quot;,&quot;South Australia&quot;)) #collapsing City and State columns and generate new column address dfa$addresses&lt;-paste0(dfa$City,&quot;,&quot;, dfa$State) #separate by comma dfa$addresses2&lt;-paste0(dfa$City,&quot;,&quot;, dfa$State,&quot;, Australia&quot;) dfa ## City State addresses ## 1 Melbourne Victoria Melbourne,Victoria ## 2 Sydney NSW Sydney,NSW ## 3 Adelaide South Australia Adelaide,South Australia ## addresses2 ## 1 Melbourne,Victoria, Australia ## 2 Sydney,NSW, Australia ## 3 Adelaide,South Australia, Australia This example is same as above but uses verbs from tidyr. This is useful for collapsing address for geocoding. library(tidyr) dfa1&lt;-dfa %&gt;% unite (&quot;new_address&quot;,City:State,sep = &quot;,&quot;) dfa1 ## new_address addresses ## 1 Melbourne,Victoria Melbourne,Victoria ## 2 Sydney,NSW Sydney,NSW ## 3 Adelaide,South Australia Adelaide,South Australia ## addresses2 ## 1 Melbourne,Victoria, Australia ## 2 Sydney,NSW, Australia ## 3 Adelaide,South Australia, Australia Using the data above, let’s split the column address library(tidyr) dfa2&lt;-dfa1 %&gt;% separate(addresses, c(&quot;City2&quot;, &quot;State2&quot;)) ## Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [3]. dfa2 ## new_address City2 State2 addresses2 ## 1 Melbourne,Victoria Melbourne Victoria Melbourne,Victoria, Australia ## 2 Sydney,NSW Sydney NSW Sydney,NSW, Australia ## 3 Adelaide,South Australia Adelaide South Adelaide,South Australia, Australia 3.3.1 Factors There are several types of factors in R: ordered and not ordered. It is important to pay attention to how factors are coded. Sometimes, male is represented as 1 and female as 0. Sometimes, female is represented as 2 as integer encoding. Integer encoding assumes a rank ordering. This discussion may seems trivial but several papers have been retracted in high impact factor journal Jama because of miscoding of the trial assignment 1 and 2 rather than the assignment of 0 and 1. This error led to reversing the results with logistic regression when 2 is exchanged for 0 (Aboumatar and Wise 2019). This error led to report that an outpatient management program for chronic obstructive pulmonary disease resulted in fewer admissions. Below is an example which can occur when data is transformed into factor and back to number. Note that the coding goes from 0 and 1 to 2 and 1. It has been suggested that the move away from coding data as 1 and 0 was historical and due to the fear that coding with punch card would treat 0 as a missing number and hence the move to coding binary variable as 1 and 2. In certain analyses, the libraries prefer to use the dependent or outcome variable as binary coding in numeric format such as 1 and 0 eg logistic regression and random forest. The library e1071 for performing support vector machine prefers the outcome variable as factor. library(Stat2Data) data(&quot;Leukemia&quot;) #treatment of leukemia Leukemia %&gt;% dplyr::glimpse() ## Rows: 51 ## Columns: 9 ## $ Age &lt;int&gt; 20, 25, 26, 26, 27, 27, 28, 28, 31, 33, 33, 33, 34, 36, 37, 40, 40, 4… ## $ Smear &lt;int&gt; 78, 64, 61, 64, 95, 80, 88, 70, 72, 58, 92, 42, 26, 55, 71, 91, 52, 7… ## $ Infil &lt;int&gt; 39, 61, 55, 64, 95, 64, 88, 70, 72, 58, 92, 38, 26, 55, 71, 91, 49, 6… ## $ Index &lt;int&gt; 7, 16, 12, 16, 6, 8, 20, 14, 5, 7, 5, 12, 7, 14, 15, 9, 12, 4, 14, 10… ## $ Blasts &lt;dbl&gt; 0.6, 35.0, 7.5, 21.0, 7.5, 0.6, 4.8, 10.0, 2.3, 5.7, 2.6, 2.5, 7.0, 4… ## $ Temp &lt;int&gt; 990, 1030, 982, 1000, 980, 1010, 986, 1010, 988, 986, 980, 984, 982, … ## $ Resp &lt;int&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, … ## $ Time &lt;int&gt; 18, 31, 31, 31, 36, 1, 9, 39, 20, 4, 45, 36, 12, 8, 1, 15, 24, 2, 33,… ## $ Status &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … The variable Resp is now a factor with levels 0 and 1 Leukemia$Resp ## [1] 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 ## [42] 0 0 1 0 0 1 0 1 1 0 Leukemia$Response.factor&lt;-as.factor(Leukemia$Resp) head(Leukemia$Response.factor) ## [1] 1 1 1 1 1 0 ## Levels: 0 1 Note in the conversion back to numeric ‘dummy’ values, the data takes the form 1 and 2. This has changed the dummy values of 0 and 1. It is important to examine the data before running analysis. Leukemia$Response.numeric&lt;-as.numeric(Leukemia$Response.factor) Leukemia$Response.numeric ## [1] 2 2 2 2 2 1 2 2 2 1 2 2 1 2 1 2 2 1 2 2 1 1 1 1 2 2 1 1 2 1 2 1 1 1 1 1 1 1 1 2 1 ## [42] 1 1 2 1 1 2 1 2 2 1 For variables which are characters but considered as factors, it is necessary to convert to class character before converting to dummy values. data(&quot;BreastCancer&quot;, package=&quot;mlbench&quot;) BreastCancer %&gt;% glimpse() ## Rows: 699 ## Columns: 11 ## $ Id &lt;chr&gt; &quot;1000025&quot;, &quot;1002945&quot;, &quot;1015425&quot;, &quot;1016277&quot;, &quot;1017023&quot;, &quot;1017… ## $ Cl.thickness &lt;ord&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4, 10, 6,… ## $ Cell.size &lt;ord&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1, 7, 1,… ## $ Cell.shape &lt;ord&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1, 7, 1,… ## $ Marg.adhesion &lt;ord&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1, 6, 1,… ## $ Epith.c.size &lt;ord&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2, 4, 2, … ## $ Bare.nuclei &lt;fct&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1, 1, 10,… ## $ Bl.cromatin &lt;fct&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3, 4, 3, … ## $ Normal.nucleoli &lt;fct&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1, 1, 1, … ## $ Mitoses &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, … ## $ Class &lt;fct&gt; benign, benign, benign, benign, benign, malignant, benign, b… The steps for conversion are illustrated below. Conversion of multiple columns of factors and ordered factors can be done in one step using lapply function. This will be described much further below. BreastCancer$Class&lt;-as.character(BreastCancer$Class) BreastCancer$Class[BreastCancer$Class==&quot;benign&quot;]&lt;-0 BreastCancer$Class[BreastCancer$Class==&quot;malignant&quot;]&lt;-1 BreastCancer$Class&lt;-as.numeric(BreastCancer$Class) head(BreastCancer$Class) ## [1] 0 0 0 0 0 1 This illustration describes conversion of a continuous variable into orderly factors. library(Stat2Data) data(&quot;Leukemia&quot;) #treatment of leukemia #partition Age into 8 ordered factors Leukemia$AgeCat&lt;-ggplot2::cut_interval(Leukemia$Age, n=8, ordered_result=TRUE) class(Leukemia$AgeCat) ## [1] &quot;ordered&quot; &quot;factor&quot; 3.3.2 Multiple files Merging of files can be done using dplyr to perform inner_join, outer_join, left_join and right_join. Note that this can also be done in base R or using syntax of data.table. These files can be joined using %&gt;% operator. DF1&lt;-data.frame(ID=c(1,2,3),Age=c(20,30,40)) DF2&lt;-data.frame(ID=c(1,2,3),Sex=c(1,0,0)) DF3&lt;-data.frame(ID=c(1,2,3), Diabetes=c(1,1,0)) DF &lt;-DF1 %&gt;% left_join(DF2, by=&quot;ID&quot;) %&gt;% left_join(DF3, by=&quot;ID&quot;) 3.3.3 Tidy evaluation Data in a dataframe can be summarised with the help of group_by function in dplyr. The summarise function returns 1 row of data per group. The number of observations in each group can be counted using n() function. # spot sign dataset ss&lt;-read.csv(&quot;./Data-Use/ss150718.csv&quot;) ss %&gt;% group_by(Country) %&gt;% summarise (Number=n()) ## # A tibble: 11 × 2 ## Country Number ## &lt;chr&gt; &lt;int&gt; ## 1 Brazil 1 ## 2 Canada 3 ## 3 China 6 ## 4 China/US 1 ## 5 Denmark 1 ## 6 Germany 1 ## 7 Japan 5 ## 8 Korea 4 ## 9 Multiple 5 ## 10 Spain 1 ## 11 US 11 We can add more arguments to the group_by function ss %&gt;% group_by(Country, Study.type) %&gt;% summarise (Number=n()) ## `summarise()` has grouped output by &#39;Country&#39;. You can override using the `.groups` ## argument. ## # A tibble: 16 × 3 ## # Groups: Country [11] ## Country Study.type Number ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Brazil Prospective 1 ## 2 Canada Prospective 1 ## 3 Canada Retro 2 ## 4 China Prospective 3 ## 5 China Retro 3 ## 6 China/US Retro 1 ## 7 Denmark Prospective 1 ## 8 Germany Prospective 1 ## 9 Japan Prospective 1 ## 10 Japan Retro 4 ## 11 Korea Prospective 1 ## 12 Korea Retro 3 ## 13 Multiple Prospective 5 ## 14 Spain Prospective 1 ## 15 US Prospective 4 ## 16 US Retro 7 We can also apply a function across multiple columns with across. Here we use reframe argument to unlock restriction impose by group_by. Here we use tibble function to return several output columns. DR&lt;- function (w,x,y,z){ tibble( Sensitivity=round(w/(w+y),2), Specificity=round(z/(z+x),2)) } ss %&gt;% group_by(Country, Study.type) %&gt;% #the output is limted to 6 rows by head argument reframe (across(TP:TN), DR(TP,FP,FN,TN)) %&gt;% head() ## # A tibble: 6 × 8 ## Country Study.type TP FP FN TN Sensitivity Specificity ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Brazil Prospective 5 6 3 57 0.62 0.9 ## 2 Canada Prospective 10 3 1 25 0.91 0.89 ## 3 Canada Retro 49 22 6 290 0.89 0.93 ## 4 Canada Retro 11 0 4 12 0.73 1 ## 5 China Prospective 60 16 17 219 0.78 0.93 ## 6 China Prospective 17 7 11 77 0.61 0.92 3.3.4 Pivot wide and long A variety of different expressions are used to describe data format such as wide and long formats. In some case the distinction between such formats is not clear. The verbs for performing these operations are pivot_wide, pivot_long. Again data.table uses different verbs such as cast and melt. In general, most regression analyses are performed with data in wide format. In this case each row represents a unique ID. Longitudinal analyses are performed with data in long format. In this format, there are several rows with the same ID. In the next Chapter on Statistics, an example of data generated in wide format and coverted to long format using plyr. Here we will demonstrate the use of tidyr to pivot loner or wider. The best way to think about how data should be presented is that data is analyzed according to columns not rows. The data below is extracted from CDC COVID website. Details are given below under Web scraping on how this task was performed. library(dplyr) library(tidyr) library(stringr) usa&lt;-read.csv(&quot;./Data-Use/Covid_bystate_Table130420.csv&quot;) # for demonstration we will select 3 columns of interest usa_long &lt;-usa %&gt;% select(Jurisdiction,NumberCases31.03.20,NumberCases07.04.20) %&gt;% pivot_longer(-Jurisdiction,names_to = &quot;Date&quot;,values_to = &quot;Number.Cases&quot;) usa_long$Date &lt;- str_replace(usa_long$Date,&quot;NumberCases&quot;,&quot;&quot;) #data in wide format head(usa %&gt;%select(Jurisdiction,NumberCases31.03.20,NumberCases07.04.20),6) ## Jurisdiction NumberCases31.03.20 NumberCases07.04.20 ## 1 Alabama 999 2197 ## 2 Alaska 133 213 ## 3 Arizona 1289 2575 ## 4 Arkansas 560 993 ## 5 California 8131 15865 ## 6 Colorado 2966 5429 #data in long format head(usa_long,6) ## # A tibble: 6 × 3 ## Jurisdiction Date Number.Cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Alabama 31.03.20 999 ## 2 Alabama 07.04.20 2197 ## 3 Alaska 31.03.20 133 ## 4 Alaska 07.04.20 213 ## 5 Arizona 31.03.20 1289 ## 6 Arizona 07.04.20 2575 3.4 Regular Expressions Here is a short tutorial on regular expression. We will begin using base R. This section is based on experience trying to clean a data frame containing many words used to describe one disease or one drug. 3.4.1 base R #create example dataframe df&lt;-data.frame( drug=c(&quot;valium 1mg&quot;,&quot;verapamil sr&quot;,&quot;betaloc zoc&quot;,&quot;tramadol&quot;,&quot;valium (diazepam)&quot;), infection=c(&quot;pneumonia&quot;,&quot;aspiration pneumonia&quot;,&quot;tracheobronchitis&quot;,&quot;respiratory tract infection&quot;,&quot;respiratory.tract.infection&quot;)) df ## drug infection ## 1 valium 1mg pneumonia ## 2 verapamil sr aspiration pneumonia ## 3 betaloc zoc tracheobronchitis ## 4 tramadol respiratory tract infection ## 5 valium (diazepam) respiratory.tract.infection Now that we have a data frame, we can use pattern matching to replace part of phrase. This step can be done simply using gsub command. First create a list so that the computer searches the phrases in the list. #create list to remove phrase redun=c(&quot;1mg&quot;, &quot;zoc&quot;, &quot;sr&quot;) pat=paste0(&quot;\\\\b(&quot;,paste0(redun,collapse = &quot;|&quot;),&quot;)\\\\b&quot;) df$drug1&lt;-gsub(pat,&quot;&quot;,df$drug) df$drug1 ## [1] &quot;valium &quot; &quot;verapamil &quot; &quot;betaloc &quot; &quot;tramadol&quot; ## [5] &quot;valium (diazepam)&quot; #create list to remove phrase redunc1=c(&quot;respiratory tract infection&quot;, &quot;tracheobronchitis&quot;, &quot;aspiration&quot;) pat=paste0(&quot;\\\\b(&quot;,paste0(redunc1,collapse = &quot;|&quot;),&quot;)\\\\b&quot;) df$infection1&lt;-gsub(pat,&quot;&quot;,df$infection) df$infection1 ## [1] &quot;pneumonia&quot; &quot; pneumonia&quot; ## [3] &quot;&quot; &quot;&quot; ## [5] &quot;respiratory.tract.infection&quot; This section deals with meta-characterers. Examples of meta-characters include $ . + * ? ^ () {} []. These meta-characters requires the double back slashes \\. #create list to remove phrase redun=c(&quot;1mg&quot;, &quot;zoc&quot;, &quot;sr&quot;) pat=paste0(&quot;\\\\b(&quot;,paste0(redun, collapse = &quot;|&quot;),&quot;)\\\\b&quot;) df$drug2&lt;-gsub(pat,&quot;&quot;,df$drug) #[a-z] indicates any letter #[a-z]+ indicates any letter and those that follow the intial letter df$drug2&lt;-gsub(&quot;\\\\(|[a-z]+\\\\)&quot;,&quot;&quot;,df$drug2) df$drug2 ## [1] &quot;valium &quot; &quot;verapamil &quot; &quot;betaloc &quot; &quot;tramadol&quot; &quot;valium &quot; Back to our data frame df, we want to remove or change the different words accounting for pneumonia. redunc=c(&quot;\\\\.&quot;) redunc1=c(&quot;respiratory tract infection&quot;, &quot;tracheobronchitis&quot;, &quot;aspiration&quot;) pat=paste0(&quot;\\\\b(&quot;,paste0(redunc,collapse = &quot;|&quot;),&quot;)\\\\b&quot;) df$infection2&lt;-gsub(pat,&quot; &quot;,df$infection) pat=paste0(&quot;\\\\b(&quot;,paste0(redunc1,collapse = &quot;|&quot;),&quot;)\\\\b&quot;) df$infection2&lt;-gsub(pat,&quot; &quot;,df$infection2) df$infection2 ## [1] &quot;pneumonia&quot; &quot; pneumonia&quot; &quot; &quot; &quot; &quot; &quot; &quot; 3.4.2 stringr The following examples are taken from excel after conversion from pdf. In the process of conversion errors were introduced in the conversion from pdf to excel. A full list of the options available can be found at https://stringr.tidyverse.org/articles/regular-expressions.html library(stringr) #error introduced by double space a&lt;-c(&quot;8396 (7890 to 8920)&quot;,&quot;6 301 113(6 085 757 to 6 517 308)&quot;, &quot;4 841 208 (4 533 619 to 5 141 654)&quot;, &quot;1 407 701 (127 445 922 to 138 273 863)&quot;, &quot;4 841 208\\n(4 533 619 to\\n5 141 654)&quot;) b&lt;-str_replace (a, &quot;\\\\(c.*\\\\)&quot;,&quot;&quot;) #this is a complex example to clean and requires several steps. Note that the original data in the list a is now assigned to b. b&lt;-str_replace(a,&quot;\\n&quot;,&quot;&quot;) %&gt;% #remove ( str_replace(&quot;\\\\(.*&quot;,&quot;&quot;) %&gt;% str_replace(&quot;\\n.*&quot;,&quot;&quot;) %&gt;% #remove ) str_replace(&quot;\\\\)&quot;,&quot;&quot;) %&gt;% #remove empty space str_replace(&quot;\\\\s&quot;,&quot;&quot;) %&gt;% str_replace(&quot;\\\\s&quot;,&quot;&quot;)%&gt;% as.numeric() b ## [1] 8396 6301113 4841208 1407701 4841208 Another example. This time the 2 numbers in the column are separated by a slash sign. Supposed you want to keep the denominator. The first remove the number before the slash sign. The _*_ metacharacter denotes the action occurs at the end. df.d&lt;-data.frame(seizure.rate=c(&quot;59/90&quot;, &quot;90/100&quot;, &quot;3/23&quot;)) df.d$seizure.number&lt;-str_replace(df.d$seizure.rate,&quot;[0-9]*&quot;,&quot;&quot;) df.d$seizure.number ## [1] &quot;/90&quot; &quot;/100&quot; &quot;/23&quot; Now combine with the next step to remove the slash sign. #We used [0-9] to denote any number from 0 to 9. For text, one can use [A-Z]. df.d$seizure.number&lt;-str_replace(df.d$seizure.rate,&quot;^[0-9]*&quot;,&quot;&quot;)%&gt;% str_replace(&quot;/&quot;,&quot;\\\\&quot;) df.d$seizure.number ## [1] &quot;90&quot; &quot;100&quot; &quot;23&quot; Removing the denominator requires a different approach. First remove the last number then the slash sign. df.d$case&lt;-str_replace(df.d$seizure.rate,&quot;/[0-9]*&quot;,&quot; &quot;) df.d$case ## [1] &quot;59 &quot; &quot;90 &quot; &quot;3 &quot; The example below has several words mixed in numeric vector columns. The words are a mixture of upper and lower cases. Note that “NA” is treated as a word character while NA is treated as Not Available by R. This recognition is important as removing them requires different actions. Character “NA” can be removed by str_replace while NA requires is.na operator. A&lt;-c(1,2,3,&quot;NA&quot;,4,&quot;no COW now&quot;) B&lt;-c(1,2,NA,4,&quot;NA&quot;,&quot;check&quot;) C&lt;-c(1,&quot;not now&quot;,2,3, NA ,5) D&lt;-data.frame(A,B,C) #str_replace on one column D$A1&lt;-str_replace(D$A,&quot;[A-Z]+&quot;,&quot;&quot;) %&gt;% str_replace(&quot;[a-z]+&quot;,&quot;&quot;) #change to lower case D$A2&lt;-str_to_lower(D$A) %&gt;% str_replace(&quot;[a-z]+&quot;,&quot;&quot;) #remove space before replacement D$A3&lt;-str_to_lower(D$A) %&gt;% str_replace(&quot;\\\\s+&quot;,&quot;&quot;) %&gt;% str_replace(&quot;[a-z]+&quot;,&quot;&quot;) #note that this action does not remove the third word D$A4&lt;-str_to_lower(D$A) %&gt;% str_replace(&quot;\\\\s&quot;,&quot;&quot;) %&gt;% str_replace(&quot;[a-z]+&quot;,&quot;&quot;) #repeat removal of empty space D$A5&lt;-str_to_lower(D$A) %&gt;% str_replace(&quot;\\\\s&quot;,&quot;&quot;) %&gt;% str_replace(&quot;\\\\s&quot;,&quot;&quot;) %&gt;% str_replace(&quot;[a-z]+&quot;,&quot;&quot;) #apply str_replace_all rather than repeat D$A6&lt;-str_to_lower(D$A) %&gt;% str_replace_all(&quot;\\\\s&quot;,&quot;&quot;) %&gt;% str_replace(&quot;[a-z]+&quot;,&quot;&quot;) #now combine into vector. Note the use of c to combine the vector and replace #the comma with equal sign D$A7&lt;-str_to_lower(D$A) %&gt;% str_replace_all(c(&quot;\\\\s&quot;=&quot;&quot;,&quot;[a-z]+&quot;=&quot;&quot;)) D ## A B C A1 A2 A3 A4 A5 A6 A7 ## 1 1 1 1 1 1 1 1 1 1 1 ## 2 2 2 not now 2 2 2 2 2 2 2 ## 3 3 &lt;NA&gt; 2 3 3 3 3 3 3 3 ## 4 NA 4 3 ## 5 4 NA &lt;NA&gt; 4 4 4 4 4 4 4 ## 6 no COW now check 5 now cow now now now The lessons from above can be combine in when creating data frame. The mutate_if function enable multiple columns to be changed. One problem to handle adding multiple columns which contain NA is the use of rowSums and dplyr::select. These examples are illustrated below. #use the mutate function E&lt;-data.frame(A,B,C) %&gt;% mutate (A=str_to_lower(A) %&gt;% str_replace_all(c(&quot;\\\\s&quot;=&quot;&quot;,&quot;[a-z]+&quot;=&quot;&quot;)), B=str_to_lower(B) %&gt;%str_replace_all(c(&quot;\\\\s&quot;=&quot;&quot;,&quot;[a-z]+&quot;=&quot;&quot;)), C=str_to_lower(C) %&gt;%str_replace_all(c(&quot;\\\\s&quot;=&quot;&quot;,&quot;[a-z]+&quot;=&quot;&quot;)))%&gt;% #change character columns to numeric mutate_if(is.character, as.numeric)%&gt;% #add across columns and avoid NA mutate(ABC=rowSums(dplyr::select(.,A:C),na.rm = T)) Another example of NA creating issues with sum in mutate is provided below. Here, the function rowwise from dplyr can be used to emphasise the operation across rows. df&lt;-data.frame(Mon_SBP=c(160,NA,180),Tues_SBP=c(0,0,150), Wed_SBP=c(NA,130,125)) %&gt;% #error from NA rowwise %&gt;% mutate(SBP=ifelse(sum(Mon_SBP&gt;140 &amp; Tues_SBP&gt;120, Tues_SBP&gt;100 &amp; Wed_SBP&gt;120, Mon_SBP&gt;100 &amp; Wed_SBP&gt;115, na.rm=T)&gt;1.5,1,0)) df ## # A tibble: 3 × 4 ## # Rowwise: ## Mon_SBP Tues_SBP Wed_SBP SBP ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 160 0 NA 0 ## 2 NA 0 130 0 ## 3 180 150 125 1 Sometimes, you may only want to keep the number and ignore the words in the column. This can be done using the str_extract function. df.e&lt;-data.frame(disability=c(&quot;1 - No significant disability despite symptoms; able to carry out all usual duties and activities&quot;,&quot;5 - Severe disability, bedridden, incontinent and requiring constant nursing care and attention&quot;,&quot; 1 - No significant disability despite symptoms; able to carry out all usual duties and activities&quot;)) df.e$disability2&lt;-str_extract(df.e$disability,&quot;\\\\w&quot;) #extract number 3.5 PDF to xcel Sometimes data from public government sites come in PDF form instead of excel. Conversion from pdf to excel or text can be difficult especially with special character eg Danish. There are several libraries for doing this: pdftables (require API key) and pdftools. The example below uses pdftools. available at https://docs.ropensci.org/pdftools/. The document is the 2018 Danish Stroke Registry report. The tabulizer package is excellent for converting table data. However, tabulizer package depends on rJava and requires deft handling. The PDE libray has user interface for performing data extraction from pdf. library(pdftools) ## Using poppler version 22.04.0 #Danish stroke registry txt&lt;-pdf_text(&quot;./Data-Use/4669_dap_aarsrapport-2018_24062019final.pdf&quot;) cat(txt[17]) #browse data page 13+4 filler pages ## 3. Indikatorresultater på lands-, regions- og afdelingsniveau ## Indikator 1a: Andel af patienter med akut apopleksi som indlægges inden for 3 timer ## efter symptomdebut. Standard: ≥ 30% ## ## Indikator 1b: Andel af patienter med akut apopleksi som indlægges inden for 4,5 ## timer efter symptomdebut. Standard: ≥ 40% ## ## Inden for 3 timer ## ## Uoplyst Aktuelle år Tidligere år ## Standard Tæller/ antal 2018 2017 2016 ## opfyldt nævner (%) % 95% CI % (95% CI) % (95% CI) ## ## Danmark ja 4730 / 11794 49 (0) 40 (39 - 41) 39 (38-40) 37 (36-38) ## Hovedstaden ja 1502 / 3439 49 (1) 44 (42 - 45) 40 (38-42) 40 (39-42) ## Sjælland ja 760 / 1917 0 (0) 40 (37 - 42) 39 (36-41) 40 (38-43) ## Syddanmark ja 942 / 2433 0 (0) 39 (37 - 41) 39 (37-41) 35 (33-37) ## Midtjylland ja 918 / 2590 0 (0) 35 (34 - 37) 36 (34-38) 35 (33-37) ## Nordjylland ja 577 / 1341 0 (0) 43 (40 - 46) 41 (39-44) 35 (32-37) ## Bopæl uden for Danmark ja 31 / 74 0 (0) 42 (31 - 54) 51 (36-66) 39 (26-53) ## ## Hovedstaden ja 1502 / 3439 49 (1) 44 (42 - 45) 40 (38-42) 40 (39-42) ## Albertslund ja 17 / 52 0 (0) 33 (20 - 47) 30 (18-44) 43 (27-59) ## Allerød ja 22 / 53 0 (0) 42 (28 - 56) 32 (20-46) 35 (22-50) ## Ballerup ja 43 / 106 0 (0) 41 (31 - 51) 48 (38-58) 40 (31-51) ## Bornholms Regionskommune ja 38 / 98 0 (0) 39 (29 - 49) 28 (19-38) 32 (22-43) ## Brøndby ja 45 / 113 0 (0) 40 (31 - 49) 31 (22-42) 34 (23-47) ## Dragør ja 18 / 43 0 (0) 42 (27 - 58) 47 (30-65) 33 (17-54) ## Egedal ja 45 / 95 0 (0) 47 (37 - 58) 40 (30-52) 48 (37-59) ## Fredensborg ja 36 / 99 0 (0) 36 (27 - 47) 37 (27-47) 43 (33-53) ## Frederiksberg ja 74 / 141 13 (8) 52 (44 - 61) 42 (35-50) 52 (44-61) ## Frederikssund ja 53 / 141 0 (0) 38 (30 - 46) 39 (31-48) 42 (33-51) ## Furesø ja 55 / 110 0 (0) 50 (40 - 60) 56 (44-67) 50 (38-62) ## Gentofte ja 65 / 123 1 (1) 53 (44 - 62) 46 (37-56) 38 (30-47) ## Gladsaxe ja 68 / 129 0 (0) 53 (44 - 62) 48 (38-57) 36 (27-44) ## Glostrup ja 38 / 72 0 (0) 53 (41 - 65) 40 (27-54) 48 (33-63) ## Gribskov ja 46 / 129 0 (0) 36 (27 - 45) 35 (26-44) 36 (28-46) ## Halsnæs ja 45 / 92 0 (0) 49 (38 - 60) 34 (25-45) 34 (25-44) ## Helsingør ja 50 / 129 0 (0) 39 (30 - 48) 32 (24-40) 38 (30-45) ## Herlev ja 25 / 50 0 (0) 50 (36 - 64) 52 (38-65) 33 (22-46) ## Hillerød ja 47 / 121 0 (0) 39 (30 - 48) 39 (30-49) 40 (30-50) ## Hvidovre ja 57 / 135 0 (0) 42 (34 - 51) 38 (29-48) 47 (37-57) ## ## ## 13 screenshot13&lt;- pdf_render_page(&quot;./Data-Use/4669_dap_aarsrapport-2018_24062019final.pdf&quot;, page =17) png::writePNG(screenshot13, &quot;./Data-Use/Danish-Stroke-page13.png&quot;) knitr::include_graphics(&quot;./Data-Use/Danish-Stroke-page13.png&quot;) 3.5.1 Scanned text or picture Importing data from scanned text will require use of Optical Character Recognition (OCR). The tesseract library provides an R interface for OCR. In the example below, a picture is taken from same CDC website containing mortality data (https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/04102020/ nchs-data.html). The screenshot of this website was then cleaned in paint. The data is available in the Data-Use folder. library(tesseract) eng &lt;- tesseract(&quot;eng&quot;) #english text &lt;- tesseract::ocr(&quot;./Data-Use/Covid_PNG100420.png&quot;, engine = eng) cat(text) ## NCHS Mortality Surveillance Data ## Data as of April 9, 2020 ## For the Week Ending April 4, 2020 (Week 14) ## ## COVID-19 Deaths Pneumonia Deaths* Influenza Deaths ## Year Week TotalDeaths Number %ofTotal Number %ofTotal Number %of Total ## 2019 40 52,452 0 0 2,703 5.15 16 0.03 ## 2019 4l 52,860 0 0 2,770 5.24 16 0.03 ## 2019 42 54,129 0 0 2,977 5.50 18 0.03 ## 2019 43 53,914 0 0 2,985 5.54 30 0.06 ## 2019 44 53,980 0 0 2,908 5.39 31 0.06 ## 2019 4S 55,468 0 0 3,063 5.52 31 0.06 ## 2019 46 55,684 0 0 3,096 5.56 39 0.07 ## 2019 47 55,986 0 0 2,993 5.35 50 0.09 ## 2019 48 55,238 0 0 2,976 5.38 65 0.12 ## 2019 49 56,990 0 0 3,305 5.80 99 0.17 ## 2019 50 57,276 0 0 3,448 6.02 111 0.19 ## 2019 51 56,999 0 0 3,345 5.87 125 0.22 ## 2019 52 57,956 0 0 3,478 5.99 198 0.34 ## 2020 4 58,961 0 0 3,998 6.77 416 0.71 ## 2020 2 58,962 0 0 3,995 6.76 450 0.76 ## 2020 3 57,371 0 0 3,903 6.78 44) 0.77 ## 2020 4 56,666 0 0 3,742 6.56 468 0.83 ## 2020 5 56,381 0 0 3,617 6.42 452 0.80 ## 2020 6 56,713 0 0 3,599 6.35 482 0.85 ## 2020 7 55,237 0 0 3,577 6.48 487 0.88 3.6 Web scraping The readers may ask why web scraping for healthcare. A pertinent example related to COVID-19 data is provided below. The library rvest is helpful at scraping data from an internet page. The rvest library assumes that web contents have xml document-tree representation. The different options available for web scraping with rvest are available at the website https://rvest.tidyverse.org/reference/. The user can use CSS selectors to scrape content. The library Rselenium is also useful for web scraping. For dynamic web page, the library CasperJS library does a better job especially if the data contain embedded java script. The library cdccovidview provides access to the CDC website on COVID-19. In the example below, we will try to this manually. Data from CDC website on COVID-19 is downloaded, cleaned and saved in csv format. It is important to pay attention to the data. The first row contains header and is removed. There are several columns with commas. These commas can be removed using the exercises above. Further the data is updated on weekly basis. As such the data needs to be converted into a date time format using lubridate. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) #assign handle to web page accessed 12/4/20 #cdc&lt;-read_html(&quot;https://www.cdc.gov/coronavirus/2019-ncov/covid-data/ # covidview/04102020/nchs-data.html&quot;) # scrape all div tags #html_tag &lt;- cdc %&gt;% html_nodes(&quot;div&quot;) # scrape header h1 tags #html_list&lt;-html_tag %&gt;% html_nodes(&quot;h1&quot;) %&gt;% html_text() #there is only one table on this web page #Table1&lt;- cdc %&gt;% html_node(&quot;table&quot;) %&gt;% html_table(fill = TRUE) #Table1 has a header row #Table1&lt;-Table1[-1,] #The data in the Total Deaths column has a comma #Table1$Total.Deaths&lt;-as.numeric(gsub(&quot;,&quot;,&quot;&quot;,Table1$`Total Deaths`)) #now combine the year and week column to Date #Table1$Date&lt;-lubridate::parse_date_time(paste(Table1$Year, Table1$Week, &#39;Mon&#39;, sep=&quot;/&quot;),&#39;Y/W/a&#39;) #there are still commas remaining in some columns. This is a useful exercise for the reader. A solution is provided in the next example. #write.csv(Table1,file=&quot;./Data-Use/Covid_Table100420.csv&quot;) The next example is from the CDC COVID-19 website. It poses a different challenges as there are several columns with the same names. In this case we will rename the column by index. There are several columns containing commas. Rather than removing column by column we will write a function with lapply to do it over the table. the apply function returns a matrix whereas lapply returns a dataframe. There is one column containing percentage enclosed in a bracket. This can be removed using the example above on metacharacter ie using doule back slash in front of bracket and again at close of bracket. library(rvest) library(tidyverse) cdc&lt;- read_html(&quot;https://www.cdc.gov/mmwr/volumes/69/wr/mm6915e4.htm?s_cid=mm6915e4_w&quot;) # scrape all div tags html_tag &lt;- cdc %&gt;% html_nodes(&quot;div&quot;) # scrape header h1 tags html_list&lt;-html_tag %&gt;% html_nodes(&quot;h1&quot;) %&gt;% html_text() #there is only one table on this web page Table2&lt;- cdc %&gt;% html_node(&quot;table&quot;) %&gt;% html_table(fill = TRUE) #first row is header names(Table2) &lt;- as.matrix(Table2[1, ]) Table2&lt;-Table2[-c(1:2,55),]#rows 1 and 2 are redundant #rename the columns by index names(Table2)[2] &lt;-&quot;NumberCases31.03.20&quot; names(Table2)[3]&lt;-&quot;CumulativeIncidence31.03.20&quot; names(Table2)[4]&lt;-&quot;NumberCases07.04.20&quot; names(Table2)[5]&lt;-&quot;NumberDeath07.04.20&quot; names(Table2)[6]&lt;-&quot;CumulativeIncidence07.04.20&quot; #rather than removing column by column we will write a function with lapply to remove commas over the table. the apply function returns a matrix whereas lapply returns a dataframe. Table2&lt;-as.data.frame(lapply(Table2, function(y) gsub(&quot;,&quot;, &quot;&quot;, y))) Table2&lt;-as.data.frame(lapply(Table2, function(x) gsub(&quot;\\\\(|[0-9]+\\\\)&quot;,&quot;&quot;,x))) #write.csv(Table2,file=&quot;./Data-Use/Covid_bystate_Table130420.csv&quot;) 3.7 Medical Images 3.7.1 DICOM and nifti format R can handle a variety of different data format. Medical images are stored as DICOM files for handling and converted to nifti files for analysis. The workhorses are the oro.dicom and oro.nifti libraries. Nifti is an S4 class object with multiple slots for data type. These slots can be accessed by typing the @ after the handle of the file. The values in an image can be evaluated using range function. Alternately, use cal_max and cal_min to perform the same task. It appears that in the conversion from minc file to nifti file, a scaling factor has been applied and transformed the values. library(oro.nifti) mca&lt;-readNIfTI(&quot;./Data-Use/mca_notpa.nii.gz&quot;, reorient = FALSE) range(mca) ## [1] 0.000000000 0.001411765 The slots also contain information on whether the data has been scaled. This can be checked by accessing the scl_slope and scl_inter slots. These data on slope and intercept provide a mean of returning an image to its correct value. mca@scl_slope ## [1] 1 To find available slots #find available of slots slotNames(mca) ## [1] &quot;.Data&quot; &quot;sizeof_hdr&quot; &quot;data_type&quot; &quot;db_name&quot; ## [5] &quot;extents&quot; &quot;session_error&quot; &quot;regular&quot; &quot;dim_info&quot; ## [9] &quot;dim_&quot; &quot;intent_p1&quot; &quot;intent_p2&quot; &quot;intent_p3&quot; ## [13] &quot;intent_code&quot; &quot;datatype&quot; &quot;bitpix&quot; &quot;slice_start&quot; ## [17] &quot;pixdim&quot; &quot;vox_offset&quot; &quot;scl_slope&quot; &quot;scl_inter&quot; ## [21] &quot;slice_end&quot; &quot;slice_code&quot; &quot;xyzt_units&quot; &quot;cal_max&quot; ## [25] &quot;cal_min&quot; &quot;slice_duration&quot; &quot;toffset&quot; &quot;glmax&quot; ## [29] &quot;glmin&quot; &quot;descrip&quot; &quot;aux_file&quot; &quot;qform_code&quot; ## [33] &quot;sform_code&quot; &quot;quatern_b&quot; &quot;quatern_c&quot; &quot;quatern_d&quot; ## [37] &quot;qoffset_x&quot; &quot;qoffset_y&quot; &quot;qoffset_z&quot; &quot;srow_x&quot; ## [41] &quot;srow_y&quot; &quot;srow_z&quot; &quot;intent_name&quot; &quot;magic&quot; ## [45] &quot;extender&quot; &quot;reoriented&quot; In this example below we will simulated an image of dimensions 5 by 5 by 5. see simulation using mand library. library(oro.nifti) set.seed(1234) dims = rep(5, 3) SimArr = array(rnorm(5*5*5), dim = dims) SimIm = oro.nifti::nifti(SimArr) print(SimIm) ## NIfTI-1 format ## Type : nifti ## Data Type : 2 (UINT8) ## Bits per Pixel : 8 ## Slice Code : 0 (Unknown) ## Intent Code : 0 (None) ## Qform Code : 0 (Unknown) ## Sform Code : 0 (Unknown) ## Dimension : 5 x 5 x 5 ## Pixel Dimension : 1 x 1 x 1 ## Voxel Units : Unknown ## Time Units : Unknown View the simulated image. neurobase::ortho2(SimIm) This section provides a brief introduction to viewing nifti files. Data are stored as rows, columns and slices. To view sagital image then assign a number to the row data. #plot mca #sagittal image(mca[50,,]) To see coronal image, assign a number to the column data. #plot mca #coronal image(mca[,70,]) To see axial image, assign a number to the slice data. #plot mca #axial in third column image(mca[,,35]) 3.7.2 Manipulating array of medical images These arrays of medical images should be treated no differently from any other arrays. The imaging data are stored as arrays within the .Data slot in nifti. Data can be subset using the square bracket. The image is referred to x (right to left), y (front to back), z (superior to inferior). library(oro.nifti) #extract data as array using @ function img&lt;-readNIfTI(&quot;./Data-Use/mca_notpa.nii.gz&quot;, reorient = FALSE) k&lt;-img@.Data #change x orientation to right to left 91*109*91 k1&lt;-k[91:1,,] #access slice 35 to verify that the image orientation has been switched. image(k1[,,35]) With the image now flipped to the other side, we can create an image by returning the array into a data slot. img2&lt;-img img2@.Data &lt;- k1 img2 ## NIfTI-1 format ## Type : nifti ## Data Type : 16 (FLOAT32) ## Bits per Pixel : 32 ## Slice Code : 0 (Unknown) ## Intent Code : 0 (None) ## Qform Code : 0 (Unknown) ## Sform Code : 1 (Scanner_Anat) ## Dimension : 91 x 109 x 91 ## Pixel Dimension : 2 x 2 x 2 ## Voxel Units : mm ## Time Units : sec Arrays can be manipulated to split an image into 2 separate images. Below is a function to split B0 and B1000 images from diffusion series. #split dwi file into b0 and b1000 #assume that b1000 is the second volume #dwi&lt;-readNIfTI(&quot;....nii.gz&quot;,reorient = F) DWIsplit&lt;-function(D) { DWI&lt;-readNIfTI(D,reorient = F) b1000&lt;-dwi #dim (b1000) [1] 384 384 32 2 k&lt;-dwi@.Data b1000k&lt;-k[,,,2] #dim(b1000k) [1] 384 384 32 writeNIfTI(b1000k,&quot;b1000&quot;) } Measurement of volume requires information on the dimensions of voxel. #measure volume A=&quot;./Ext-Data/3000F_mca_blur.nii&quot; VoxelDim&lt;-function(A){ library(oro.nifti) img&lt;-readNIfTI(A,reorient = F) VoxDim&lt;-pixdim(img) Volume&lt;-sum(img&gt;.5)*VoxDim[2]*VoxDim[3]*VoxDim[4]/1000 Volume } VoxelDim(A) ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## [1] 69 Find unsigned angle between 2 vectors k and k1 Morpho::angle.calc(k,k1) ## [1] 1.570771 Determine centre of gravity of an object. #centre of gravity neurobase::cog(img) ## dim1 dim2 dim3 ## 65.74983 53.09204 46.67048 3.7.3 Combining arrays This is an illustration of combining array using cbind. #stack arrays k2&lt;-cbind(k,k1) dim(k2) ### [1] 902629 2 ## [1] 902629 2 The abind function produces a different array output. Later we will repeat the same exercise using list function. #combine multi-dimensional arrays ab&lt;-abind::abind(k,k1) dim(ab) ### [1] 91 109 182 ## [1] 91 109 182 This example uses 25 files. Rather than open one file at a time create a list from pattern matching. library(oro.nifti) library(abind) library(CHNOSZ) # for working with arrays ## CHNOSZ version 2.0.0 (2023-03-13) ## reset: creating &quot;thermo&quot; object ## OBIGT: loading default database with 1904 aqueous, 3448 total species ## ## Attaching package: &#39;CHNOSZ&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following objects are masked from &#39;package:tidygraph&#39;: ## ## convert, slice ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice ## The following objects are masked from &#39;package:NMF&#39;: ## ## basis, entropy, ibasis ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## species ## The following object is masked from &#39;package:oro.nifti&#39;: ## ## slice library(RNiftyReg) ## ## Attaching package: &#39;RNiftyReg&#39; ## The following object is masked from &#39;package:rvest&#39;: ## ## forward ## The following object is masked from &#39;package:ggpubr&#39;: ## ## rotate ## The following object is masked from &#39;package:scales&#39;: ## ## rescale ## The following objects are masked from &#39;package:oro.nifti&#39;: ## ## pixdim, pixdim&lt;- #create a list using pattern matching mca.list&lt;-list.files(path=&quot;./Ext-Data/&quot;,pattern = &quot;*.nii&quot;, full.names = TRUE) #length of list length(mca.list) ## [1] 42 #read multiple files using lapply function #use lappy to read in the nifti files #note lapply returns a list mca.list.nii &lt;- lapply(mca.list, readNIfTI) ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! class(mca.list.nii) ## [1] &quot;list&quot; This example illustrates how to view the first image in the list. #view each image in the list neurobase::ortho2(mca.list.nii[[1]]) In this example, the first 3 segmented images from the list are averaged and viewed. #view average image mca_ave3&lt;-(mca.list.nii[[5]]+mca.list.nii[[6]]+mca.list.nii[[7]])/3 neurobase::ortho2(mca_ave3) The output below is the same as above but is performed on arrays. The image_data function from oro.nifti library extracts the image attribute from the slot .Data. #extract multiple arrays using lapply mca.list.array&lt;-lapply(mca.list.nii, img_data) m3&lt;-(mca.list.array[[5]]+mca.list.array[[6]]+mca.list.array[[7]])/3 #compare this with the output from above neurobase::double_ortho(m3, mca_ave3) Arrays can be extracted from list using list2array function. class(mca.list.array) ## [1] &quot;list&quot; #convert list to array #CHNOSZ function m.listarray&lt;-CHNOSZ::list2array(mca.list.array)#91 109 91 25 class(m.listarray) ## [1] &quot;array&quot; 3.7.4 Math operation on multidimensional array Before illustrating more complex operations on ultidimensional array or tensor, let’s consider the basics of multidimensional array. The rank of a tensor represents the dimension of an array. Rank 0 represents 1 dimension and so on. Here we use apply function to average over every element of the multidimensional array. The first argument of apply is the array, the second argument is the margin or the component for analysis and the last argument is the function. If the idea is to analyse the row then the margin argument is c(1), column then the margin argumen is c(2) and so on. ma&lt;-apply(m.listarray,c(1,2,3), mean) neurobase::ortho2(ma) Thresholding can be performed using mask_img function. This function can also be used to create a mask. ma_mask=neurobase::mask_img(ma, ma&gt;0.1) neurobase::double_ortho(ma_mask, ma) 3.7.5 Math operation on list In this example we us lapply to a function within a list. This is an example of a functional or a function which takes a function as an input and return a vector as an output. In this example, a functional operates on one element of the list at a time. This example of functional is the same as the function VoxelDim describes above. vox=unlist(lapply(mca.list.nii, function(A) sum(A&gt;.5)* #obtain voxel dimensions oro.nifti::pixdim(A)[2]* oro.nifti::pixdim(A)[3]* oro.nifti::pixdim(A)[4]/1000 )) vox ## [1] 0.000 0.000 0.000 0.000 69.000 3.624 8.768 58.200 326.760 1.608 ## [11] 0.000 3.904 3.328 66.776 106.976 70.928 26.912 11.768 17.592 14.720 ## [21] 151.872 87.528 0.000 24.192 80.248 152.792 69.552 25.576 0.000 8.644 ## [31] 4.230 3.781 3.643 5.720 5.061 8.644 4.230 3.781 3.643 5.720 ## [41] 5.061 43.968 3.7.6 Vectorising nifti object One way of handling imaging data for analysis is to flatten the image, then create an empty array of the same size to return the image. #flatten 3D niivector = as.vector(img[,,]) #902629 #Create empty array of same size to fill up niinew = array(0, dim=dim(img)) #return to 3D niinew = array(niivector, dim=dim(img)) #confirm neurobase::ortho2(niinew) Another way of creating vector from the image is to use c function. #vector can also be created using c niivector2 = c(img[,,]) #902629 3.7.7 tar file Image files can be large and are often stored as tar files. The tar (tgz file), untar, zip (gz file) and unzip function are from the utils library. colin_1mm&lt;-untar(&quot;./Data-Use/colin_1mm.tgz&quot;) colinIm&lt;-readNIfTI(&quot;colin_1mm&quot;) #1 x 1 x 1 class(colinIm) ## [1] &quot;nifti&quot; ## attr(,&quot;package&quot;) ## [1] &quot;oro.nifti&quot; neurobase::ortho2(colinIm) The readNIfTI call can open gz file without the need to call unzip function. library(RNiftyReg) epi_t2&lt;- readNIfTI(system.file(&quot;extdata&quot;, &quot;epi_t2.nii.gz&quot;, package=&quot;RNiftyReg&quot;)) class(epi_t2) ## [1] &quot;nifti&quot; ## attr(,&quot;package&quot;) ## [1] &quot;oro.nifti&quot; neurobase::ortho2(epi_t2) 3.7.8 Image registration There are many different libraries for performing registration. library(RNiftyReg) #example from data source&lt;-readNifti(&quot;./Data-Use/mca10_pca10_border10.nii.gz&quot;) pixdim(source) ## [1] 2 2 2 colin_1mm&lt;-untar(&quot;./Data-Use/colin_1mm.tgz&quot;) target&lt;-readNifti(&quot;colin_1mm&quot;) pixdim(target) ## [1] 1 1 1 1 target ## Image array of mode &quot;double&quot; (54.2 Mb) ## - 181 x 217 x 181 x 1 voxels ## - 1 x 1 x 1 mm x 1 s per voxel #register source to target result &lt;- niftyreg(source, target) #affine transformation result$forwardTransforms ## [[1]] ## NiftyReg affine matrix: ## 0.773388 0.569432 0.471194 4.534728 ## -0.595046 0.720880 -0.807965 -19.901979 ## -0.562192 -0.069332 0.552411 21.512533 ## 0.000000 0.000000 0.000000 1.000000 #image in target space result$image ## Image array of mode &quot;double&quot; (54.2 Mb) ## - 181 x 217 x 181 voxels ## - 1 x 1 x 1 mm per voxel neurobase::ortho2(result$image) Output from RNiftyReg are niftiImage objects. They can be converted to oro.nifti objects using nii2oro function. otarget&lt;-nii2oro(target) oimage&lt;-nii2oro(result$image) overlay(otarget, y=oimage,z = 90, plot.type = &quot;single&quot; ) 3.7.9 Rescaling Perform affine registration and resampling of image using the transformation file. #affine ica1000&lt;-readNifti(&quot;./Ext-Data/1000M_ica.nii&quot;) ica1000 ## Image array of mode &quot;double&quot; (6.9 Mb) ## - 91 x 109 x 91 voxels ## - 2 x 2 x 2 mm per voxel colin_affine&lt;-buildAffine(source=ica1000, target=target) #apply transformation from above #assume 1000M_ica.nii and source are in the same space colin_like&lt;-applyTransform(colin_affine,ica1000) neurobase::ortho2(colin_like) Resampling an image to different dimensions. This example is different from above in which rescaling is performed as part of registration to higher resolution image. Here the rescale function from RNiftyReg library is used to change the dimensions from 1x1x1 mm to 2x2x2 mm. ica1000.rescale&lt;-RNiftyReg::rescale(ica1000,c(.5,.5,.5)) ica1000 ## Image array of mode &quot;double&quot; (6.9 Mb) ## - 91 x 109 x 91 voxels ## - 2 x 2 x 2 mm per voxel #compare with rescale ica1000.rescale ## Image array of mode &quot;double&quot; (855.6 Kb) ## - 45 x 54 x 45 voxels ## - 4 x 4 x 4 mm per voxel 3.7.10 MNI template There are several different MRI templates. The well known one is the MNI 152 template (Mazziotta J 2001). This was developed from male right-handed medical students. The MNI 152 is also known as International Consortium for Brain Mapping (ICBM) 152. 3.7.11 Atlases A list of available atlases for human and animals is available at https://loni.usc.edu/research/atlases. 3.7.11.1 AAL atlas The automated anatomical labeling (AAL) atlas of activation contains 45 volume of interest in each hemisphere (Tzourio-Mazoyer N 2002). The atlas is aligned to MNI 152 coordinates. The updated AAL has additional parcellation of orbitofrontal cortex. AAL3 update includes further parcellation of thalamus. library(rgl) library(misc3d) library(MNITemplate) #source(&quot;https://neuroconductor.org/neurocLite.R&quot;) #neuro_install(&#39;aal&#39;) library(aal) library(neurobase) library(magick) ## Linking to ImageMagick 6.9.12.3 ## Enabled features: cairo, freetype, fftw, ghostscript, heic, lcms, pango, raw, rsvg, webp ## Disabled features: fontconfig, x11 library(oro.nifti) img = aal_image() template = readMNI(res = &quot;2mm&quot;) cut &lt;- 4500 dtemp &lt;- dim(template) # All of the sections you can label labs = aal_get_labels() # highlight - in this case the Cingulate_Post_L cingulate = labs$index[grep(&quot;Cingulate_Post_R&quot;, labs$name)] #mask of object for rendering mask = remake_img(vec = img %in% cingulate, img = img) #contour for MNI template contour3d(template, x=1:dtemp[1], y=1:dtemp[2], z=1:dtemp[3], level = cut, alpha = 0.1, draw = TRUE) #contour for mask contour3d(mask, level = c(0.5), alpha = c(0.5), add = TRUE, color=c(&quot;red&quot;) ) ### add text text3d(x=dtemp[1]/2, y=dtemp[2]/2, z = dtemp[3]*0.98, text=&quot;Top&quot;) text3d(x=-0.98, y=dtemp[2]/2, z = dtemp[3]/2, text=&quot;Right&quot;) #create movie #movie file is saved to temporary folder #movie3d(spin3d(),duration=30) #add digital map of mca territory MCA&lt;-readNIfTI(&quot;./Data-Use/MCA_average28_MAP_100.nii&quot;) ## Malformed NIfTI - not reading NIfTI extension, use at own risk! #mask2 = remake_img(vec = img %in% source, img = img) contour3d(template, x=1:dtemp[1], y=1:dtemp[2], z=1:dtemp[3], level = cut, alpha = 0.1, draw = TRUE) #contour for mask contour3d(MCA, level = c(0.5), alpha = c(0.5), add = TRUE, color=c(&quot;Yellow&quot;)) #add frontal angular = labs$index[grep(&quot;Angular_R&quot;, labs$name)] #mask of object for rendering mask2 = remake_img(vec = img %in% angular, img = img) contour3d(mask2, level = c(0.5), alpha = c(0.5), add = TRUE, color=c(&quot;red&quot;)) #add cingulate contour3d(mask, level = c(0.5), alpha = c(0.5), add = TRUE, color=c(&quot;blue&quot;)) ### add text text3d(x=dtemp[1]/2, y=dtemp[2]/2, z = dtemp[3]*0.98, text=&quot;Top&quot;) text3d(x=-0.98, y=dtemp[2]/2, z = dtemp[3]/2, text=&quot;Right&quot;) #create movie #movie file is saved to temporary folder #movie3d(spin3d(),duration=5) #rglwidget() 3.7.11.2 Eve template The Eve template is from John Hopkins (“Atlas-Based Whole Brain White Matter Analysis Using Large Deformation Diffeomorphic Metric Mapping: Application to Normal Elderly and Alzheimer’s Disease Participants.” 2009). It is a single subject high resolution white matter atlas that has been morphed into MNI 152 coordinates. The atlas is parcellated into 176 regions based on ICBM-DTI-81 atlas. #source(&quot;https://neuroconductor.org/neurocLite.R&quot;) #neuro_install(&#39;EveTemplate&#39;, release = &quot;stable&quot;, release_repo = &quot;github&quot;) library(EveTemplate) eve_labels = readEveMap(type = &quot;II&quot;) eve_labels ## NIfTI-1 format ## Type : nifti ## Data Type : 4 (INT16) ## Bits per Pixel : 16 ## Slice Code : 0 (Unknown) ## Intent Code : 0 (None) ## Qform Code : 2 (Aligned_Anat) ## Sform Code : 1 (Scanner_Anat) ## Dimension : 181 x 217 x 181 ## Pixel Dimension : 1 x 1 x 1 ## Voxel Units : mm ## Time Units : Unknown neurobase::ortho2(eve_labels) library(RColorBrewer) library(tidyverse) unique_labs = eve_labels %&gt;% c %&gt;% unique %&gt;% sort breaks = unique_labs rf &lt;- colorRampPalette(rev(brewer.pal(11,&#39;Spectral&#39;))) cols &lt;- rf(length(unique_labs)) neurobase::ortho2(eve_labels, col = cols, breaks = c(-1, breaks)) 3.7.11.3 Sensorimotor tract atlas Template for corticofugal tracts from primary motor cortex, dorsal premotor cortex, ventral premotor cortex, supplementary motor area (SMA), pre-supplementary motor area (preSMA), and primary somatosensory cortex is available from (Derek B Archer 2018). Below is an illustration of the M1 tract obtained from LRNLAB. RightM1&lt;-readNifti(&quot;./Ext-Data/Right-M1-S-MATT.nii&quot;) neurobase::ortho2(RightM1) #to down weight the voxel from 1 mm to 2 mm RightM1.rescale&lt;-RNiftyReg::rescale(RightM1,c(.5,.5,.5)) RightM1o&lt;-nii2oro(RightM1) #dimensions of RightM1o (182 218 182) and eve_labels (181 217 181) are dissimilar #overlay(eve_labels, y=RightM1o, z=90,plot.type=&quot;single&quot;) 3.8 ECG Signal processing The ECG records the electrical signal from the heart. In a clinical record it is performed as a 12 lead recording. The ECG contains an initial P wave which originates from the atrium. The P wave is absent in patients with atrial fibrillation. The RR interval is used to measure the heart rate. A false assumption is that the heart rate is constant but measurement shows beat to beat variation. Heart rate variability is a feature of health. 3.8.0.1 ECG data The ecg dataset of one patient contains 2048 observations collected at a rate of 180 samples per second. library(ade4) ## ## Attaching package: &#39;ade4&#39; ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## score data(&quot;ecg&quot;) #ts object #Time Series: #Start = 0.31 #End = 11.6822222222222 #Frequency = 180 head(ecg) ## [1] -0.104773 -0.093136 -0.081500 -0.116409 -0.081500 -0.116409 plot(ecg) Let’s look now at the Hart dataset. ECG&lt;-read.csv(&quot;../../DataMining/python_journey/Heart/ECG/data.csv&quot;) #ECG is a dataframe object plot(as.ts(ECG)) # from base R create a ts object by providing starting and end time and frequency. the plot of the data now has a new x limit. ECG1&lt;-ts(ECG$hart, start=c(0.31,13.7), frequency=180) plot(ECG1) EEGECG&lt;-read.csv(&quot;../../DataMining/python_journey/Heart/ECG/eeg_stroke_ecg.csv&quot;) #this data has 2 columns time and ECG plot(as.ts(EEGECG$ECG)) ECG2&lt;-ts(EEGECG$ECG, start=c(0.31,6.75), frequency=60) plot(ECG2) Here we illustrate the use of RHRV package to analyse ECG signal (Rodriguez-Linares et al. 2017). The code is provided on the RHRV website https://rhrv.r-forge.r-project.org/documentation.html. The example.beats data was download from this website and stored in the Data-Use folder. Here we make changes regarding the path of the data. library(RHRV) ## Loading required package: waveslim ## ## waveslim: Wavelet Method for 1/2/3D Signals (version = 1.8.4) ## ## Attaching package: &#39;waveslim&#39; ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## basis ## The following object is masked from &#39;package:lubridate&#39;: ## ## pm ## The following object is masked from &#39;package:NMF&#39;: ## ## basis ## Loading required package: nonlinearTseries ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## ## Attaching package: &#39;nonlinearTseries&#39; ## The following object is masked from &#39;package:grDevices&#39;: ## ## contourLines ## Loading required package: lomb hrv.data = CreateHRVData() hrv.data = SetVerbose(hrv.data, TRUE) #setwd(&quot;C:/RHRV&quot;) hrv.data = LoadBeatAscii(hrv.data, &quot;example.beats.txt&quot;, RecordPath = &quot;./Data-Use/&quot; ) ## Loading beats positions for record: example.beats.txt ## Path: ./Data-Use/ ## Scale: 1 ## Date: 01 / 01 / 1900 ## Time: 00 : 00 : 00 ## Number of beats: 17360 plot(hrv.data$Beat$Time) hrv.data = BuildNIHR(hrv.data) ## Calculating non-interpolated heart rate ## Number of beats: 17360 PlotNIHR(hrv.data) ## Plotting non-interpolated instantaneous heart rate ## Number of points: 17360 hrv.data = FilterNIHR(hrv.data) ## Filtering non-interpolated Heart Rate ## Number of original beats: 17360 ## Number of accepted beats: 17248 PlotNIHR(hrv.data) ## Plotting non-interpolated instantaneous heart rate ## Number of points: 17248 3.9 EEG signal processing The eegUtils package has useful methods for plotting EEG. https://craddm.github.io/eegUtils/index.html. According to the site, it has functions for importing data from Biosemi, Brain Vision Analyzer, and EEGLAB. #devtools::install_github(&quot;mne-tools/mne-r&quot;) #remotes::install_github(&quot;craddm/eegUtils@develop&quot;) library(eegUtils) ## ## Attaching package: &#39;eegUtils&#39; ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following objects are masked from &#39;package:plyr&#39;: ## ## mutate, rename ## The following object is masked from &#39;package:stats&#39;: ## ## filter plot_butterfly(demo_epochs) ## Creating epochs based on combinations of variables: epoch_label participant_id topoplot(demo_epochs, time_lim = c(.22, .25 )) ## Creating epochs based on combinations of variables: epoch_label participant_id ## Using electrode locations from data. ## Plotting head r 95 mm The following data is from https://datashare.ed.ac.uk/handle/10283/2189 library(eegUtils) library(R.matlab) limo_test &lt;- import_set(&quot;limo_dataset_S1.set&quot;) limo_cont &lt;- R.matlab::readMat(&quot;continuous_variable.mat&quot;) limo_cat &lt;- readr::read_csv(&quot;categorical_variable.txt&quot;, col_names = c(&quot;cond_lab&quot;)) The MNE-R package is an interface to MNE package in Python. 3.10 Python Python can be run directly from R markdown file. It requires that python be specified instead of R. The use of python here requires that Miniconda or Anaconda be installed. Installation of [Miniconda][Minconda environment] via reticulate package will be shown below. import pandas as pd import matplotlib.pyplot as plt import numpy as np #dataset containing locations of ms clinics in Victoria dataset = pd.read_csv(&#39;./Data-Use/msclinic.csv&#39;) #Read data from CSV datafile dat=pd.DataFrame(dataset) #print 10 rows print(dat.head(10)) ## id public clinic ... lat lon metropolitan ## 0 mmc 1 1 ... -37.920668 145.123387 1 ## 1 rmh 1 1 ... -37.778945 144.946894 1 ## 2 aus 1 1 ... -37.756412 145.060279 1 ## 3 alf 1 1 ... -37.845960 144.981852 1 ## 4 stvin 1 1 ... -37.807586 144.975029 1 ## 5 bhh 1 1 ... -37.813525 145.118510 1 ## 6 frankston 1 1 ... -38.151211 145.129160 1 ## 7 geelong 1 1 ... -38.152047 144.364647 0 ## 8 sunshine 1 1 ... -37.760219 144.815301 1 ## 9 northern 1 0 ... -37.652886 145.014486 1 ## ## [10 rows x 10 columns] exit ## Use exit() or Ctrl-Z plus Return to exit Passing Python object to R and py$ in front of Python object in R. head(py$dat) To pass a R object to python then add r. in front of object. #The ECG data is now passed as r.ECG print(r.ECG) ## hart ## 0 530 ## 1 518 ## 2 506 ## 3 494 ## 4 483 ## ... ... ## 2478 489 ## 2479 491 ## 2480 492 ## 2481 493 ## 2482 494 ## ## [2483 rows x 1 columns] The data can now be plotted using matplotlib library from Python. import pandas as pd import matplotlib.pyplot as plt import numpy as np import math #Matplotlib plt.title(&quot;Heart Rate Signal&quot;) #The title of our plot #ECG is data frame and hart is the column plt.plot(r.ECG.hart) #Draw the plot object #Display the plot plt.show() exit 3.10.1 Reticulate Information on the use of Python in R is available at https://rstudio.github.io/reticulate/. The package reticulate can import Python function to work directly in R. Note that the chunk code heading here is r. library(reticulate) os &lt;- import(&quot;os&quot;) #os is operating system package os$listdir(&quot;.&quot;) ## [1] &quot;.git&quot; &quot;.gitignore&quot; ## [3] &quot;.Rhistory&quot; &quot;.Rproj.user&quot; ## [5] &quot;.travis.yml&quot; &quot;01-intro.Rmd&quot; ## [7] &quot;02-Data-Wrangling.Rmd&quot; &quot;03-Statistics.Rmd&quot; ## [9] &quot;04-multivariate-analysis.Rmd&quot; &quot;05-machinelearning.Rmd&quot; ## [11] &quot;06-machinelearningpt2.Rmd&quot; &quot;07-Bayesian-analysis.Rmd&quot; ## [13] &quot;08-operational-research.Rmd&quot; &quot;09-graph-theory.Rmd&quot; ## [15] &quot;10-geospatial-analysis.Rmd&quot; &quot;11-App.Rmd&quot; ## [17] &quot;12-Appendix.Rmd&quot; &quot;13-references.Rmd&quot; ## [19] &quot;Applications-of-R-in-Healthcare.bbl&quot; &quot;Applications-of-R-in-Healthcare.blg&quot; ## [21] &quot;Applications-of-R-in-Healthcare.Rmd&quot; &quot;Applications-of-R-in-Healthcare.tex&quot; ## [23] &quot;Applications-of-R-in-Healthcare.toc&quot; &quot;Applications-of-R-in-Healthcare_files&quot; ## [25] &quot;Applications_of_R_in_Healthcare_files&quot; &quot;bbc.png&quot; ## [27] &quot;book.bib&quot; &quot;bookdown-demo.log&quot; ## [29] &quot;bookdown-demo.tex&quot; &quot;bookdown-demo_files&quot; ## [31] &quot;bym2.stan&quot; &quot;bym_predictor_plus_offset.stan&quot; ## [33] &quot;car.stan&quot; &quot;car_ht.stan&quot; ## [35] &quot;colin_1mm.hdr&quot; &quot;colin_1mm.img&quot; ## [37] &quot;colin_1mm.mat&quot; &quot;cta.csv&quot; ## [39] &quot;Data-Use&quot; &quot;DESCRIPTION&quot; ## [41] &quot;desktop.ini&quot; &quot;DKnut2.graph&quot; ## [43] &quot;Dockerfile&quot; &quot;Ext-Data&quot; ## [45] &quot;Fagan_SpotSign.png&quot; &quot;HealthcareRbook.Rproj&quot; ## [47] &quot;index.Rmd&quot; &quot;LICENSE&quot; ## [49] &quot;lm.rds&quot; &quot;lm.stan&quot; ## [51] &quot;lmr.rds&quot; &quot;lmr.stan&quot; ## [53] &quot;Logistic_GeneticAlgorithm.Rda&quot; &quot;Logistic_SimulatedAnnealing.Rda&quot; ## [55] &quot;NGS_publications_per_journal.txt&quot; &quot;NGS_publications_per_year.txt&quot; ## [57] &quot;normal_regression.stan&quot; &quot;now.json&quot; ## [59] &quot;NY.graph&quot; &quot;Occipital-Visual.html&quot; ## [61] &quot;Occipital-Visual.png&quot; &quot;packages.bib&quot; ## [63] &quot;preamble.tex&quot; &quot;README.md&quot; ## [65] &quot;rodentData.xlsx&quot; &quot;schools.stan&quot; ## [67] &quot;style.css&quot; &quot;svc.stan&quot; ## [69] &quot;toc.css&quot; &quot;total_journal.Rda&quot; ## [71] &quot;world_income.png&quot; &quot;_book&quot; ## [73] &quot;_bookdown.yml&quot; &quot;_bookdown_files&quot; ## [75] &quot;_build.sh&quot; &quot;_deploy.sh&quot; ## [77] &quot;_output.yml&quot; Here we provide another example on how to use Python in R. Note the change in the way we extract the stats module from scipy Python package. data(&quot;mtcars&quot;) #mtcars data in R library(reticulate) np&lt;-import(&quot;numpy&quot;) pd&lt;-import(&quot;pandas&quot;) #equivalent in Python is from scipy import stats sc&lt;-import(&quot;scipy&quot;) sc$stats$linregress(mtcars$mpg,mtcars$cyl) ## LinregressResult(slope=-0.2525149506667544, intercept=11.260683180739264, rvalue=-0.8521619594266132, pvalue=6.112687142580981e-10, stderr=0.02830980675303087, intercept_stderr=0.5930361857152716) 3.10.2 Minconda Miniconda and Anaconda can be installed directly from its website. Here we will illustrate installation of Miniconda from Rstudio. The install_miniconda function from reticulate library download Miniconda from the web. #library(reticulate) #this function is turned off as it only needs to be done once #install_miniconda(path = miniconda_path(), update = TRUE, force = FALSE) To find the libraries install in Miniconda conda list 3.10.3 Python environment Unless specified, the default environment is r-reticulate. Setting the Python environment is important to avoid package incompatibility. To set the environment library(reticulate) #virtualenv_create(&quot;SignalProcessing&quot;) Some Python libraries such as pycox can be installed in R using install_py… this way.Some python packages have library(reticulate) #library(survivalmodels) #install pycox for survivalmodels #install_pycox(pip = TRUE, install_torch = TRUE) #install_keras(pip = TRUE, install_tensorflow = TRUE) #install other Python packages #this is similar to pip install torch #install_torch(method = &quot;auto&quot;, conda = &quot;auto&quot;, pip = TRUE) 3.11 Matlab The package R.mat provides interface between R and Matlab. This is useful for running EEG analysis where we import analysis from Matlab for visualisation in R. Matlab must be installed to for this to run. 3.12 Stata The package Rs provides interface between R and Stata. Stata must be installed to for this to run. Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure ??. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 3.1. knitr::kable( df, caption = &#39;Table containing uncleaned data above&#39;, booktabs = TRUE ) Table 3.1: Table containing uncleaned data above Mon_SBP Tues_SBP Wed_SBP SBP 160 0 NA 0 NA 0 130 0 180 150 125 1 knitr::kable( Table2, caption = &#39;Table containing cleaned data above&#39;, booktabs = TRUE ) Table 3.2: Table containing cleaned data above Jurisdiction NumberCases31.03.20 CumulativeIncidence31.03.20 NumberCases07.04.20 NumberDeath07.04.20 CumulativeIncidence07.04.20 Absolute.change.in.cumulative.incidence. Alabama 999 20.4 2197 39 1. 44.9 24.5 Alaska 133 18.0 213 6 2. 28.9 10.8 Arizona 1289 18.0 2575 73 2. 35.9 17.9 Arkansas 560 18.6 993 18 1. 32.9 14.4 California 8131 20.6 15865 374 2. 40.1 19.6 Colorado 2966 52.1 5429 179 3. 95.3 43.2 Connecticut 3128 87.6 7781 277 3. 217.8 130.2 Delaware 319 33.0 928 16 1. 95.9 63.0 District of Columbia 495 70.5 1211 24 2. 172.4 101.9 Florida 6490 30.5 14302 296 2. 67.1 36.7 Georgia 4585 43.6 9713 351 3. 92.3 48.7 Hawaii 185 13.0 362 5 1. 25.5 12.5 Idaho 525 29.9 1210 15 1. 69.0 39.0 Illinois 5994 47.0 13549 380 2. 106.3 59.3 Indiana 2159 32.3 5507 173 3. 82.3 50.0 Iowa 497 15.7 1048 26 2. 33.2 17.5 Kansas 428 14.7 900 27 3. 30.9 16.2 Kentucky 591 13.2 1149 65 5. 25.7 12.5 Louisiana 5237 112.4 16284 582 3. 349.4 237.1 Maine 303 22.6 519 12 2. 38.8 16.1 Maryland 1660 27.5 5529 124 2. 91.5 64.0 Massachusetts 6620 95.9 15202 356 2. 220.3 124.3 Michigan 7615 76.2 18970 845 4. 189.8 113.6 Minnesota 689 12.3 1154 39 3. 20.6 8.3 Mississippi 1073 35.9 2003 67 3. 67.1 31.1 Missouri 1327 21.7 3037 53 1. 49.6 27.9 Montana 203 19.1 332 6 1. 31.3 12.1 Nebraska 177 9.2 478 10 2. 24.8 15.6 Nevada 1113 36.7 2087 71 3. 68.8 32.1 New Hampshire 367 27.1 747 13 1. 55.1 28.0 New Jersey 18696 209.9 44416 1232 2. 498.6 288.7 New Mexico 315 15.0 794 13 1. 37.9 22.9 New York† 32656 293.1 61897 1378 2. 555.5 262.4 New York City 41771 497.3 76876 4111 5. 915.3 418.0 North Carolina 1584 15.3 3221 46 1. 31.0 15.8 North Dakota 126 16.6 237 4 1. 31.2 14.6 Ohio 2199 18.8 4782 167 3. 40.9 22.1 Oklahoma 565 14.3 1472 67 4. 37.3 23.0 Oregon 690 16.5 1181 33 2. 28.2 11.7 Pennsylvania 4843 37.8 14559 240 1. 113.7 75.9 Rhode Island 520 49.2 1414 30 2. 133.7 84.6 South Carolina 1083 21.3 2417 51 2. 47.5 26.2 South Dakota 108 12.2 320 6 1. 36.3 24.0 Tennessee 2239 33.1 4139 72 1. 61.1 28.1 Texas 3266 11.4 8262 154 1. 28.8 17.4 Utah 934 29.5 1804 13 0. 57.1 27.5 Vermont 293 46.8 575 23 4. 91.8 45.0 Virginia 1484 17.4 3645 75 2. 42.8 25.4 Washington 4896 65.0 8682 394 4. 115.2 50.2 West Virginia 162 9.0 412 4 1. 22.8 13.8 Wisconsin 1351 23.2 2578 92 3. 44.3 21.1 Wyoming 120 20.8 221 0 —) 38.3 17.5 American Samoa 0 0.0 0 0 —) 0.0 0.0 Federated States of Micronesia 0 0.0 0 0 —) 0.0 0.0 Guam 71 42.8 122 4 3. 73.6 30.8 Marshall Islands 0 0.0 0 0 —) 0.0 0.0 Northern Mariana Islands 2 3.5 8 2 25. 14.1 10.5 Palau 0 0.0 0 0 —) 0.0 0.0 Puerto Rico 239 7.5 573 23 4. 17.9 10.5 U.S. Virgin Islands 30 28.0 45 1 2. 42.1 14.0 U.S. Total 186101 56.2 395926 12757 3. 119.6 63.4 References "],["statistics.html", "Chapter 4 Statistics 4.1 Univariable analyses 4.2 Regression 4.3 Confounder 4.4 Causal inference 4.5 Special types of regression 4.6 Sample size estimation 4.7 Randomised clinical trials 4.8 Diagnostic test 4.9 Metaanalysis 4.10 Data simulation", " Chapter 4 Statistics This section is not intended as a textbook on statistics. Rather it demonstrates regression approaches that can be used including sample size estimation, R codes provided. 4.1 Univariable analyses 4.1.1 Parametric tests T-test is the workhorse for comparing if 2 datasets are have the same distribution. Performing t-test in R requires data from 2 columns: one containing the variables for comparison and one to label the group. There are different forms of t-test depending on whether the two samples are paired or unpaired. In general, the analysis takes the form of \\(t=\\frac{\\mu_1 - \\mu_2}{variance}\\). It is recommended to check the distribution of the data by using histogram. For this exercise, we will use the simulated data from ECR trials. The grouping variable is the trial assignment. #comparison of early neurological recovery (ENI) by tral (T) dtTrial&lt;-read.csv(&quot;./Data-Use/dtTrial_simulated.csv&quot;) t.test(dtTrial$ENI~dtTrial$T) ## ## Welch Two Sample t-test ## ## data: dtTrial$ENI by dtTrial$T ## t = 0.17454, df = 487.36, p-value = 0.8615 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -0.04569535 0.05460540 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.3084644 0.3040094 4.1.2 Non-parametric tests Chi-squared and Fisher-exact tests can be done by using the table function for setting up the count data into 2 x 2 contingency table or confusion matrix. The formula for the Chi-squared test takes on a familiar form \\(\\chi^2=\\frac{(observed-expected)^2}{expected}\\). In this example we will use the data above. table(dtTrial$HT,dtTrial$T) ## ## 0 1 ## 0 112 101 ## 1 144 143 chisq.test(dtTrial$HT,dtTrial$T) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: dtTrial$HT and dtTrial$T ## X-squared = 0.19553, df = 1, p-value = 0.6584 The Wilcoxon rank sum test is performed with continuous data organised in the same way as the t-test. There are several different approaches to performing Wilcoxon rank sum test. The coin package allows handling of ties. library(coin) ## ## Attaching package: &#39;coin&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## pvalue wilcox.test(ENI~T, data=dtTrial) ## ## Wilcoxon rank sum test with continuity correction ## ## data: ENI by T ## W = 31159, p-value = 0.9642 ## alternative hypothesis: true location shift is not equal to 0 4.2 Regression There are many different form of regression methods. A key principle is that the predictors are independent of each others. This issue will be expand on in the later in the section on collinearity. Special methods are required when the predictors are collinear. 4.2.1 Brief review of matrix A vector is has length one. A matrix is an ordered array in 2 dimensions. A tensor is an ordered array in 3 dimensions. A matrix in which the columns are linearly related are said to be rank deficient. The rank of a given matrix is an expression of the number of linearly independent columns of that matrix. Given that row rank and column rank are equivalent, rank deficiency of a matrix is expressed as the difference between the lesser of the number of rows and columns, and the rank of the matrix. A matrix with rank of 1 is likely to be linearly related. 4.2.2 Linear (least square) regression Least square regression uses the geometric properties of Euclidean geometry to identify the line of best. The sum of squares \\(SSE\\) is \\(\\sum(observed-expected)^2\\). The \\(R^2\\) is a measure of the fit of the model. It is given by \\(1-\\frac{SS_(res)}{SS_(total)}\\). Low \\(R^2\\) indicates a poorly fitted model and high \\(R^2\\) indicates excellent fitting. The assumption here is that the outcome variable is a continuous variable. library(ggplot2) load(&quot;./Data-Use/world_stroke.Rda&quot;) ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ geom_smooth(method=&quot;lm&quot;, aes(Group=Income, linetype=Income))+geom_point()+xlab(&quot;Life Expectancy&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 4.2.3 Logistic regression For outcome that are binary in nature such as yes or no, then least square regression is not appropriate. There are no close form solution for this analysis and a numerical approach using maximum likelihood approach is needed. When examining the results of logistic regression one is often enchanted by the large odds ratio. It is important to look at the metrics of model calibration (discussed below). A clue to a poorly calibrated model is the observation that the width of the confidence interval for odds ratio is wide. #glm data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) #remove id column and column with NA to feed into iml later BreastCancer2&lt;-lapply(BreastCancer[,-c(1,7)], as.numeric) BreastCancer2&lt;-as.data.frame(BreastCancer2) DCa&lt;-glm(Class~., data=BreastCancer2) summary(DCa) ## ## Call: ## glm(formula = Class ~ ., data = BreastCancer2) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.723054 0.018613 38.847 &lt; 2e-16 *** ## Cl.thickness 0.042754 0.003992 10.709 &lt; 2e-16 *** ## Cell.size 0.019263 0.007293 2.641 0.00845 ** ## Cell.shape 0.032217 0.007016 4.592 5.22e-06 *** ## Marg.adhesion 0.021463 0.004386 4.893 1.24e-06 *** ## Epith.c.size 0.011637 0.005965 1.951 0.05148 . ## Bl.cromatin 0.035266 0.005650 6.241 7.57e-10 *** ## Normal.nucleoli 0.016928 0.004247 3.986 7.44e-05 *** ## Mitoses 0.001086 0.006048 0.180 0.85757 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.0481263) ## ## Null deviance: 157.908 on 698 degrees of freedom ## Residual deviance: 33.207 on 690 degrees of freedom ## AIC: -126.1 ## ## Number of Fisher Scoring iterations: 2 4.2.4 Discrimination and Calibration A high _\\(R^2\\) suggests that the linear regression model is well calibrated. This metric is often not displayed but should be sought when interpreting the data. The areas under the receiver operating characteristic curve (AUC) is used to assess how well the models discriminate between those who have the disease and those who do not have the disease of interest. An AUC of 0.5 is classified as no better than by chance; 0.8 to 0.89 provides good (excellent) discrimination, and 0.9 to 1.0 provides outstanding discrimination. This rule of thumb about interpreting AUC when reading the literature is language the authors used to describe the AUC. This test of discrimination is not synonymous with calibration. It is possible to have a model with high discrimination but poor calibration (Diamond 1992). The AUC is similar to Harrell’s c-index but the interpretation of difference in AUC and c-index between models is not straightforward. A difference in 0.1 of AUC correspond to the number of subject rank correctly. The c-index was originally described for survival analysis (Harrell FE Jr 1982). Harrell described the c-index (concordance index) as estimating the probability that, of two randomly chosen patients, the patient with the higher prognostic score will outlive the patient with the lower prognostic score. As such the c-index should be interpreted as the number of concordant pairs relative to the total number of comparable pairs. It has been proposed that the AUC and c-index is not appropriate for survival analysis as they do not account for the dynamic nature of the data(Longato, Vettoretti, and Di Camillo 2020). The integrated Graf score has been proposed to account for difference in the estimated event-free survival probabilities with the actual outcome (Graf E 1999). Calibration of logistic regression model is performed using the Hosmer–Lemeshow goodness-of-ﬁt test and the Nagelkerke generalized R2. A model is well calibrated when the Hosmer–Lemeshow goodness-of-ﬁt test shows no difference between observed and expected outcome or P value approaching 1. A high generalized \\(R^2\\) value suggests a well-calibrated regression model. Running regression through the rms or PredictABEL library provide these results. The generalized \\(R^2\\) can be obtained manually from base R by running an intercept only model and again with covariates. It is given by \\(1-\\frac{L1}{L0}\\). #lrm on logistic regression analysis for Breast Cancer library(rms) ## Loading required package: Hmisc ## ## Attaching package: &#39;Hmisc&#39; ## The following object is masked from &#39;package:RNiftyReg&#39;: ## ## translate ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## mtitle ## The following object is masked from &#39;package:plotly&#39;: ## ## subplot ## The following objects are masked from &#39;package:plyr&#39;: ## ## is.discrete, summarize ## The following objects are masked from &#39;package:dplyr&#39;: ## ## src, summarize ## The following object is masked from &#39;package:Biobase&#39;: ## ## contents ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## ## Attaching package: &#39;rms&#39; ## The following object is masked from &#39;package:lomb&#39;: ## ## lsp DCa_rms&lt;-lrm(Class~., data=BreastCancer2) DCa_rms ## Logistic Regression Model ## ## lrm(formula = Class ~ ., data = BreastCancer2) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 699 LR chi2 759.86 R2 0.915 C 0.993 ## 1 458 d.f. 8 R2(8,699)0.659 Dxy 0.986 ## 2 241 Pr(&gt; chi2) &lt;0.0001 R2(8,473.7)0.795 gamma 0.986 ## max |deriv| 8e-10 Brier 0.028 tau-a 0.446 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept -9.9477 1.0317 -9.64 &lt;0.0001 ## Cl.thickness 0.5776 0.1190 4.85 &lt;0.0001 ## Cell.size -0.0115 0.1759 -0.07 0.9479 ## Cell.shape 0.5679 0.1913 2.97 0.0030 ## Marg.adhesion 0.3137 0.1004 3.13 0.0018 ## Epith.c.size 0.1306 0.1406 0.93 0.3529 ## Bl.cromatin 0.5800 0.1456 3.98 &lt;0.0001 ## Normal.nucleoli 0.1232 0.0987 1.25 0.2120 ## Mitoses 0.6092 0.3226 1.89 0.0590 4.2.4.1 Measuring Improvement in Regression Models The net reclassification improvement (NRI) and integrated discrimination improvement (IDI) have been proposed as more sensitive metrics of improvement in model discrimination.The NRI can be considered as a percentage reclassiﬁcation for the risk categories and the IDI is the mean difference in predicted probabilities between 2 models (constructed from cases with disease and without disease). The NRI and IDI scores are expressed as fractions and can be converted to percentage by multiplying 100.The continuous NRI and IDI can be performed using PredictABEL [Phan et al. (2017)](Phan et al. 2016). 4.2.4.2 Shapley value We can use ideas from game theory relating to fair distribution of proﬁt in coalition games; the coalition (co-operative) game in this case can be interpreted as contribution of the covariates to the model. The Shapley value regression method calculates the marginal contribution of each covariate as the average of all permutations of the coalition of the covariates containing the covariate of interest minus the coalition without the covariate of interest. The advantage of this approach is that it can handle multicollinearity (relatedness) among the covariates. The feature importance is used to assess the impact of the features on the model’s decision #this section takes the output from logistic regression above. library(iml) X = BreastCancer2[which(names(BreastCancer2) != &quot;Class&quot;)] predictor = Predictor$new(DCa, data = X, y = BreastCancer2$Class) imp = FeatureImp$new(predictor, loss = &quot;mae&quot;) plot(imp) From the logistic regression above cell thickness and cromatin have the highest coefficient and lowest p value. This is the same as feature importance. By contrast the Shapley values show that cell shape and marg adhesion make the largest impact on the model when considering the contribution to the model after considering all the contribution by different coalitions. #explain with game theory shapley &lt;- Shapley$new(predictor, x.interest = X[1, ]) shapley$plot() 4.2.4.3 ICE The individual conditional expectation (ICE) curves is the plot of the expectation fof the predictive value for each observation at the unique value for the feature. #feature is the covariate of interest par(mfrow=c(1,2)) eff_thick &lt;- FeatureEffect$new(predictor, feature = &quot;Cl.thickness&quot;, method = &quot;ice&quot;, center.at = 0) plot(eff_thick) 4.2.5 Interaction Interactions is plotted here using lollipop bar. The ggalt library can be used to create this type of plot with geom_lollipop. The strength of interaction is measured using Friedman’s H-statistics. The H-statistics ranges from 0 to 1 with 1 indicating the overall interaction strength. In the case with Breast Cancer data, the interaction strength is low. When describing interaction terms it is recommended that the results be expressed as β coefficients rather than as odds ratio. #plot interactions interact &lt;- Interaction$new(predictor) plot(interact) 4.3 Confounder A confounder is a covariate that serves as a cause of both exposure and outcome and as such confound the analysis. A mediator exist on the causal pathway from exposure to outcome. A common misconception is that the multiple regression adjust for imbalance in covariates in clinical trial. This issue was observed in the pivotal NINDS alteplase trial. The results of the trial has since been accepted with re-analysis of this trial using covariate adjustment (Ingall et al. 2004). There are several methods for covariate adjustment in radomised trials: direct adjustment, standardisation and inverse-probability-of-treatment weighting. 4.3.1 Confounder weighted model The issue asked is whether one should choose to perform confounder analysis or propensity matching. 4.3.2 Propensity matching Propensity matching is an important technique to adjust for imbalance in covariates between 2 arms. There are concerns with mis-use of this technique such as difference in placebo arms from multiple sclerosis trials (Signori et al. 2020). It is proposed that this technique should be used only if all the confounders are measurable. This situation may not be satisfied if the data were accrued at different period, in different continent etc. 4.3.3 E-values The E-values (VanderWeele TJ 2017) has been proposed a measure of unmeasured confounders in observational studies. The E-value is a measure of the extent to which the confounder have on the treatment–outcome association, conditional on the measured covariates. A large E-value is desirable. The E-values is available in Evalue library (Mathur MB 2018). 4.4 Causal inference Causation and association are often miscontrued to be the same. However, the finding of correlation (association) does necessarily imply causation. Causal inference evaluates the response of an effect variable in the setting of change in the cause of the effect variable. There are issues with approach to analysis of causal inference. It can be performed using frequentist such as confounder weighted model or Bayesian methods such as (Baysian additive regression tree). 4.5 Special types of regression 4.5.1 Ordinal regression Ordinal regression is appropriate when the outcome variable is in the form of ordered categorical values. For example, the Rankin scale of disability is bounded between the values of 0 and 6. This type of analysis uses the proportional odds model and the requirement for this model is stringent. When examining results of ordinal regression check that the authors provide this metric, the Brant test. The Brant test assesses the parallel regression assumption. Ordinal regression is performed using polr function in MASS library. The Brant test is available in the Brant library. 4.5.2 Survival analysis Survival analysis is useful when dealing with time to event data. Time to event data can be left, interval and right censoring. Left censoring exists when events may have already occurred at the start of the study eg purchase of phones. Right censoring exists when events have not happened yet eg cancer trial. Interval censoring exists when an insurance has been purchased but the date of product purchase is not yet known. The Cox model assesses the hazard of outcome between two groups. The assumption of this model is that the hazard between each arm is proportional (Stensrud and Hernan 2020). The proportional hazard model can be tested based on the weighted Schoenfeld residuals(Grambsch and Therneau 1994). There are non-parametric models available when the assumption of the proportional hazard model does not hold. In the next chapter on machine learning, an illustration of random survival forest with rfrsc library and ranger library are provided. In the section on [clinical trial][Interpreting clinical trials] we illustrate how the results can be converted to numbers needed to treat. The median survival corresponding to survival probability of 0.50 can be determined here. Metrics for assessing survival model was described above. library(survival) library(survminer) #data from survival package on NCCTG lung cancer trial #https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html data(cancer, package=&quot;survival&quot;) #time in is available in days #status censored=1, dead=2 #sex:Male=1 Female=2 survfit(Surv(time, status) ~ 1, data = cancer) ## Call: survfit(formula = Surv(time, status) ~ 1, data = cancer) ## ## n events median 0.95LCL 0.95UCL ## [1,] 228 165 310 285 363 sfit&lt;- coxph(Surv(time, status) ~ age+sex+ph.ecog+ph.karno+wt.loss, data = cancer) summary(sfit) ## Call: ## coxph(formula = Surv(time, status) ~ age + sex + ph.ecog + ph.karno + ## wt.loss, data = cancer) ## ## n= 213, number of events= 151 ## (15 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.015157 1.015273 0.009763 1.553 0.120538 ## sex -0.631422 0.531835 0.177134 -3.565 0.000364 *** ## ph.ecog 0.740204 2.096364 0.191332 3.869 0.000109 *** ## ph.karno 0.015251 1.015368 0.009797 1.557 0.119553 ## wt.loss -0.009298 0.990745 0.006699 -1.388 0.165168 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.0153 0.9850 0.9960 1.0349 ## sex 0.5318 1.8803 0.3758 0.7526 ## ph.ecog 2.0964 0.4770 1.4408 3.0502 ## ph.karno 1.0154 0.9849 0.9961 1.0351 ## wt.loss 0.9907 1.0093 0.9778 1.0038 ## ## Concordance= 0.64 (se = 0.026 ) ## Likelihood ratio test= 33.53 on 5 df, p=3e-06 ## Wald test = 32.27 on 5 df, p=5e-06 ## Score (logrank) test = 32.83 on 5 df, p=4e-06 Test proportional hazard assumption using weighted residuals (Grambsch and Therneau 1994). The finding below shows that inclusion of covariate ph.karno violate proportional hazard assumption. cox.zph(sfit) ## chisq df p ## age 0.186 1 0.666 ## sex 2.059 1 0.151 ## ph.ecog 1.359 1 0.244 ## ph.karno 4.916 1 0.027 ## wt.loss 0.110 1 0.740 ## GLOBAL 7.174 5 0.208 Plot fit of survival model ggcoxdiagnostics(sfit, type = &quot;deviance&quot;, ox.scale = &quot;linear.predictions&quot;) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## ℹ Please use `gather()` instead. ## ℹ The deprecated feature was likely used in the survminer package. ## Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## `geom_smooth()` using formula = &#39;y ~ x&#39; Forest plot of outcome from survival analysis ggforest(sfit) An alternative way to display the output from Cox regression is to use forestmodel library . forestmodel::forest_model(sfit) The pseudoR2 for Cox regression model proposed by Royston can be evaluated royston(sfit) ## D se(D) R.D R.KO R.N C.GH ## 0.8546894 0.1495706 0.1484960 0.1515108 0.1459168 0.6418894 4.5.3 Quantile regression Least spare regression is appropriate when the data is homoscedascity or the error term remain constant. This can be seen as the data varies around the fitted line. Homoscedascity implies that the data is homogenous. Quantile regression is appropriate when the distribution of the data is non-normal and it is more appropriate to look at the conditional median of the dependent variable. There are several libraries for this task quantreg and Bayesian libraries. In the example below, the life time risk of stroke is regressed against life expectancy using lest square and quantile regression. library(quantreg) library(ggplot2) load(&quot;./Data-Use/world_stroke.Rda&quot;) #quantile regression rqfit &lt;- rq( MeanLifetimeRisk~ LifeExpectancy, data = world_sfdf) rqfit_sum&lt;-summary(rqfit) #least square regression lsfit&lt;-lm(MeanLifetimeRisk~LifeExpectancy,data=world_sfdf) lsfit_sum&lt;-summary(lsfit) #plot ggplot(world_sfdf, aes(x=LifeExpectancy,y=MeanLifetimeRisk))+ #add fitted line for least square geom_abline(intercept =lsfit_sum$coefficients[1], slope=lsfit_sum$coefficients[2],color=&quot;red&quot;)+ #add fitted line for quantile regression geom_point()+xlab(&quot;Life Expectancy&quot;)+ geom_abline(intercept =rqfit_sum$coefficients[1], slope=rqfit_sum$coefficients[2],color=&quot;blue&quot;) #annotate least square and quantile at position x, y #annotate(&quot;text&quot;,x=60, y=27, label=paste0(&quot;least square =&quot;, round(lsfit_sum$coefficients[1],2) ,&quot;+&quot;, round(lsfit_sum$coefficients[2],2),&quot;x &quot;,&quot;Life Expectancy&quot;),color=&quot;red&quot;)+ annotate(&quot;text&quot;,x=75, y=12,label=paste0(&quot;quantile =&quot;,round(rqfit_sum$coefficients[1],2), &quot; + &quot;, round(rqfit_sum$coefficients[2],2),&quot; x &quot;,&quot;Life Expectancy&quot;),color=&quot;blue&quot;) 4.5.4 Non-negative regression In certain situations, it is necessary to constrain the analysis so that the regression coefifcients are non-negative. For example, when regressing brain regions against infarct volume, there is no reason believe that a negative coefficient attributable to a brain region is possible(Phan, Donnan, Koga, et al. 2006) . Non-negative regression can be performed in R using nnls. 4.5.5 Poisson regression Poisson regression is used when dealing with number of event over time or distance such as number of new admissions or new cases of hepatitis or TIA over time. An assumption of the Poisson distribution is that the mean &amp; lambda; and variance λ are the same. A special case of Poisson regression is the negative binomial regression. This latter method is used when the variance is greater than the mean pf the data or over-dispersed data. Negative binomial regression can be applied to number of ‘failure’ event over time. Here ‘failure’ has a lose definition and can be stroke recurrence after TIA or cirrhosis after hepatitis C infection. Zero-inflated data occurs when there is an abundance of zeroes in the data (true and excess zeroes). 4.5.6 Conditional logistic regression Conditional logistical regression model should be used when the aim is to compare pair of objects from the same patient. Examples include left and right arms or left and right carotid arteries. This method is available from clogit in survival. 4.5.7 Multinomial modelling Multinomial modelling is used when the outcome categorical variables are not ordered. This situation can occur when analysis involves choice outcome (choices of fruit: apple, orange or pear). In this case, the log odds of each of the categorical outcomes are analysed as a linear combination of the predictor variables. The nnet library have functions for performing this analysis. 4.6 Sample size estimation Clinicians are often frustrated about sample size and power estimation for a study, grant or clinical trial. This aspect is scrutinised by ethics committee and in peer review process for journals. Luckily, R provides several packages for sample size amd power estimation: pwr library. Cohen has written reference textbook on this subject (Cohen 1977). 4.6.1 Proportion library(pwr) #ttest-d is effect size #d = )mean group1 -mean group2)/variance pwr.t.test(n=300,d=0.2,sig.level=.05,alternative=&quot;greater&quot;) ## ## Two-sample t test power calculation ## ## n = 300 ## d = 0.2 ## sig.level = 0.05 ## power = 0.7886842 ## alternative = greater ## ## NOTE: n is number in *each* group We provided an example below for generating power of clinical trial. Examples are taken from a paper on sample size estimation for phase II trials (Phan, Donnan, Davis, et al. 2006). library(pwr) #h is effect size. effect size of 0.5 is very large #sample size pwr.2p.test(h=0.5,n=50,sig.level=0.05,alternative=&quot;two.sided&quot;) ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.5 ## n = 50 ## sig.level = 0.05 ## power = 0.705418 ## alternative = two.sided ## ## NOTE: same sample sizes #medium effect size pwr.2p.test(h=0.1,n=50,sig.level=0.05,alternative=&quot;two.sided&quot;) ## ## Difference of proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.1 ## n = 50 ## sig.level = 0.05 ## power = 0.07909753 ## alternative = two.sided ## ## NOTE: same sample sizes The output of the sample size calculation can be put into a table or plot. library(pwr) #pwr.2p.test(h=0.3,n=80,sig.level=0.05,alternative=&quot;two.sided&quot;) h &lt;- seq(.1,.5,.1) #from 0.1 to 0.3 by 0.05 nh &lt;- length(h) #5 p &lt;- seq(.3,.9,.1)# power from 0.5 to 0.9 by 0.1 np &lt;- length(p) #9 # create an empty array 9 x 5 samplesize &lt;- array(numeric(nh*np), dim=c(nh,np)) for (i in 1:np){ for (j in 1:nh){ result &lt;- pwr.2p.test(n = NULL, h = h[j], #result &lt;- pwr.r.test(n = NULL, h = h[j], sig.level = .05, power = p[i], alternative = &quot;two.sided&quot;) samplesize[j,i] &lt;- ceiling(result$n) } } samplesize ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 412 583 769 980 1235 1570 2102 ## [2,] 103 146 193 245 309 393 526 ## [3,] 46 65 86 109 138 175 234 ## [4,] 26 37 49 62 78 99 132 ## [5,] 17 24 31 40 50 63 85 #graph xrange &lt;- range(h) yrange &lt;- round(range(samplesize)) colors &lt;- rainbow(length(p)) plot(xrange, yrange, type=&quot;n&quot;, xlab=&quot;Effect size (h)&quot;, ylab=&quot;Sample Size (n)&quot; ) # add power curves for (i in 1:np){ lines(h, samplesize[,i], type=&quot;l&quot;, lwd=2, col=colors[i]) } # add annotation (grid lines, title, legend) abline(v=0, h=seq(0,yrange[2],50), lty=2, col=&quot;grey89&quot;) abline(h=0, v=seq(xrange[1],xrange[2],.02), lty=2, col=&quot;grey89&quot;) title(&quot;Sample Size Estimation\\n Difference in Proportion&quot;) legend(&quot;topright&quot;, title=&quot;Power&quot;, as.character(p), fill=colors) 4.6.1.1 Non-inferiority Non-inferiority trials may offer information in a way that a traditional superiority design do not. The design may be interested in other aspect of the treatment such as cost and lower toxicity (Kaji and Lewis 2015). Examples of non-inferiority trial designs include antibiotics versus surgery for appendicitis (Salminen et al. 2015). There are concerns with reporting of noninferiority trial. Justification for the margin provided in 27.6% (Gopal et al. 2015). The following describes a trial design where it’s expected that drug will result in a certain outcome p1 and the control arm p2 and the ratio of subject in treatment to control arm is k. The difference in outcome is delta. The margin is defined as non-inferior if &lt;0. library(TrialSize) TwoSampleProportion.NIS(alpha=.05, beta=.8, p1=.6, p2=.7, k=1, delta = .1, margin=-.2 ) ## [1] 3.225911 4.6.2 Logistic regression library(powerMediation) library(ggplot2) #continuous predictor #p1=event rate #exp(0.405) =1.5 powerLogisticCon(n=200, p1=0.265, OR=exp(0.014), alpha=0.05) ## [1] 0.03056289 # creating a data frame using data from a=seq(0,05.4,0.05) df_power&lt;-data.frame(`ASPECTS`= a, &quot;Power&quot;=powerLogisticCon(n=100, p1=a, OR=exp(.695), alpha=0.05) ) ## Warning in sqrt(n * beta.star^2 * p1 * (1 - p1)): NaNs produced ggplot(data=df_power, aes(x=ASPECTS, y=Power))+geom_point() ## Warning: Removed 88 rows containing missing values (`geom_point()`). An alternative library to perform sample size for logistic regression is WebPower library. library(WebPower) ## Loading required package: lme4 ## ## Attaching package: &#39;lme4&#39; ## The following object is masked from &#39;package:raster&#39;: ## ## getData ## The following object is masked from &#39;package:nlme&#39;: ## ## lmList ## The following object is masked from &#39;package:modeltools&#39;: ## ## refit ## The following object is masked from &#39;package:generics&#39;: ## ## refit ## The following object is masked from &#39;package:MatrixModels&#39;: ## ## mkRespMod ## Loading required package: lavaan ## This is lavaan 0.6-15 ## lavaan is FREE software! Please report any bugs. ## ## Attaching package: &#39;lavaan&#39; ## The following object is masked from &#39;package:TTR&#39;: ## ## growth ## The following object is masked from &#39;package:tm&#39;: ## ## inspect ## Loading required package: PearsonDS wp.logistic(p0=0.007, #Prob (Y=1|X=0) p1=0.012, #Prob (Y=1|X=1) alpha=0.05, power=0.80, alternative=&quot;two.sided&quot;, family=&quot;normal&quot;) ## Power for logistic regression ## ## p0 p1 beta0 beta1 n alpha power ## 0.007 0.012 -4.954821 0.5440445 3336.2 0.05 0.8 ## ## URL: http://psychstat.org/logistic 4.6.3 Survival studies Sample size for survival studies can be performed using powerSurvEpi or gsDesign. library(powerSurvEpi) #sample size ssizeEpi.default(power = 0.80, theta = 2, p = 0.408 , psi = 0.15, rho2 = 0.344^2, alpha = 0.05) ## [1] 512 #power powerEpi.default(n = 2691, theta = 2, p = 0.408, psi = 0.250, rho2 = 0.344^2, alpha = 0.05) ## [1] 1 #Amarenco NEJM 2020 #equal sample size k=1 ssizeCT.default(power = 0.8, k = .8, pE = 0.085, pC = 0.109, RR = 0.78, alpha = 0.05) ## nE nC ## 2417 3021 4.6.4 Multiple regression The power for general linear model can be calculated using the pwr.f2.test function. library(pwr) #u=degrees of freedom for numerator #v=degrees of freedomfor denominator #f2=effect size 4.7 Randomised clinical trials A common misconception is that the multiple regression adjust for imbalance in covariates in clinical trial. This issue was observed in the pivotal NINDS alteplase trial. The results of the trial has since been accepted with re-analysis of this trial using covariate adjustment(Ingall et al. 2004). There are several methods for covariate adjustment in radomised trials: direct adjustment, standardisation and inverse-probability-of-treatment weighting. 4.7.1 Covariate adjustment in trials Specifically, covariate adjustment refers to adjustment of covariates available at the time of randomisation, i.e. prespecified variables and not variables after randomisation such as pneumonia post stroke trials. The advantage of covariate adjustment is that it results in narrower confidence interval as well as increase the power of the trial up to 7% (Kahan 2014). The increased power is highest when prognostic variables are used but can decrease power when non-prognostic variables are used (Kahan 2014). 4.7.2 Subgroup analysis Subgroup analysis can be misleading especially if not specified prior to trial analysis (Wang et al. 2007). Furthermore, increasing the number of subgroup analysis will lead to increasing the chance of false positive result or multiplicity. It is important to differentiate between prespecified and posthoc analyses as posthoc analyses may be biased by examination of the data. 4.7.3 p value for interaction The p value for interaction describe the influence by a baseline variable treatment effect on outcome in clinical trial (Wang et al. 2007). In a hypothetical trial, a significant p value for interaction between males and females for treatment effect on primary outcome indicates heterogenity of treatment effect. library(Publish) ## Warning: package &#39;Publish&#39; was built under R version 4.3.2 ## Registered S3 method overwritten by &#39;Publish&#39;: ## method from ## print.ci coin library(survival) 4.7.4 Interpreting risk reduction in clinical trials A key issue in interpreting of clinical trials occurs when the relative risk reduction or relative hazard risk are provided. This issue affect the clinical interpretation of the trial finding and its application in practice. An example is the result of the ACAS asymptomatic carotid artery trial is often quoted as showing 50% risk reduction. In fact, there was 2% annual risk of ipsilateral stroke in the medical and 1% risk in the surgical arm. The absolute risk reduction or ARR was 1% per year. However, the 50% relative risk reduction is often quoted to explain to patients. 4.7.5 NNT from ARR In this case, the number needed to treat (NNT) is given by \\(\\frac{1}{ARR}\\) or \\(\\frac{1}{0.01}=100\\) to achieve the trial outcome or 100 patients are needed to be treated to reduce the stroke risk to 1%. The recommendation is that the 95% confidence interval for NNT should be provided. 4.7.6 NNT from odds ratio Calculation of NNT for odds ratio requires knowledge of the outcome of the placebo group. The NNT is given by \\(\\frac{1}{ACR-\\frac{OR*ACR}{1-(ACR+OR*ACR)}}\\). The ACR represents the assumed control risk. The NNT can be calculated from nnt function in meta library. library(meta) ## Warning: package &#39;meta&#39; was built under R version 4.3.2 ## Loading &#39;meta&#39; package (version 7.0-0). ## Type &#39;help(meta)&#39; for a brief overview. ## Readers of &#39;Meta-Analysis with R (Use R!)&#39; should install ## older version of &#39;meta&#39; package: https://tinyurl.com/dt4y5drs ## ## Attaching package: &#39;meta&#39; ## The following object is masked from &#39;package:survMisc&#39;: ## ## ci ## The following object is masked from &#39;package:mixmeta&#39;: ## ## blup ## The following object is masked from &#39;package:pROC&#39;: ## ## ci ## The following object is masked from &#39;package:sp&#39;: ## ## bubble ## The following object is masked from &#39;package:mada&#39;: ## ## forest ## The following object is masked from &#39;package:mvmeta&#39;: ## ## blup #p.c = baseline risk nnt(0.73, p.c = 0.3, sm = &quot;OR&quot;) ## OR p.c NNT ## 1 0.73 0.3 16.20811 4.7.7 NNT from risk ratio #data from EXTEND-IV trial in NEJM 2019 #outcome 35.4% in tpa and 29.5% in control nnt(1.44, p.c = 0.295, sm = &quot;RR&quot;) ## RR p.c NNT ## 1 1.44 0.295 -7.70416 4.7.8 NNT from hazard ratio Calculation of NNT for hazard ratio requires knowledge of the outcome of the placebo group and the hazard ratio or HR. The formula using the binomial theorem is \\(p=1-q\\) where q is given by the ratio of outcome and numbers recruited in the placebo group. The formula is taken from (Ludwig, Darmon, and Guerci 2020) . The NNT is given \\(\\frac{1}{[p^{HR}-p}\\) . We illustrated this using data from metaanalysis on aspirin use in stroke in Lancet 2016. There were 45 events among 16053 patients in the control group. The HR was 0.44. The NNT from this formula \\(\\frac{1}{.9971968^.44-.9971968}\\) was 637. 4.7.9 NNT from metaananlysis There are concerns with using NNT from the results of metaanalysis as the findings are amalgations of trials with different settings (Marx 2003) (Smeeth 1999). The caution applies when baseline risks or absolute risk differences vary across trials. 4.8 Diagnostic test 4.8.1 Sensitivity, specificity The sensitivity of a diagnostic test is the true positive rate and the specificity is the true negative rate. Example of 2 x 2 table is provided here. As an exercise, consider a paper about a diagnostic test for peripheral vertigo reporting 100% sensitivity and 94.4% specificity. There are 114 patients, 72 patients without stroke have vertigo and positive test findings. Among patients with stroke 7 of 42 have positive test findings. The sensitivity is \\(TP=\\frac{TP}{TP+FN}\\) and the specificity is the \\(TN=\\frac{TN}{TN+FP}\\). # Peripheral Vertigo # Disease Positive Disease Negative #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ # HIT Test# # $ # Positive# True Positive # False Positive $ # # 72 # 7 $ #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ # HIT Test# # $ # Negative# False Negative # True Negative $ # # 0 # 35 $ #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ # Peripheral Vertigo #Sensitiviy=TP/(TP+FN)=100% #Specificity=TN/(TN+FP)=83% 4.8.2 AUC The area under the receiver operating characteristics (ROC) curve or AUC is a measure of the accuracy of the test. It is recommended that ROC curve is used when there are multiple threshold. It should not be used when the test has only one threshold. Some investigators suggest caution regarding the validity of using receiver operating curve with single threshold diagnostic tests (J 2020). An AUC of 0.5 is classified as no better than by chance; 0.6–0.69 provides poor discrimination; 0.7–0.79 provides acceptable (fair) discrimination, 0.8 to 0.89 provides good (excellent) discrimination, and 0.9 to 1.0 provides outstanding discrimination. 4.8.3 Likelihood ratio Positive likelihood ratio (PLR) is the ratio of sensitivity to false positive rate (FPR); the negative (NLR) likelihood ratio is the ratio of 1-sensitivity to specificity. A PLR indicates the likelihood that a positive spot sign (test) would be expected in a patient with ICH (target disorder) compared with the likelihood that the same result would be expected in a patient without ICH. Using the recommendation by Jaeschke et al, a high PLR (&gt;5) and low NLR (&lt;0.2) indicate that the test results would make moderate changes in the likelihood of hematoma growth from baseline risk. PLRs of &gt;10 and NLRs of &lt;0.1 would confer very large changes from baseline risk . 4.8.3.1 Fagan’s normogram Fagan’s normogram can be conceptualised as a sliding ruler to match the disease prevalence and likelihood ratios to evaluate the impact on the post-test probability (TJ 1975). At the current disease prevalence of 23.4% and PLR 4.65, the post-test probability remains low at 0.60. library(tidyverse) source(&quot;https://raw.githubusercontent.com/achekroud/nomogrammer/master/nomogrammer.r&quot;) p&lt;-nomogrammer(Prevalence = .234, Plr = 4.85, Nlr = 0.49) p+ggtitle(&quot;Fagan&#39;s normogram for Spot Sign and ICH growth&quot;) #to save the file #ggsave(p,file=&quot;Fagan_SpotSign.png&quot;,width=5.99,height=3.99,units=&quot;in&quot;) 4.8.3.2 Likelihood ratio graph Likelihood ratio graph is a tool for comparing diagnostic tests (BJ 2000). #plot likelihood ratio graph LR_graph&lt;-function (Read,sheet,Sensitivity, Specificity){ Read1&lt;-readxl::read_xlsx(Read, sheet = sheet) #binary data #A=True pos %B=False positive %C=False negative %D=True negative A=Read1$TP B=Read1$FP C=Read1$FN D=Read1$TN TPR=A/(A+C) FPR=1-(D/(D+B)) TPR_DiagnosticTest=Sensitivity FPR_DiagnosticTest=1-Specificity # set plot X=seq(0,1,by=.1) Y=seq(0,1,by=.1) plot(X,Y,main=&quot;Likelihood Ratio graph&quot;, xlab=&quot;1-Specificity&quot;,ylab=&quot;Sensitivity&quot;,cex=.25) #pch describe the shape. The value 1 corresponds o points(FPR_DiagnosticTest,TPR_DiagnosticTest,pch=8,col=&quot;blue&quot;,cex=2) #pch describe the shape. The value 8 corresponds * points(FPR,TPR,pch=1,col=&quot;red&quot;,cex=2) #add point #abline(coef = c(0,1)) #add diagonal line df1&lt;-data.frame(c1=c(0,TPR_DiagnosticTest),c2=c(0,FPR_DiagnosticTest)) reg1&lt;-lm(c1~c2,data=df1) df2&lt;-data.frame(c1=c(TPR_DiagnosticTest,1),c2=c(FPR_DiagnosticTest,1)) reg2&lt;-lm(c1~c2,data=df2) abline(reg1) abline(reg2) text(x=FPR_DiagnosticTest,y=TPR_DiagnosticTest+.3,label=&quot;Superior&quot;,cex=.7) text(x=FPR_DiagnosticTest+.2,y=TPR_DiagnosticTest+.2,label=&quot;Absence&quot;,cex=.7) text(x=.0125,y=TPR_DiagnosticTest-.1,label=&quot;Presence&quot;,cex=.7) text(x=FPR_DiagnosticTest+.1,y=TPR_DiagnosticTest,label=&quot;Inferior&quot;,cex=.7) text(x=.7,y=.2,label=&quot;Reference = Content Expert&quot;,cex=.7) text(x=.7,y=.15, label=&quot;Diagnostic Test software&quot;, cex=.7) } Runnning the function from above #Sensitivity=0.623 #Specificity=1-.927 LR_graph(&quot;./Data-Use/Diagnostic_test_summary.xlsx&quot;,1,.623,.927) ## New names: ## • `` -&gt; `...11` 4.9 Metaanalysis During journal club, junior doctors are often taught about the importance of metaanalysis. It is worth knowing how to perform a metaanalysis in order to critique the study. This is an important issue as the junior doctor is supervised by someone who a content expert but not necessarily a method expert. Metaanalysis can be performed for clinical trials, cohort studies or diagnostic studies. As an example, it is not well known outside of statistics journal that the bivariate analysis is the preferred method to evaluate diagnostic studies (Reitsma et al. 2005). By contrast, the majority of metaanalysis of diagnostic studies uses the univariate method of Moses and Littenberg (Moses, Shapiro, and Littenberg 1993). This issue will be expanded below. 4.9.1 Quality of study All studies require evaluation of the quality of the individual studies. This can be done with the QUADAS2 tool, available at https://annals.org/aim/fullarticle/474994/quadas-2-revised-tool-quality-assessment-diagnostic-accuracy-studies. 4.9.2 PRISMA The PRISMA statement is useful for understanding the search strategy and the papers removed and retained in the metaanalysis. An example of generating the statement is provided below in R. The example given here is from a paper on the use of spot sign to predict enlargment of intracerebral hemorrhage (Phan, Krishnadas, et al. 2019). library(PRISMAstatement) #example from Spot sign paper. Stroke 2019 prisma(found = 193, found_other = 27, no_dupes = 141, screened = 141, screen_exclusions = 3, full_text = 138, full_text_exclusions = 112, qualitative = 26, quantitative = 26, width = 800, height = 800) #https://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html#attributes library(DiagrammeR) grViz(&quot; digraph boxes_and_circles { # a &#39;graph&#39; statement graph [overlap = true, fontsize = 10] # several &#39;node&#39; statements node [shape = box, fontname = Helvetica] Stroke node [shape = oval, fixedsize = false, color=red, width = 0.9] Hypertension; &#39;No Hypertension&#39; node [shape= circle, fontcolor=red, color=blue, fixedsize=false] Hypokalemia; &#39;No Hypokalemia&#39; # several &#39;edge&#39; statements edge [arrowhead=diamond] Stroke-&gt;{Hypertension, &#39;No Hypertension&#39;} Hypertension-&gt;{Hypokalemia, &#39;No Hypokalemia&#39;} } &quot;) ## alternative grViz(&quot;digraph flowchart { # node definitions with substituted label text node [fontname = Helvetica, shape = rectangle] tab1 [label = &#39;@@1&#39;] tab2 [label = &#39;@@2&#39;] tab3 [label = &#39;@@3&#39;] tab4 [label = &#39;@@4&#39;] tab5 [label = &#39;@@5&#39;] # edge definitions with the node IDs tab1 -&gt; tab3 tab1-&gt; tab2 #tab2-&gt;tab3 tab2 -&gt; tab4 tab2-&gt; tab5; } [1]: &#39;Stroke n=19&#39; [2]: &#39;Hypertension n=10&#39; [3]: &#39;No Hypertension n=9&#39; [4]: &#39;Hypokalemia n=?&#39; [5]: &#39;No Hypokalemia n=?&#39; &quot;) 4.9.3 Conversion of median to mean One issue with performing metaanalysis is that one paper may report mean and another report median age. The formula for the mean is given by \\(\\frac{a+2m+b}{4}\\) where m is the median, a is the upper and b is the lower range (Wan 2014). The variance is given by \\(S^2=\\frac{1}{12}(\\frac{(a+2m+b)^2}{4}+{(b-a)^2})\\). This formula requires examination of the data such as the figure to obtain the upper and lower range. These changes are incorporated into meta libray using meatamean function (Balduzzi, Rücker, and Schwarzer 2019). More recently, investigators suggest to also consider the skewness of the data from the 5 number summary data (Shi et al. 2023). The argument method.mean in function metamean is used to specify the method for estimating the mean. In this case we chose the Luo method for illustration Luo (Luo et al. 2018). See the help page by typing question mark before metamean for more options. library(meta) #NIHSS data from ANGEL large core trial in NEJM 2023 metamean(q1=4,q3=20,median=16,n=230, method.mean = &quot;Luo&quot;) ## Number of observations: o = 230 ## ## mean 95%-CI ## 13.1932 [11.6506; 14.7358] ## ## Details: ## - Untransformed (raw) means Here the same data is used for the McGrath method (McGrath et al. 2020). metamean(q1=4,q3=20,median=16,n=230, method.mean = &quot;QE-McGrath&quot;) ## Number of observations: o = 230 ## ## mean 95%-CI ## 13.3333 [11.7907; 14.8759] ## ## Details: ## - Untransformed (raw) means The conversion from median to mean in metafor is performed using conv.fivenum function (Viechtbauer 2010). There is an update on the metafor page as well as discussion on alternate approach. The default method of this function is to use the methods by Luo (Luo et al. 2018), Wan (Wan 2014) and Shi (Shi et al. 2023) library(metafor) # example data frame EstMean &lt;- data.frame(Paper=c(1:4,NA), min=c(1,2,NA,2,NA), q1=c(NA,NA,4,4,NA), median=c(5,6,6,6,NA), q3=c(NA,NA,10,10,NA), max=c(12,14,NA,14,NA), mean=c(NA,NA,NA,NA,7.0), sd=c(NA,NA,NA,NA,4.2), n=c(30,30,30,30,30)) EstMean ## Paper min q1 median q3 max mean sd n ## 1 1 1 NA 5 NA 12 NA NA 30 ## 2 2 2 NA 6 NA 14 NA NA 30 ## 3 3 NA 4 6 10 NA NA NA 30 ## 4 4 2 4 6 10 14 NA NA 30 ## 5 NA NA NA NA NA NA 7 4.2 30 EstMean &lt;- conv.fivenum(min=min, q1=q1, median=median, q3=q3, max=max, n=n, data=EstMean) EstMean ## Paper min q1 median q3 max mean sd n ## 1 1 1 NA 5 NA 12 5.356748 2.695707 30 ## 2 2 2 NA 6 NA 14 6.475664 2.940771 30 ## 3 3 NA 4 6 10 NA 6.713000 4.670521 30 ## 4 4 2 4 6 10 14 6.882074 3.546379 30 ## 5 NA NA NA NA NA NA 7.000000 4.200000 30 The estmeansd library uses quantile estimation method with qe.mean.sd function when the data available are the median and quartile ranges (McGrath et al. 2020) (McGrath et al. 2022).The approach here is to use simulation to estimate the parameters. library(estmeansd) ## Warning: package &#39;estmeansd&#39; was built under R version 4.3.2 #data from ANGEL large core trial in NEJM 2023 res_qe &lt;- bc.mean.sd(q1.val = 13, med.val = 16, q3.val = 20, n = 230) res_qe ## $est.mean ## [1] 17.07168 ## ## $est.sd ## [1] 5.672963 The standard error from the mean can be estimated using get_SE function. get_SE(res_qe) ## $est.se ## [1] 0.5051669 ## ## $boot_means ## [1] 17.32080 16.63567 16.93157 16.22506 16.81751 16.74528 17.43474 16.11106 ## [9] 17.47347 16.43032 16.44389 16.11995 16.72996 16.21527 16.97045 17.36650 ## [17] 16.93584 17.01512 17.64598 16.15726 16.55998 17.00313 15.80268 16.58914 ## [25] 17.48917 17.24221 16.77229 16.07140 16.04065 17.08900 17.51776 16.99848 ## [33] 16.80582 16.42138 16.48949 16.20926 17.01369 17.26644 16.39943 16.21991 ## [41] 16.27875 17.25187 17.55987 15.58216 16.54443 15.61870 16.64487 17.00805 ## [49] 16.42508 17.20625 15.77155 16.97637 16.83431 17.80827 16.82612 16.65967 ## [57] 16.85211 15.74729 15.96911 16.37632 16.65517 16.96014 17.14616 16.41785 ## [65] 16.40626 16.20178 16.27367 16.75516 16.54564 17.38497 16.62009 16.81253 ## [73] 17.13541 16.73494 17.33151 16.45154 16.51921 16.50343 16.12960 16.60396 ## [81] 16.18093 16.43201 17.14915 16.83220 16.64586 16.07443 15.78909 17.07393 ## [89] 16.23504 16.97492 16.66537 16.91315 16.84260 16.30903 17.16021 16.41322 ## [97] 16.89036 16.41005 16.69129 17.24132 17.02483 16.54984 17.27668 16.19821 ## [105] 16.59967 17.40905 16.50958 15.87896 16.84670 16.40646 16.57890 16.13591 ## [113] 17.66015 16.86417 16.33229 16.45668 17.54818 15.84977 15.97001 16.84287 ## [121] 16.47263 16.69285 16.80196 16.68237 14.97224 16.13378 15.80667 17.04423 ## [129] 17.16538 16.05389 16.17857 16.49585 16.86023 15.87037 17.05982 16.69008 ## [137] 17.44114 17.10851 16.60494 16.68109 17.88056 15.98815 17.05229 17.53937 ## [145] 17.08673 17.31375 16.74816 17.06345 16.27700 16.38915 17.42180 16.34431 ## [153] 16.35144 17.30443 16.86197 16.19756 16.26297 15.70390 16.48744 17.52096 ## [161] 15.88988 16.22413 17.52307 16.90889 16.99480 17.75782 17.33927 17.24755 ## [169] 16.30885 15.78247 16.39264 16.43002 17.44886 17.31091 17.25882 16.82647 ## [177] 17.05872 17.07597 16.45283 16.64616 16.78093 16.29680 16.69348 17.34924 ## [185] 16.05006 17.04674 16.91597 17.37912 17.10926 16.28712 16.19510 16.40520 ## [193] 16.50581 16.32661 17.49536 15.78012 16.83333 16.60773 17.38507 16.55612 ## [201] 17.32284 15.81106 16.90375 17.31391 15.94982 16.95401 16.80364 15.81292 ## [209] 16.53285 16.33320 17.01881 17.42728 16.25028 16.46858 16.49107 16.67096 ## [217] 17.58910 16.80771 17.28705 17.33232 17.45333 15.70452 16.63157 16.78871 ## [225] 17.05351 16.73961 16.90675 16.43698 16.90910 16.69251 16.92140 17.77825 ## [233] 16.39586 17.25467 16.61300 16.28133 16.19347 16.95948 16.04262 16.40943 ## [241] 16.85606 17.73207 16.05969 16.56175 16.95998 16.77456 16.53733 17.05043 ## [249] 16.92095 17.63673 16.12897 17.27621 16.98738 16.77801 17.10649 16.57076 ## [257] 17.72980 16.47077 17.56929 16.05106 16.52822 16.17694 16.74069 16.53214 ## [265] 16.40624 16.22842 17.04366 16.92633 16.18737 16.46046 17.60398 17.25585 ## [273] 16.63055 16.15188 16.88281 16.71369 16.24270 17.18643 17.67773 15.87091 ## [281] 16.42196 15.88506 17.03498 16.75611 16.88050 16.36898 16.21923 15.86119 ## [289] 16.95522 16.38514 16.96344 16.78450 16.33910 16.55447 16.05376 16.23442 ## [297] 16.88179 16.89877 16.10907 16.41496 17.60726 16.50519 16.95768 16.27211 ## [305] 16.84034 16.65331 16.72439 16.42965 16.53121 17.40950 16.84293 15.97706 ## [313] 17.29369 16.79389 16.91178 16.80644 16.79246 16.56639 16.93687 15.93549 ## [321] 16.97803 16.53235 17.39980 17.11808 16.96614 16.56736 17.22072 16.13415 ## [329] 17.23012 17.33037 16.17044 16.84768 16.36681 17.43284 16.54669 17.52670 ## [337] 16.66046 17.02428 16.27942 17.66086 16.31591 17.29889 16.69726 17.42116 ## [345] 16.93894 16.42335 16.46556 16.02648 17.32738 16.53211 16.64578 17.13451 ## [353] 16.76469 17.41858 15.90484 16.70750 17.75160 16.94354 15.32914 17.15816 ## [361] 16.63345 16.52527 16.94031 16.54089 16.40911 16.72562 16.40795 16.08741 ## [369] 16.70375 16.99509 16.96411 17.33042 16.40155 16.43432 16.46230 17.06799 ## [377] 16.54744 16.63031 15.87108 16.59283 17.37594 17.03591 16.71549 16.40636 ## [385] 16.61947 17.12760 16.36781 17.41405 17.02543 16.59164 17.16652 16.70825 ## [393] 16.42348 16.33087 15.97193 16.57085 16.47704 17.06115 16.46346 16.37825 ## [401] 16.66852 16.90237 17.43670 17.02540 16.77567 17.12490 15.99466 17.57754 ## [409] 16.86391 16.09653 16.58349 16.88690 16.86661 16.78141 16.36360 17.76195 ## [417] 17.03920 17.26835 16.94910 16.84278 15.95507 17.29961 18.17420 17.06272 ## [425] 16.89956 17.01672 15.86038 17.66528 16.77691 16.48180 16.96160 16.20121 ## [433] 17.27824 18.36727 16.99313 17.28873 16.32290 16.62773 16.49563 17.46167 ## [441] 16.86582 16.69366 16.46467 16.35606 16.32886 15.93923 16.74221 16.89277 ## [449] 16.17542 15.53911 16.66733 17.46548 17.62017 17.04305 16.52943 16.42345 ## [457] 16.82823 17.05646 16.30915 16.90865 17.64410 15.89357 15.64827 17.27918 ## [465] 16.25759 16.84754 16.59172 16.52818 16.79152 17.48837 16.47858 16.35069 ## [473] 16.30569 16.35047 17.73610 16.67548 16.59373 16.54415 16.24393 17.73981 ## [481] 17.09083 16.82437 16.24566 16.31238 17.00293 16.77603 16.87141 16.65573 ## [489] 17.21202 16.89045 16.60988 16.74007 16.40177 16.94051 16.70215 16.03732 ## [497] 16.78091 16.56094 16.96411 16.67807 16.46205 16.95894 16.25541 17.30972 ## [505] 16.73450 16.89718 17.68449 16.80379 16.78249 16.70885 17.23930 15.60452 ## [513] 16.53229 16.94299 16.55256 17.53529 16.71421 16.72136 16.98154 16.76428 ## [521] 16.77496 17.21101 16.35697 17.00819 17.18196 16.52609 16.76834 16.21473 ## [529] 16.87524 16.50914 16.84794 17.20918 17.08759 16.91032 16.61788 17.20873 ## [537] 17.24702 16.26339 16.08962 16.89839 16.85254 17.27665 17.02339 16.80542 ## [545] 16.31260 16.95343 16.92637 15.76584 16.32428 15.52002 16.72758 16.10489 ## [553] 16.45044 17.15246 16.31862 17.54586 15.71766 16.03180 15.91454 16.50847 ## [561] 17.58034 16.44514 16.39053 16.44523 17.74103 16.60574 16.07944 17.24384 ## [569] 16.95394 17.25527 16.85086 16.76437 17.05588 16.89740 16.76604 17.47259 ## [577] 16.19189 16.57155 16.69201 15.58509 17.71348 16.89278 17.13621 16.98097 ## [585] 16.37209 16.63458 16.45756 15.82214 17.21805 15.87205 17.06057 16.51364 ## [593] 16.27850 16.61759 17.07980 17.12253 16.68874 16.46898 15.74514 16.47257 ## [601] 16.31795 17.16294 15.54235 17.07123 17.64895 17.51728 16.96985 17.29238 ## [609] 17.22470 16.64837 16.81209 17.09945 15.79052 16.14030 17.16385 16.97909 ## [617] 16.83372 16.35119 17.26060 17.22305 16.94219 17.00113 16.00311 15.83068 ## [625] 16.35277 16.42369 16.16125 17.56700 17.72774 16.89224 15.78370 17.10190 ## [633] 17.03751 17.38876 16.37537 16.28928 16.35935 17.80802 17.59292 16.55081 ## [641] 16.71701 16.60376 17.37924 16.87200 17.06629 16.81133 16.48532 16.70384 ## [649] 16.37222 16.59232 16.68356 16.87583 15.68926 16.92536 16.68019 16.49383 ## [657] 16.67838 16.81643 16.75565 16.84990 17.12278 16.46088 16.60467 15.56857 ## [665] 15.96341 16.51045 17.49161 16.84776 16.83310 16.45928 17.23271 16.55874 ## [673] 16.83858 16.74439 16.42704 16.62623 16.46970 16.65565 16.64940 16.72787 ## [681] 16.05734 16.95689 17.40792 16.82100 17.95568 16.68546 16.30957 17.38197 ## [689] 16.48733 16.38327 17.57027 16.69437 16.62862 16.45147 16.72926 16.47564 ## [697] 16.57953 16.79304 17.06867 16.77500 16.78006 16.11964 15.66531 15.82804 ## [705] 16.07783 16.04600 17.29471 16.03482 16.52480 16.76726 16.27465 16.30180 ## [713] 17.42910 16.25024 16.85514 16.96097 15.90684 17.45888 17.82760 16.72508 ## [721] 17.66872 17.29185 15.86558 17.43733 17.16304 16.01827 17.68125 17.49691 ## [729] 17.40581 15.98998 16.98909 16.62700 16.80940 15.86383 17.48405 17.03081 ## [737] 16.46182 16.06585 16.28275 16.35536 16.64192 17.25586 17.14385 15.28296 ## [745] 16.88277 17.04900 16.51647 16.80045 17.03180 15.58833 15.83423 16.27429 ## [753] 17.09833 16.14325 17.22260 16.63649 16.50760 16.60789 16.70458 16.50868 ## [761] 17.30587 17.05021 16.21258 16.39001 16.19909 16.50069 17.19249 16.54396 ## [769] 17.56024 17.01114 16.89944 16.12434 16.58944 16.52147 17.00033 16.43708 ## [777] 16.02595 16.07578 16.55887 16.52394 15.85640 17.03190 17.34103 17.41698 ## [785] 16.27320 15.91875 16.48158 17.46042 17.51108 16.35286 16.77062 16.48599 ## [793] 16.72312 17.31181 16.07275 15.72165 17.16926 17.32051 16.89491 16.62822 ## [801] 17.01115 16.37084 16.12324 17.83333 16.05555 15.38466 17.21237 17.30975 ## [809] 16.71356 16.73145 17.56062 16.65791 16.12143 16.79715 16.50287 16.87419 ## [817] 16.96025 16.68269 17.04287 16.75458 16.48178 16.79741 17.45758 16.65515 ## [825] 17.17917 17.40719 16.41046 16.91729 15.70365 17.31044 17.25410 16.78650 ## [833] 16.38805 16.69747 16.43540 17.05249 16.76991 16.90452 16.63552 16.93199 ## [841] 17.15012 16.14041 17.25313 17.71210 17.57161 16.65153 16.24913 16.35281 ## [849] 16.78632 16.40571 16.26677 17.22347 17.27656 17.00094 17.37401 16.28819 ## [857] 15.80487 17.06161 16.91467 17.23262 16.40746 16.82565 16.03394 17.11749 ## [865] 17.21168 16.75113 17.27095 17.23839 17.27505 17.22773 16.84900 16.23501 ## [873] 17.41735 15.79513 17.24155 17.66714 16.61896 16.47522 17.48019 17.25277 ## [881] 16.75882 16.34438 17.32721 16.76092 16.25335 17.10654 17.85040 17.38365 ## [889] 16.74501 17.19499 15.65981 17.10181 16.70042 16.83165 17.24912 16.89086 ## [897] 16.51408 16.57233 16.23706 16.39943 16.82704 17.46186 17.33337 16.81203 ## [905] 16.57303 16.18194 17.01169 16.50243 16.81371 16.49241 17.05909 17.12803 ## [913] 16.48085 17.71828 16.41684 16.61332 17.81266 17.55997 16.10346 16.80757 ## [921] 16.72565 16.85029 17.57266 16.69815 17.77317 16.88347 17.31666 17.38245 ## [929] 16.71139 17.78796 17.46005 16.22178 16.91866 16.24808 16.54747 16.77779 ## [937] 16.30747 16.93671 16.57135 16.64717 16.29048 16.49133 16.08467 16.19816 ## [945] 16.84333 16.01256 16.95237 17.25605 17.30110 16.67069 17.18615 16.81070 ## [953] 17.07441 16.82427 17.75016 17.77161 17.12579 16.15967 16.32598 16.71931 ## [961] 17.36472 16.91306 16.49229 16.37570 17.18497 16.83517 17.41335 16.25491 ## [969] 17.36015 17.37834 18.10737 16.20733 16.36659 16.68521 17.63915 16.65351 ## [977] 16.66799 17.04463 16.16653 16.65916 17.15032 16.80087 16.88071 16.64969 ## [985] 16.94993 15.87733 17.16749 17.45193 16.42774 16.76773 15.93248 17.49813 ## [993] 16.59596 16.95711 16.89192 16.92386 16.81038 17.22405 16.06465 15.77358 ## ## $boot_sds ## [1] 5.669277 5.508349 5.109292 5.592562 6.218733 5.307864 5.809315 4.899424 ## [9] 7.208243 5.458417 5.734239 4.787992 4.516531 4.370137 5.286856 5.800177 ## [17] 5.012863 6.309766 6.045292 4.921840 4.673309 5.287436 4.765139 5.333925 ## [25] 6.166147 7.541317 5.889731 4.904010 4.612772 6.218395 5.467560 6.223837 ## [33] 5.110560 5.358854 5.606700 4.684122 5.612416 5.286019 5.809526 4.989274 ## [41] 4.965773 5.390094 6.211669 4.988085 5.884405 4.482961 5.509400 5.508933 ## [49] 5.697158 6.150458 5.484107 5.240583 5.781409 5.269203 5.384987 5.139572 ## [57] 5.943782 4.249527 5.080733 5.045971 4.922043 6.608014 5.948594 5.553118 ## [65] 4.976964 5.355675 4.964096 6.008930 5.070808 6.479337 4.972689 5.282599 ## [73] 5.821185 5.946350 5.936063 4.199516 4.827795 5.030317 4.828092 5.089822 ## [81] 4.484215 5.390550 5.421295 5.395197 5.434002 5.001402 5.002902 5.750827 ## [89] 5.242838 5.182797 4.388755 5.821155 4.434183 5.559507 5.128159 5.043826 ## [97] 5.552359 5.036931 5.960320 5.894778 6.012889 5.387769 6.143185 5.525135 ## [105] 5.526565 5.687884 5.401752 4.657631 5.584659 5.431086 4.983146 5.770939 ## [113] 6.389793 4.993723 5.616583 5.541819 5.836037 5.115144 5.328142 5.614901 ## [121] 5.235661 5.175439 5.683657 5.806004 3.830777 4.604799 4.632568 5.194978 ## [129] 6.276799 4.635780 4.910419 4.200274 5.927151 5.262476 5.440839 5.629381 ## [137] 6.151632 5.308700 4.284609 5.644680 7.164149 4.710861 5.635203 6.464585 ## [145] 5.086197 5.722234 5.553721 5.192437 5.178818 4.574973 6.112224 5.277278 ## [153] 4.521001 5.271763 5.571575 5.213318 5.089882 4.710150 5.167669 5.327717 ## [161] 5.131110 4.971960 5.510781 5.199150 5.581366 6.136668 6.100888 5.954844 ## [169] 4.420062 4.675947 5.592506 4.532860 6.757629 5.800709 5.963455 5.769346 ## [177] 6.269866 5.809664 5.200662 5.039870 5.733799 5.072235 4.458281 6.198105 ## [185] 5.729800 5.429180 5.067705 5.838539 5.766541 4.838142 5.299943 4.888502 ## [193] 4.977343 5.155639 5.993686 4.683699 5.345948 5.354408 5.474036 5.141078 ## [201] 5.426939 4.957163 5.290081 6.342756 5.299247 5.230104 5.339983 5.294244 ## [209] 5.480981 4.924739 4.850202 6.206263 4.726634 4.880705 5.603770 5.397005 ## [217] 5.271676 5.825731 5.439690 5.806685 5.629737 4.517182 5.170393 5.527755 ## [225] 4.970875 5.600942 4.828678 4.757029 6.367782 5.643383 5.333641 5.731101 ## [233] 5.375504 5.727543 5.263414 5.331891 4.877895 5.380011 5.278720 5.132609 ## [241] 5.246073 5.914658 4.903399 5.124540 5.493577 5.042939 5.275788 5.729057 ## [249] 5.036021 6.904192 4.350137 6.367434 4.988176 6.164236 5.472143 5.914742 ## [257] 5.764948 5.167117 6.218050 4.759078 4.771939 4.806167 5.866601 5.896364 ## [265] 4.941511 4.669679 5.971075 4.799473 5.631947 4.521371 6.837624 5.758999 ## [273] 5.699130 4.780147 4.899566 4.734375 4.811192 5.860145 5.191585 5.440233 ## [281] 5.391730 5.283298 5.702956 5.509921 5.379408 4.849132 5.293163 5.080530 ## [289] 5.765737 5.520168 5.022043 6.601782 4.804895 5.359011 5.331187 5.624267 ## [297] 5.964134 6.095333 4.878584 5.053382 6.151942 5.293009 5.096226 5.473060 ## [305] 6.292115 5.485447 5.918487 4.719922 4.920197 6.103356 4.522213 4.897352 ## [313] 5.901608 5.450349 5.400803 5.258577 5.667187 5.583343 4.609447 4.676833 ## [321] 4.848359 4.583381 5.296744 5.998544 4.556362 4.600910 6.177752 5.309240 ## [329] 6.265441 5.983669 4.853106 5.155831 5.856315 5.095427 5.026916 6.093716 ## [337] 4.978370 5.587803 5.540588 6.197822 5.028848 6.180398 5.893777 6.281222 ## [345] 6.130393 4.513786 4.579872 4.650598 5.067987 4.967956 5.323755 5.650343 ## [353] 5.849026 5.313498 4.906459 4.447321 5.861254 6.091417 4.123884 6.350824 ## [361] 5.482860 5.161626 4.920577 5.218523 4.882035 5.684609 5.548581 4.424046 ## [369] 5.117988 5.255800 4.603356 5.896842 5.236412 5.419099 4.984052 6.166173 ## [377] 4.776923 4.957308 4.269468 4.954452 5.977942 5.178662 5.485672 4.636163 ## [385] 4.988910 6.224789 4.578611 5.346970 6.201602 5.464458 5.324962 5.623781 ## [393] 4.989653 6.153749 5.100176 5.503797 5.500942 5.284042 5.156966 4.451290 ## [401] 5.256292 5.566409 5.771891 5.136844 5.900659 6.050755 5.169927 6.328945 ## [409] 5.492473 5.405024 5.059255 5.723359 5.265623 6.271261 4.949182 7.129772 ## [417] 5.881810 5.605255 5.309854 5.696762 4.723416 4.854296 6.813987 5.571711 ## [425] 5.203545 4.551869 5.399752 6.829664 6.013998 4.936658 5.778855 5.167203 ## [433] 5.464527 7.403815 5.185640 6.047474 5.283099 4.964649 4.989754 7.124428 ## [441] 5.424114 6.175196 4.385240 5.295086 4.797235 4.799827 6.068061 5.402460 ## [449] 5.521802 5.323645 4.570692 5.180087 6.748420 5.474827 5.070344 5.494117 ## [457] 5.110445 5.106421 5.354015 5.712631 6.631424 4.658070 4.409409 6.629541 ## [465] 4.272344 5.452437 5.461192 5.414471 5.611960 6.624689 5.271773 4.485324 ## [473] 5.212952 5.350564 7.157857 5.090671 5.008218 5.245716 4.583362 7.013320 ## [481] 4.863126 5.253252 5.328005 4.919722 5.923461 6.215878 5.059282 6.334903 ## [489] 5.554206 5.457689 5.188770 5.887731 4.708679 5.503981 5.876050 4.336783 ## [497] 5.135768 5.868140 4.716863 4.978146 5.451158 5.755857 4.689894 5.232387 ## [505] 4.854791 5.365999 6.076017 6.199493 5.712603 5.183190 5.882318 4.640416 ## [513] 5.090225 5.067805 5.378030 5.394058 4.815639 5.383901 5.640261 5.539075 ## [521] 5.774063 5.803984 5.234692 5.036079 5.756800 5.213564 5.759884 4.810051 ## [529] 5.287736 4.377561 5.479318 5.817777 5.607851 5.870763 5.046783 7.079340 ## [537] 5.943646 5.636244 4.405085 5.521177 4.743231 6.151499 5.620904 6.131311 ## [545] 6.212100 6.379826 5.027431 4.299669 6.029650 4.692239 5.501436 5.232692 ## [553] 5.512188 5.169933 4.745413 6.374946 4.483546 4.976842 5.130401 5.171314 ## [561] 5.711356 4.820799 5.565019 4.904573 6.079394 5.639810 5.008685 6.574868 ## [569] 5.077572 5.788692 5.582009 5.831385 6.473742 5.525588 4.674388 6.277478 ## [577] 5.457228 5.134932 5.254777 5.280763 6.370609 4.746569 6.702959 4.441044 ## [585] 5.402938 5.486513 4.774802 5.405532 5.348135 4.387420 5.229502 5.727109 ## [593] 4.848545 5.148096 5.317991 5.866329 5.608382 5.200758 4.730587 5.734057 ## [601] 4.648363 6.028897 4.823451 5.983127 6.173709 5.597542 5.471962 5.994143 ## [609] 5.466635 5.538418 5.766781 4.885262 4.364738 4.229628 5.927275 5.206026 ## [617] 5.235118 5.507960 5.774346 5.677519 5.826407 5.699112 4.434282 4.701402 ## [625] 4.776380 5.746310 5.183075 5.629900 6.689166 5.421475 4.711610 5.465945 ## [633] 5.344789 5.544690 5.824884 4.891975 4.807112 6.173533 6.638182 5.683000 ## [641] 5.470535 5.168283 5.355560 5.657620 6.754150 5.793173 4.391000 5.138299 ## [649] 5.328578 5.226187 5.061057 5.161344 4.326504 6.196131 5.244858 5.402221 ## [657] 5.249255 5.212731 5.661118 5.152801 5.642794 4.671450 4.745872 4.326684 ## [665] 4.369963 4.706802 5.633828 4.764238 5.233685 5.317111 6.181375 5.352984 ## [673] 5.454328 5.739937 4.651983 5.586526 4.788429 6.232803 5.912870 4.882808 ## [681] 4.766566 5.372578 5.688828 5.531410 6.393524 5.659014 4.799702 6.009258 ## [689] 4.976536 5.702702 5.358783 5.650916 5.346163 5.549276 5.177960 6.290433 ## [697] 4.596657 5.463897 5.260796 5.443653 5.047310 5.335428 4.987661 4.858318 ## [705] 4.757633 5.129332 4.739391 4.879813 5.107134 5.176180 4.976160 5.026081 ## [713] 5.940426 5.414094 5.518971 6.019646 5.076985 5.674127 5.770418 5.292967 ## [721] 5.625585 5.759935 5.311945 5.954033 6.555496 4.147820 5.819625 6.090065 ## [729] 6.134194 5.198282 4.992998 5.224920 5.449490 5.070488 6.791194 5.980633 ## [737] 5.526554 4.738035 5.724984 4.955818 4.335746 5.299554 5.572962 3.928956 ## [745] 5.542717 6.781290 5.363506 5.023968 5.456428 4.669658 4.520462 4.698122 ## [753] 5.825826 4.554776 6.239634 4.984078 5.016275 4.842027 5.340701 5.066770 ## [761] 4.740747 5.768880 4.420328 4.866768 5.145507 5.207574 4.857123 5.441289 ## [769] 6.018195 5.663919 6.231833 4.436492 4.799239 5.205218 5.623671 5.579470 ## [777] 5.138087 4.648234 5.583077 5.934273 5.012666 5.175324 5.392028 6.088239 ## [785] 5.566350 4.832211 4.986781 5.028612 6.621569 5.385924 5.070501 4.966112 ## [793] 4.922910 6.751244 5.883182 4.427529 4.812153 5.754551 5.492683 5.665022 ## [801] 6.270733 4.847259 5.361722 6.305133 5.020333 4.132316 5.554914 5.720932 ## [809] 5.113226 5.512151 4.828679 5.748620 4.960810 5.047562 5.138460 5.485067 ## [817] 5.651207 5.541642 5.668446 4.955661 5.478733 5.356003 5.855924 4.676071 ## [825] 5.361941 5.690742 5.058681 6.342585 4.992347 5.856523 5.411070 5.522183 ## [833] 5.441917 4.752670 5.029355 4.836602 5.103580 6.022040 4.911669 5.679807 ## [841] 5.603809 5.268540 5.620898 5.467441 6.263353 5.238356 4.979048 5.298593 ## [849] 5.532087 5.316271 4.806540 5.613797 6.437276 5.100534 5.771849 5.500963 ## [857] 4.644098 6.346342 4.998152 6.258803 4.885733 5.257972 4.907668 5.834090 ## [865] 5.628843 4.607551 4.976500 5.912440 4.987278 6.422970 5.252544 5.063119 ## [873] 5.707828 4.966361 6.139665 6.129768 5.528423 5.364742 5.228927 5.554111 ## [881] 5.333567 4.520847 6.915562 5.534831 4.553030 6.127977 5.732583 5.691569 ## [889] 5.397207 5.370761 4.194879 6.090362 6.023668 4.954548 5.855418 5.740738 ## [897] 5.781743 5.898722 4.965829 4.860532 5.406209 5.850886 6.171433 6.382992 ## [905] 6.008717 5.324333 5.336358 5.311892 5.687700 6.138380 5.833875 4.565804 ## [913] 5.034039 6.078434 5.699993 6.227087 6.614332 6.241631 4.683072 4.769359 ## [921] 5.553602 5.570636 6.113072 5.203345 6.358322 5.233606 5.981362 5.437194 ## [929] 5.373023 6.445970 5.450706 5.173852 5.884422 5.120039 4.137771 6.010202 ## [937] 4.689393 5.282002 5.746122 5.146727 5.150762 5.701440 4.303148 4.384523 ## [945] 5.273475 4.913457 5.955601 5.233602 6.220879 5.392120 5.894625 5.624605 ## [953] 5.506664 5.555204 5.782561 5.597667 5.155400 4.753986 5.034780 5.355396 ## [961] 5.890781 5.648098 5.440233 4.720862 5.398098 5.701697 5.585825 5.559383 ## [969] 5.782810 6.160241 6.700965 5.022354 4.502207 5.252153 6.225580 5.668936 ## [977] 4.842923 4.613594 5.153302 5.286457 4.624214 5.165064 5.177915 5.012724 ## [985] 5.958666 4.974962 5.751360 6.436998 5.150759 5.902501 5.145397 6.304228 ## [993] 5.138148 5.689785 5.837269 5.418062 5.883751 5.743026 4.932924 4.437218 The estmeansd library uses Box-Cox method for estimating mean and sd when the data available are the median, minimum and maximum values. library(estmeansd) res_bc &lt;- bc.mean.sd(min.val = 13, med.val = 16, max.val = 42, n = 230) res_bc ## $est.mean ## [1] 16.79758 ## ## $est.sd ## [1] 3.55326 Again, the standard error from the mean can be estimated using get_SE function. get_SE(res_bc) ## $est.se ## [1] 0.3205701 ## ## $boot_means ## [1] 16.53855 17.16172 16.84746 17.26769 16.99600 16.40070 16.68867 17.35319 ## [9] 16.47715 17.00054 16.39592 16.69787 16.44430 16.38637 16.66363 16.75555 ## [17] 16.38342 16.23480 16.87008 16.97730 16.78287 17.20991 16.83006 16.95592 ## [25] 16.86877 16.84598 16.60542 16.58883 16.95352 16.92830 16.93559 16.73365 ## [33] 16.77089 16.67140 16.62247 16.49510 16.67180 16.80927 16.62490 15.93154 ## [41] 16.66524 16.44965 16.62981 16.60464 17.02054 16.82034 16.91287 16.74723 ## [49] 16.66625 17.49421 16.50913 16.87404 16.82580 16.75653 16.48956 17.39532 ## [57] 16.81609 16.78995 16.78702 16.75310 16.30938 16.46533 16.66659 16.92389 ## [65] 17.06997 16.87719 16.92230 16.68844 17.48527 16.32999 16.79631 16.49076 ## [73] 16.19053 16.54338 16.25810 16.68725 16.95426 16.44348 16.79267 16.91964 ## [81] 16.73076 16.78680 16.86212 16.06615 16.72927 17.29613 16.73062 16.74869 ## [89] 16.87979 17.22999 16.93810 16.85754 16.50604 17.26425 17.05474 16.57540 ## [97] 16.57971 16.35133 17.10891 16.45547 16.98462 16.91321 17.20115 16.18121 ## [105] 16.70763 16.65684 16.64388 16.59314 16.66566 16.98533 16.57492 16.68456 ## [113] 16.57619 16.75280 16.26121 16.63337 16.63603 16.61777 16.61831 16.77074 ## [121] 17.01998 16.91911 16.19127 16.89146 16.66941 16.47426 16.52808 16.34024 ## [129] 16.57619 17.01093 16.87493 16.66116 17.24104 16.65463 16.54245 16.89663 ## [137] 16.43957 16.53284 16.41137 16.73323 16.84514 16.97399 16.50004 16.36682 ## [145] 17.58424 16.57811 16.98137 17.13612 17.02551 16.50712 17.07327 16.68845 ## [153] 17.21339 16.45472 16.78635 17.01873 16.86124 17.01979 17.57762 17.38581 ## [161] 16.95522 16.64946 16.53337 16.30763 17.12675 16.61160 16.24340 16.18296 ## [169] 16.31539 17.45131 16.48885 16.44105 16.41080 16.87243 16.80773 16.78233 ## [177] 17.59130 16.80066 16.70148 16.85155 15.99938 16.66576 16.91393 17.10593 ## [185] 16.67805 17.24849 17.08898 16.80091 16.97568 16.87842 16.04852 16.98980 ## [193] 16.66124 17.40186 17.11418 16.82154 16.45975 16.91878 16.90580 16.60889 ## [201] 17.04261 16.97740 16.07934 16.91153 17.16160 17.36845 16.50313 16.44434 ## [209] 15.97912 16.34016 17.04500 16.74651 16.64481 16.60668 17.35666 16.36980 ## [217] 16.72280 16.72994 16.43293 17.06585 16.19975 16.46764 16.43989 16.36782 ## [225] 16.89052 16.66596 16.49836 16.75542 16.27259 16.76577 16.56546 16.86976 ## [233] 17.45279 16.85679 16.36821 16.70041 16.82832 16.82165 16.52397 16.69180 ## [241] 16.85287 17.43722 16.97074 16.58651 16.80781 17.16289 16.39103 16.75512 ## [249] 16.70298 16.12409 16.57071 16.59783 16.72044 16.60326 17.41887 16.78927 ## [257] 16.82112 17.19037 16.90375 16.65312 16.69827 16.88650 16.99641 16.26481 ## [265] 17.61130 16.26185 16.90153 16.52740 16.27028 16.30841 16.13914 16.91849 ## [273] 16.53002 16.63420 17.01184 16.83995 16.35769 17.73087 17.08429 16.17307 ## [281] 16.31304 16.87439 16.57657 17.04146 16.79446 16.61271 17.24376 17.48078 ## [289] 17.21659 16.93817 16.76901 16.98844 16.42593 16.59620 17.08517 15.93534 ## [297] 16.70427 16.50899 17.20000 16.61374 16.45964 16.58178 16.94086 16.50045 ## [305] 16.92374 16.70634 17.32533 16.63394 17.09020 16.88262 16.74967 16.48891 ## [313] 16.70368 17.00154 16.94325 17.55367 16.52266 16.87129 16.56381 16.82900 ## [321] 16.49188 16.50112 16.35215 16.78986 17.05724 17.04876 16.46678 16.75387 ## [329] 16.83788 16.50994 17.22934 16.91485 16.75861 16.70543 16.57665 17.07566 ## [337] 16.33382 16.22257 16.41514 16.55596 16.28800 16.80832 16.45164 16.63319 ## [345] 16.99973 16.76991 16.91269 15.84668 16.63956 17.21127 16.44080 16.62879 ## [353] 16.27920 17.32064 17.17510 16.74287 16.30417 16.22639 16.62527 17.19106 ## [361] 16.77408 16.46416 16.38475 16.85188 16.34296 16.46744 16.66397 16.76002 ## [369] 16.57630 17.14154 17.14472 16.78590 16.81236 17.20877 16.96504 16.60864 ## [377] 16.59647 16.65335 16.43910 16.54059 17.00116 16.85956 16.46515 16.42995 ## [385] 16.80363 16.92683 16.69778 16.54053 16.98770 16.97023 16.85020 16.69776 ## [393] 16.28644 16.89407 16.63712 16.99794 17.37501 16.60677 16.20686 16.87880 ## [401] 16.72866 16.55884 17.41405 16.73812 16.33178 16.70866 16.42742 17.00698 ## [409] 17.12601 17.00999 16.09743 16.45393 16.93042 16.44671 16.56388 16.86672 ## [417] 17.09565 16.68177 16.88781 16.41674 16.91354 16.56384 17.01849 16.91459 ## [425] 16.82336 16.63678 16.79713 16.36969 16.55402 17.24652 16.55832 16.70857 ## [433] 16.45813 16.71766 16.15417 16.87398 17.09179 17.34970 16.03713 16.77874 ## [441] 16.93939 16.41906 17.02958 17.29291 16.83049 16.95867 16.79623 16.56573 ## [449] 16.18661 16.42823 16.57685 16.81509 16.64287 16.33198 17.06193 16.88795 ## [457] 17.20158 16.42243 16.53874 16.29512 16.86825 16.51289 16.25547 17.59550 ## [465] 16.46574 16.98028 16.87928 16.72743 16.93192 16.69974 16.20907 16.35610 ## [473] 16.75804 17.30029 16.27646 16.54585 16.64614 17.08898 16.79712 16.62315 ## [481] 16.80776 16.68444 16.70655 16.42536 16.45076 16.85026 16.76346 16.02125 ## [489] 16.63515 16.72259 16.50896 16.53770 16.89143 16.62332 16.74160 17.08960 ## [497] 16.66772 16.68881 17.19268 16.80055 16.24587 16.69284 16.91328 16.97079 ## [505] 17.23166 16.75629 16.72728 16.84957 17.18293 16.75162 16.53064 16.81658 ## [513] 17.37288 16.95564 16.97942 16.30128 16.95140 16.17717 16.97443 16.74296 ## [521] 16.06306 17.05833 16.70495 17.01067 17.13317 16.26225 16.53278 17.04944 ## [529] 17.16689 16.57463 16.57686 16.91833 17.00159 16.76320 16.45295 17.06668 ## [537] 16.95356 16.62142 16.58989 16.54457 16.75465 16.59303 17.33738 16.98726 ## [545] 16.74079 17.08002 17.52354 17.00695 16.63873 16.86840 17.05941 16.20607 ## [553] 16.77003 17.09665 16.36659 16.29592 17.27293 16.52558 16.56473 16.96997 ## [561] 16.73457 17.09508 17.04982 16.85959 17.12244 16.72838 16.84402 16.63024 ## [569] 17.15018 16.68805 16.40171 16.24654 16.93218 16.51877 16.56053 16.47154 ## [577] 16.76290 16.78370 16.82536 16.91974 16.90697 17.18609 16.88057 16.52285 ## [585] 16.58904 17.07108 16.86712 17.11078 17.03025 17.27089 16.82204 16.55879 ## [593] 16.18616 16.62181 16.18040 17.35674 16.93192 16.75632 16.33962 16.24811 ## [601] 16.53597 17.01370 17.08313 16.65155 16.41650 16.62042 17.63284 16.85070 ## [609] 17.08466 17.15669 16.67650 16.90798 16.61464 16.69729 16.74704 16.99223 ## [617] 16.03040 16.38397 16.69972 16.37385 16.46408 16.78507 16.82286 17.26449 ## [625] 16.52820 16.52097 16.31231 17.00347 16.67405 16.31182 17.24908 16.88176 ## [633] 16.61917 16.54371 16.52864 16.32499 16.82075 17.12744 16.58330 16.83755 ## [641] 16.88958 16.65966 16.49005 16.35749 17.27677 16.89294 16.91755 17.29921 ## [649] 16.99591 16.77837 16.64829 16.41495 16.85533 16.67611 16.53016 17.20072 ## [657] 16.80568 16.58739 16.66256 17.01658 16.81588 16.83454 16.63970 16.64056 ## [665] 16.58912 16.61000 16.71940 16.44156 16.65449 16.39220 16.73003 16.60433 ## [673] 16.30743 16.56417 16.46866 17.41333 16.70161 17.04962 16.64505 16.77791 ## [681] 16.28411 17.45500 16.56972 16.30032 16.78049 16.85370 16.50799 16.49343 ## [689] 16.77566 16.84470 16.79258 17.20513 16.58204 17.01063 16.49107 17.27522 ## [697] 16.65469 16.98143 16.37711 17.03650 16.81945 17.07990 17.12262 17.04473 ## [705] 17.21986 16.71860 16.73482 16.57666 16.21299 16.21796 16.83238 16.40304 ## [713] 17.09325 17.13426 17.51818 16.37279 16.36719 16.56970 16.59251 16.50954 ## [721] 16.13266 16.28158 16.68655 17.09932 16.81514 16.70357 17.13596 17.16315 ## [729] 16.44675 16.87608 17.53992 16.49330 16.61133 16.43561 16.77096 16.62205 ## [737] 17.12499 16.55397 16.70515 16.32328 16.37555 16.36005 16.79047 16.73589 ## [745] 17.17424 16.61543 16.53134 16.67769 16.80384 17.72711 17.09926 16.86718 ## [753] 16.58203 16.95534 16.84390 16.62682 17.43345 16.62394 16.68643 16.62093 ## [761] 16.61046 16.55967 17.32239 16.52509 16.68021 16.43799 16.55776 16.71809 ## [769] 16.90588 16.44841 16.71535 16.68477 16.24259 17.38700 16.60268 16.19083 ## [777] 17.04780 17.32151 17.12372 17.23477 16.14405 16.81900 16.44705 16.78686 ## [785] 16.57265 16.88697 16.93324 16.78947 17.03214 16.74314 16.35234 16.77726 ## [793] 16.74788 16.63040 16.73047 16.80866 16.75775 16.41231 16.52820 16.55294 ## [801] 16.94417 16.61315 16.69209 16.69553 16.73042 16.42600 16.75757 16.77259 ## [809] 16.50340 17.26800 17.56690 17.48551 16.99716 16.38607 16.86259 16.65736 ## [817] 16.01072 16.57075 17.25219 16.27594 16.84225 17.01023 16.85623 17.09193 ## [825] 16.97508 16.67744 16.29909 16.36915 16.70713 16.88203 16.82178 16.67299 ## [833] 16.68475 16.62981 16.79068 17.14429 16.99612 16.85191 16.18978 17.15278 ## [841] 16.86382 16.30639 16.59065 16.77665 16.79838 17.52718 16.80842 16.52150 ## [849] 16.45441 17.31924 16.37601 16.86478 17.00703 16.73604 17.11461 16.55661 ## [857] 16.85795 17.58087 16.40591 16.57667 16.48227 16.14566 16.25445 16.66272 ## [865] 16.86025 16.36558 16.44958 16.74411 17.57601 16.60048 17.04629 16.36936 ## [873] 16.85287 17.02311 16.13828 16.76895 16.22488 17.29386 16.89654 16.44253 ## [881] 16.36088 16.42521 16.25618 16.81951 16.14763 16.59596 16.74590 16.66321 ## [889] 16.29463 16.61829 16.98271 17.00680 16.60846 17.90482 15.97628 16.46964 ## [897] 16.68703 17.12914 16.32559 16.50463 16.20398 16.72405 16.54268 16.54452 ## [905] 16.80625 16.77408 17.35981 17.20988 17.14160 17.01354 16.81230 17.00268 ## [913] 16.90444 16.03129 16.33793 16.60597 16.44810 16.50937 16.49090 17.02632 ## [921] 16.72954 17.19130 16.58994 16.68515 16.78762 16.65335 16.25103 16.61576 ## [929] 16.66212 16.15663 16.54025 16.77494 16.72628 16.34308 16.89130 17.23251 ## [937] 16.55938 17.19952 16.47165 16.59551 16.44614 16.55140 16.53338 16.97669 ## [945] 17.08316 17.06930 16.50562 17.06678 17.04878 17.35234 17.08521 16.98284 ## [953] 16.33376 16.67020 16.62487 16.96920 16.72940 16.89422 16.39369 16.49669 ## [961] 16.33677 16.81421 16.60117 16.67453 16.87237 16.15189 16.39754 16.95095 ## [969] 16.97305 16.93957 16.88274 17.20485 17.12793 16.90659 16.68984 16.56525 ## [977] 16.72233 16.95152 16.65042 16.63612 16.85717 17.26417 17.07524 17.14764 ## [985] 16.78829 16.78500 16.96694 16.78058 16.63431 16.62312 16.89596 15.95290 ## [993] 17.25899 16.37042 16.90438 16.77088 16.83227 16.40774 16.84319 17.30530 ## ## $boot_sds ## [1] 3.118987 4.163065 3.128556 4.414611 3.148322 3.188415 3.725596 3.815345 ## [9] 3.295001 3.193677 2.932956 3.996721 3.554840 3.276219 3.942116 4.438661 ## [17] 3.146084 3.844850 3.578471 3.285201 3.681074 3.396531 3.939913 3.672212 ## [25] 3.602080 3.460095 3.725530 3.633079 3.637021 3.499506 4.182657 4.447087 ## [33] 3.563430 3.053956 2.790237 3.322588 3.849292 3.934436 3.235606 3.139550 ## [41] 3.177857 3.219842 3.471712 3.209160 3.439551 3.458910 3.799652 3.544368 ## [49] 3.326254 3.311608 3.786553 3.647347 3.886440 3.664727 3.971034 4.292380 ## [57] 3.364573 4.015752 4.242725 3.653404 3.957056 2.961019 4.589478 3.488343 ## [65] 2.923345 3.552948 3.166937 3.659141 3.836929 3.240932 3.437772 4.078031 ## [73] 3.176393 3.277664 3.031664 3.574772 3.245607 3.387335 3.383982 3.792103 ## [81] 3.166707 3.185255 3.506065 3.137258 3.726499 4.921487 4.128464 2.965131 ## [89] 3.334903 3.526653 3.967907 4.227845 2.924715 3.332416 3.447559 3.048612 ## [97] 3.279116 3.148415 3.131302 3.300176 3.773657 3.674933 3.495758 3.242525 ## [105] 3.241785 3.129741 3.479734 3.311229 3.786124 3.026715 3.341887 3.229260 ## [113] 3.394429 3.907436 3.008998 3.513573 4.592018 3.264007 3.749958 3.237355 ## [121] 3.644377 3.703527 3.459729 3.623364 3.700346 3.150374 4.019042 3.225063 ## [129] 3.292281 3.263260 3.361686 3.903848 3.342370 3.103534 3.196258 3.586507 ## [137] 3.614634 3.225977 3.777554 3.155560 3.608171 3.549442 3.900876 3.368888 ## [145] 4.375370 3.371478 3.767805 4.197004 4.800930 3.620741 3.245993 3.322718 ## [153] 3.305684 3.180256 4.377647 3.733152 3.919294 3.356327 4.203464 3.237790 ## [161] 3.706943 3.656021 3.209076 3.299195 3.858353 3.324814 3.357279 3.225331 ## [169] 4.069053 4.248276 3.337441 3.340431 2.932334 3.708356 3.644704 4.146012 ## [177] 3.872917 3.568760 3.662199 3.225777 3.471235 3.841336 3.669270 3.627439 ## [185] 3.863476 3.325282 3.616874 3.412842 3.264246 3.515025 3.482801 3.443151 ## [193] 3.800133 4.130488 3.684752 3.586173 3.825210 3.647537 4.211901 3.338031 ## [201] 3.865687 3.087841 3.522549 3.389937 3.467975 3.774182 3.170123 3.534840 ## [209] 3.145845 3.350964 3.329744 3.720835 3.288948 3.304608 4.157182 4.016206 ## [217] 3.801921 3.463236 3.532969 3.702967 3.357718 3.620354 3.409518 3.074793 ## [225] 3.167737 3.666926 3.371386 3.542923 3.575950 3.759391 3.660935 3.339519 ## [233] 3.565958 3.855118 3.502587 3.063387 3.681780 3.352655 3.738328 3.190810 ## [241] 4.018548 4.202359 4.579168 3.595463 3.253461 3.608764 3.208275 3.279187 ## [249] 3.612703 3.108001 3.812366 3.883920 3.543050 3.216997 4.355952 3.786057 ## [257] 3.831845 3.982452 3.076688 3.326354 3.994550 4.175213 3.993119 2.904822 ## [265] 4.514892 3.336604 3.508754 3.444661 3.215583 3.321991 3.540025 3.275536 ## [273] 2.976101 3.659662 3.491818 3.563882 3.731702 4.444019 3.678138 3.433753 ## [281] 3.436406 4.063923 3.568428 3.948396 3.586255 3.548033 4.475138 4.064073 ## [289] 3.987076 3.778768 2.941198 3.913860 3.422441 3.243213 3.982633 2.606049 ## [297] 3.803300 2.967660 3.450428 3.528557 3.763330 3.762533 3.884074 3.322007 ## [305] 3.591037 3.075114 3.287047 3.024668 2.903103 3.793205 3.736170 2.948312 ## [313] 3.647624 4.016274 3.214999 3.941508 3.190068 3.244006 2.969484 3.224098 ## [321] 3.223736 3.392360 3.022042 3.700285 3.747954 4.529422 3.355396 3.514875 ## [329] 3.584555 3.224535 3.209143 3.537813 3.537726 3.179197 4.054327 4.116803 ## [337] 3.799517 4.109948 3.527053 3.656271 3.157312 3.501181 3.174967 3.365993 ## [345] 3.707243 3.687623 3.578792 3.700536 3.546299 4.037100 3.153064 3.451813 ## [353] 2.857949 3.436809 4.006921 3.133981 4.097774 3.238112 4.472018 3.837248 ## [361] 3.752648 4.031678 3.207097 3.952260 3.841300 3.064667 3.481593 3.632395 ## [369] 3.555463 3.937891 3.757170 3.588698 3.224971 4.411578 3.441140 3.382864 ## [377] 3.412275 3.610479 3.706991 3.605517 3.435322 3.482241 3.776917 2.471138 ## [385] 3.877954 3.746296 4.044918 3.137228 4.296352 3.252496 4.395972 3.845298 ## [393] 2.916160 3.588316 3.299518 4.295507 4.231514 3.211429 3.083544 3.313296 ## [401] 3.338672 2.844508 5.357146 3.369121 3.013823 3.664124 3.601953 3.807833 ## [409] 4.186464 3.899122 3.374402 3.472775 4.124444 2.704622 3.704080 3.088549 ## [417] 3.421998 3.527346 3.420698 3.787314 3.525808 3.094170 3.767541 3.178563 ## [425] 3.329025 3.048923 3.543825 2.771965 3.979808 3.193702 3.529247 3.277842 ## [433] 3.711734 3.290287 2.826598 3.061304 3.545603 3.835289 3.559297 3.446547 ## [441] 3.227768 3.354204 3.573656 4.080881 3.668870 3.232701 3.408280 3.388841 ## [449] 3.000478 4.200759 3.015591 3.204726 3.419986 3.018675 3.749585 3.590599 ## [457] 3.501728 3.694471 3.908108 3.395668 3.653492 4.214351 3.391009 4.192764 ## [465] 3.330914 3.007578 3.174616 3.337904 3.985673 3.597662 3.469156 4.124810 ## [473] 3.440216 3.779942 3.149474 3.206994 3.717717 2.964454 3.758749 3.465989 ## [481] 3.572879 3.048722 3.377147 2.907331 3.546210 3.268712 3.918481 3.709996 ## [489] 3.515939 3.650927 3.165580 3.448082 3.532877 3.148674 3.726987 4.235096 ## [497] 2.973605 3.534796 4.676765 3.680225 3.385101 3.160067 3.420458 3.550816 ## [505] 3.628709 3.573380 3.416261 3.078745 4.538943 3.675143 3.979956 3.612985 ## [513] 3.353724 3.961077 3.167655 3.563828 3.454661 3.716835 4.238612 3.503887 ## [521] 3.539068 4.256376 3.444708 3.909036 3.432825 3.601240 3.232477 4.152649 ## [529] 3.426693 4.080111 3.275983 4.480541 4.023521 4.479208 3.094773 3.814674 ## [537] 3.753848 3.881275 3.501158 3.402910 3.580367 3.404145 3.421320 3.994291 ## [545] 3.488285 3.311375 4.520765 3.393344 3.270052 3.498358 3.893993 3.169597 ## [553] 3.517245 3.626044 3.354067 3.121941 4.054472 3.541987 3.216038 4.390514 ## [561] 3.500757 4.249832 3.517445 4.040855 3.310257 3.613353 3.748514 3.203501 ## [569] 3.095675 3.600397 3.197291 2.964320 3.643485 3.327415 3.474187 3.508507 ## [577] 3.734031 4.040932 3.499595 3.915037 3.349897 3.676118 3.871131 3.738276 ## [585] 3.530261 3.722852 3.215753 3.623688 3.350093 4.174209 4.390585 3.840236 ## [593] 3.205209 3.088567 3.504053 4.119746 3.629258 3.188122 4.029170 3.119552 ## [601] 2.724213 3.283157 3.687499 3.495507 3.545418 3.632420 3.617260 3.089800 ## [609] 3.448851 3.815123 3.864141 3.576108 2.957123 3.822953 3.968441 3.514251 ## [617] 3.443229 3.372122 3.437809 3.328136 3.370528 2.972444 3.362452 3.844060 ## [625] 3.477156 3.105480 3.093476 3.814944 2.732489 3.422670 3.311809 3.628504 ## [633] 3.924907 3.621896 3.433843 3.574093 3.796609 4.143671 3.342434 3.046264 ## [641] 3.915063 3.608841 4.042441 2.992725 4.082711 4.159978 3.473690 3.073568 ## [649] 3.610996 2.801086 4.314819 3.420548 3.599318 3.036653 3.050736 3.452617 ## [657] 3.868433 3.712222 3.687629 3.560925 3.638989 4.137417 3.045407 3.518634 ## [665] 3.268133 3.042585 3.514124 3.026890 3.879301 3.575761 3.673190 3.601999 ## [673] 3.418796 3.201171 3.512070 4.279431 3.578118 3.580666 3.463787 3.392937 ## [681] 2.897410 3.500506 3.454685 3.787164 3.443618 2.892461 3.703412 3.824615 ## [689] 3.781620 4.014250 3.490038 4.467898 3.627221 4.238716 3.395129 3.734122 ## [697] 3.576727 3.011816 3.238168 3.562170 3.340729 3.232838 3.947463 3.564143 ## [705] 3.580127 3.374502 3.051596 3.264161 3.389745 3.469605 3.517966 3.600020 ## [713] 4.568184 4.090798 3.629317 3.418889 3.277099 3.296048 3.381816 3.359149 ## [721] 3.678728 2.781492 3.483793 3.029115 3.852640 3.715701 4.384869 3.350746 ## [729] 4.383594 3.730887 5.010518 3.484131 3.897634 2.952767 3.728422 3.404896 ## [737] 3.920097 3.250025 3.936229 3.327491 3.522542 3.152616 4.077235 3.922923 ## [745] 4.301049 3.056477 3.260847 3.918645 3.725800 4.360867 3.800377 3.295639 ## [753] 3.687896 4.487209 3.683871 3.645308 4.536000 3.181836 3.526228 2.957789 ## [761] 3.054450 3.459777 3.713923 3.624116 3.611734 3.102458 3.188463 3.999205 ## [769] 4.067762 3.187847 3.301334 3.350096 3.389712 3.407689 3.813634 2.901006 ## [777] 3.456077 3.889696 3.410100 4.175085 3.381333 3.813575 3.358913 3.612962 ## [785] 3.478830 3.462849 3.684838 4.037834 4.254413 3.076452 3.248725 4.169198 ## [793] 3.341871 3.452452 3.031142 3.452577 3.244797 3.451686 3.144704 3.086348 ## [801] 3.649428 3.459956 3.164521 3.858200 3.186287 3.543517 3.921093 3.383810 ## [809] 3.497869 4.477916 4.196560 4.048334 3.146904 3.269685 3.222466 4.105588 ## [817] 3.093108 3.592807 4.839527 3.532504 3.758243 3.406233 3.865893 3.184751 ## [825] 3.884920 3.775399 2.626797 3.866526 3.944168 3.277670 3.361363 3.556576 ## [833] 3.815645 3.835566 3.386188 3.693580 3.693809 3.797821 3.060715 4.131278 ## [841] 4.069430 2.808481 3.606162 3.303872 3.279409 4.078712 4.182787 3.830369 ## [849] 3.889372 3.034410 2.912417 3.384432 3.445728 3.811673 3.379953 3.729324 ## [857] 3.838796 4.503273 3.797466 3.145575 2.945436 2.690332 3.464504 3.315750 ## [865] 3.153387 4.106080 3.904416 3.712526 4.784392 2.963245 4.204969 3.672814 ## [873] 2.909335 3.516471 3.519499 3.744623 3.062973 4.063158 3.367640 3.463387 ## [881] 3.067771 3.643033 3.666723 4.165302 3.223407 3.557592 3.375011 3.638411 ## [889] 3.702199 3.907633 3.636880 3.665414 3.170902 3.471467 3.219847 3.852063 ## [897] 3.151140 4.159883 3.528328 3.484913 3.220102 3.396601 3.735399 2.999186 ## [905] 3.688063 3.218958 4.259463 4.774786 3.885768 3.438824 3.491892 3.589522 ## [913] 3.616030 2.736361 3.240731 3.675236 2.868779 3.417071 3.528073 3.506961 ## [921] 4.423738 3.166902 3.673617 4.070319 3.300601 3.280591 3.131195 4.026948 ## [929] 3.373737 3.375605 3.366802 3.441325 2.968311 2.826515 3.314644 3.644018 ## [937] 4.015419 4.639231 3.343865 3.195881 3.268739 3.426043 2.991416 3.963460 ## [945] 3.436009 3.883516 3.237230 3.782727 4.832585 3.353586 3.262508 3.615530 ## [953] 3.385231 3.317174 3.505840 3.721540 4.010973 3.705451 3.865648 3.453823 ## [961] 3.855976 3.891238 2.983689 3.700195 3.011344 3.800488 3.807066 3.037438 ## [969] 3.222418 3.609108 3.437103 3.227223 3.721323 3.546616 3.263066 3.251939 ## [977] 4.111107 3.930736 3.602448 3.750272 3.751193 3.608419 3.439083 4.015285 ## [985] 3.037031 3.811253 3.577906 3.840151 3.728452 3.610627 3.503262 2.758456 ## [993] 3.774201 3.103596 3.693632 3.613589 3.896618 3.506989 3.759905 4.769135 4.9.4 Inconsistency I2 The inconsistency \\(I^2\\) index is the sum of the squared deviations from the overall effect and weighted by the study size. Value &lt;25% is classified as low and greater than 75% as high heterogeneity. This test can be performed using metafor package (Viechtbauer 2010). The presence of high \\(I^2\\) suggests a need to proceed to meta-regression on the data to understand the source of heterogeneity. The fixed component were the covariates which were being tested for their effect on heterogeneity. The random effect components were the sensitivity and FPR. The \\(I^2\\) value for the TIA clinic study is 34.41%. As such meta-regression is not needed for that study. By contrast, the \\(I^2\\) is much higher for the spot sign study, necessitating metaregression. library(PRISMAstatement) #example from Spot sign paper. Stroke 2019 prisma(found = 193, found_other = 27, no_dupes = 141, screened = 141, screen_exclusions = 3, full_text = 138, full_text_exclusions = 112, qualitative = 26, quantitative = 26, width = 800, height = 800) 4.9.5 Metaanalysis of proportion This is an example of metaanalysis of stroke recurrence following management in rapid TIA clinic. A variety of different methods for calculating the 95% confidence interval of the binomial distribution. The mean of the binomial distribution is given by p and the variance by \\(\\frac{p \\times (1-p)}{n}\\). The term \\(z\\) is given by \\(1-\\frac{\\alpha}{2}\\) quantile of normal distribution. A standard way of calculating the confidence interval is the Wald method \\(p\\pm z\\times \\sqrt{\\frac{p \\times(1-p)}{n}}\\). The Freeman-Tukey double arcsine transformation tries to transform the data to a normal distribution. This approach is useful when occurence of event is rare. The exact or Clopper-Pearson method is suggested as the most conservative of the methods for calculating confidence interval for proportion. It is based on cumulative properties of the binomial distribution. The Wilson method has similarities to the Wald method. It has an extra term \\(z^2/n\\). There are many different methods for calculating the confidence interval for proportions. Investigators such as Agresti proposed that approximate methods are better than exact method (Agresti and Coull 1998). Brown and colleagues proposed the use of the Wilson method (Brown, Cai, and DasGupta 2001) library(metafor) #open software metafor #create data frame dat #xi is numerator #ni is denominator dat &lt;- data.frame(model=c(&quot;melbourne&quot;,&quot;paris&quot;,&quot;oxford&quot;,&quot;stanford&quot;,&quot;ottawa&quot;,&quot;new zealand&quot;), xi=c(7,7,6,2,31,2), ni=c(468,296, 281,223,982,172)) #calculate new variable pi base on ratio xi/ni dat$pi &lt;- with(dat, xi/ni) #Freeman-Tukey double arcsine trasformation dat &lt;- escalc(measure=&quot;PFT&quot;, xi=xi, ni=ni, data=dat, add=0) res &lt;- rma(yi, vi, method=&quot;REML&quot;, data=dat, slab=paste(model)) #create forest plot with labels metafor::forest(res, transf=transf.ipft.hm, targs=list(ni=dat$ni), xlim=c(-1,1.4),refline=res$beta[1], cex=.8, ilab=cbind(dat$xi, dat$ni), #position of data on x-axis ilab.xpos=c(-.6,-.4),digits=3) #par function combine multiple plots into one op &lt;- par(cex=.75, font=2) #position of column names on x-axis text(-1.0, 7.5, &quot;model &quot;,pos=4) text(c(-.55,-.2), 7.5, c(&quot;recurrence&quot;, &quot; total subjects&quot;)) text(1.4,7.5, &quot;Proportion [95% CI]&quot;, pos=2) par(op) Exact 95% confidence interval for proportion is provided below using the TIA data above. This solution was provided on stack overflow. This is performed using the binomial.test function. #exact confidence interval sapply(split(dat, dat$model), function(x) binom.test(x$xi, x$ni)$conf.int) ## melbourne new zealand ottawa oxford paris stanford ## [1,] 0.00603419 0.001411309 0.02154780 0.007875285 0.00955965 0.001087992 ## [2,] 0.03057375 0.041370853 0.04451101 0.045893259 0.04811612 0.032020390 The data needs to be transposed using t function. This generates one column for lower and another for upper CI. Note that sapply returns a matrix or array and the column names have to be assigned. #the data tmp &lt;- t(sapply(split(dat, dat$model), function(x) binom.test(x$xi, x$ni)$conf.int)) tmp ## [,1] [,2] ## melbourne 0.006034190 0.03057375 ## new zealand 0.001411309 0.04137085 ## ottawa 0.021547797 0.04451101 ## oxford 0.007875285 0.04589326 ## paris 0.009559650 0.04811612 ## stanford 0.001087992 0.03202039 The exact confidence is now put back into the dat data frame. dat$ci.lb &lt;- tmp[,1] #adding column to data frame dat dat$ci.ub &lt;- tmp[,2] #adding column to data frame dat dat &lt;- escalc(measure=&quot;PFT&quot;, xi=xi, ni=ni, data=dat, add=0) res &lt;- rma.glmm(measure=&quot;PLO&quot;, xi=xi, ni=ni, data=dat) #insert the exact confidence interval with(dat, metafor::forest(yi, ci.lb=ci.lb, ci.ub=ci.ub, ylim=c(-1.5,8.5), xlim=c(-1.1,1), refline=predict(res, transf=transf.ilogit)$pred, cex=.8, ilab=cbind(dat$xi, dat$ni), ilab.xpos=c(-.6,-.2),digits=3)) op &lt;- par(cex=.75, font=2) addpoly(res, row=-1, transf=transf.ilogit) abline(h=0) #position of column names on x-axis text(-.9, 7.5, &quot;Model&quot;, pos=2) text(c(-.55,-.2), 7.5, c(&quot;recurrence&quot;, &quot; total subjects&quot;)) text( 1, 7.5, &quot;Proportion [95% CI]&quot;, pos=2) 4.9.6 Metaanalysis of continuous data Metanalysis of continuous outcome data can be performed using standardised mean difference or ratio of means. It is available in the meta library using metacont function (Balduzzi, Rücker, and Schwarzer 2019). 4.9.7 Bivariate Metaanalysis The univariate method of Moses-Shapiro-Littenberg combines these measures (sensitivity and specificity) into a single measure of accuracy (diagnostic odds ratio)(Moses, Shapiro, and Littenberg 1993) . This approach has been criticized for losing data on sensitivity and specificity of the test. Similar to the univariate method, the bivariate method employs a random effect to take into account the within study correlation (Reitsma et al. 2005). Additionally, the bivariate method also accounts for the between-study correlation in sensitivity and specificity. Bivariate analysis is performed using mada package. A Bayesian library for bivariate analysis meta4diag is illustrated later. The example below is taken from a metaanalysis of spot sign as predictor expansion of intracerebral hemorrhage (Phan, Krishnadas, et al. 2019). The data for this analysis is available in the Data-Use sub-folder. library(mada) #spot sign data Data&lt;-read.csv(&quot;./Data-Use/ss150718.csv&quot;) #remove duplicates using subset #another way is to use filter from dplyr Dat&lt;-subset(Data, Data$retain==&quot;yes&quot;) (ss&lt;-reitsma(Dat)) ## Call: reitsma.default(data = Dat) ## ## Fixed-effects coefficients: ## tsens tfpr ## (Intercept) 0.2548 -1.9989 ## ## 27 studies, 2 fixed and 3 random-effects parameters ## logLik AIC BIC ## 52.0325 -94.0650 -84.1201 summary(ss) ## Call: reitsma.default(data = Dat) ## ## Bivariate diagnostic random-effects meta-analysis ## Estimation method: REML ## ## Fixed-effects coefficients ## Estimate Std. Error z Pr(&gt;|z|) 95%ci.lb 95%ci.ub ## tsens.(Intercept) 0.255 0.152 1.676 0.094 -0.043 0.553 . ## tfpr.(Intercept) -1.999 0.097 -20.664 0.000 -2.189 -1.809 *** ## sensitivity 0.563 - - - 0.489 0.635 ## false pos. rate 0.119 - - - 0.101 0.141 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Variance components: between-studies Std. Dev and correlation matrix ## Std. Dev tsens tfpr ## tsens 0.692 1.000 . ## tfpr 0.400 -0.003 1.000 ## ## logLik AIC BIC ## 52.033 -94.065 -84.120 ## ## AUC: 0.858 ## Partial AUC (restricted to observed FPRs and normalized): 0.547 ## ## I2 estimates ## Zhou and Dendukuri approach: 10.2 % ## Holling sample size unadjusted approaches: 64.2 - 79.9 % ## Holling sample size adjusted approaches: 4.7 - 5 % AUC(reitsma(data = Dat)) ## $AUC ## [1] 0.8576619 ## ## $pAUC ## [1] 0.5474316 ## ## attr(,&quot;sroc.type&quot;) ## [1] &quot;ruttergatsonis&quot; sumss&lt;-SummaryPts(ss,n.iter = 10^3) #bivariate pooled LR summary(sumss) ## Mean Median 2.5% 97.5% ## posLR 4.750 4.710 3.810 5.830 ## negLR 0.496 0.496 0.409 0.581 ## invnegLR 2.030 2.020 1.720 2.440 ## DOR 9.730 9.570 6.690 13.900 4.9.8 Metaanalysis of clinical trial. Fixed effect (Peto or Mantel-Haenszel) approaches assume that the population is the same for all studies and thus each study is the source of error. Random effect (DerSimonian Laird) assumes an additional source of error between studies.The random effect approach results in more conservate estimate of effect size confidence interval. A criticism of DerSimnonian and Laird approach is that it is prone to type I error especially when the number of number of studies is small (n&lt;20) or moderate heterogeneity. It’s estimated that 25% of the significant findings with DerSimonian Laird method may be non-significant with Hartung-Knapp method (BMC Medical Research Methodology 2014, 14:25). The Hartung-Knapp method (Stat Med 2001;20:3875-89) estimate the between studies variance and treat it as fixed. It employs quantile of t-distribution rather than normal distribution. The Hartung-Knapp method is available in meta and metafor package. The data is from Jama Cardiology on Associations of Omega-3 Fatty Acid Supplement Use With Cardiovascular Disease Risks Meta-analysis of 10 Trials Involving 77917 Individuals (Aung T 2018). Subsequently a meta-analysis (Hu 2019) reported that contrary to the earlier meta-analysis, Omega-3 lowers the risk of cardiovascular diseases with effect related to dose. The analysis using DerSimonian Laird and Hartung Knapp method is illustrated below. library(tidyverse) library(metafor) #Omega 3 data Year=c(2010,2014,2010,2007,2010,2010,2013,2008,2012,1999) Trials=c(&quot;DOIT&quot;,&quot;AREDS-2&quot;,&quot;SU.FOL.OM3&quot;,&quot;JELIS&quot;,&quot;Alpha Omega&quot;,&quot;OMEGA&quot;,&quot;R&amp;P&quot;,&quot;GISSI-HF&quot;,&quot;ORIGIN&quot;,&quot;GISSI-P&quot;) Treatment=c(29,213,216,262,332,534,733,783,1276,1552) Treatment.per=c(10.3,9.9,17.2,2.8,13.8,27.7,11.7,22.4,20.3,27.4) Control=c(35,208,211,324,331,541,745,831,1295,1550) Control.per=c(12.5,10.1,16.9,3.5,13.6,28.6, 11.9,23.9,20.7,27.3) #combine into data frame rct&lt;-data.frame(Year,Trials,Treatment,Treatment.per,Control,Control.per) %&gt;% mutate(Treatment.number=round(Treatment*100/Treatment.per,0), Control.number=round(Control*100/Control.per,0), group=ifelse(Year&gt;2010,&quot;wide&quot;,&quot;narrow&quot;)) %&gt;% rename(ai=Treatment,n1i=Treatment.number,ci=Control, n2i=Control.number,study=Trials) #peto&#39;s fixed effect method res &lt;- rma.peto(ai=ai, n1i=n1i, ci=ci, n2i=n2i, data=rct) print(res, digits=2) ## ## Equal-Effects Model (k = 10) ## ## I^2 (total heterogeneity / total variability): 0.00% ## H^2 (total variability / sampling variability): 0.93 ## ## Test for Heterogeneity: ## Q(df = 9) = 8.36, p-val = 0.50 ## ## Model Results (log scale): ## ## estimate se zval pval ci.lb ci.ub ## -0.04 0.02 -1.72 0.08 -0.08 0.00 ## ## Model Results (OR scale): ## ## estimate ci.lb ci.ub ## 0.97 0.93 1.00 result&lt;-predict(res, transf=exp, digits=2) #forest plot metafor::forest(res, targs=list(study=rct$study), main=&quot;RCT of Omega3 fatty acid for cardiovascular disease-Peto&quot;) The funnel plot is used here to illustrate presence or absence publication bias. In this case the funnel plot reflects the same findings as the low \\(I^2\\) value. There is one outlier. We will explore this further with the GOSH plot below. # funnel plot funnel(res, refline=0, level=c(90, 95, 99), shade=c(&quot;white&quot;, &quot;gray&quot;, &quot;darkgray&quot;)) Random effect (DerSimonian Laird) assumes an additional source of error between studies when calculating odds ratio. #DL datO3 &lt;- escalc(measure = &quot;OR&quot;,ai=ai, n1i=n1i, ci=ci, n2i=n2i,data=rct) res.DL&lt;-rma(yi,vi, method = &quot;DL&quot;,data=datO3) metafor::forest(res.DL,main=&quot;RCT of Omega3 fatty acid for cardiovascular disease-DL&quot;) DerSimonian Laird method may not be appropriate for metaanalysis with small number of studies . The recommendation is to use the Hartung Knapp random effect method. This analysis show that inspite of the small sample size the significant findings remain. #Hartung-ignore error message res.HK&lt;-rma.uni(yi,vi=1/vi,method=&quot;FE&quot;,knha=TRUE,data=datO3) metafor::forest(res.HK,main=&quot;RCT of Omega3 fatty acid for cardiovascular disease-HK&quot;) Subgroup analysis is one way of exploring the data for group effect. Metaregression is illustrated later. #datO3$group&lt;-rct3$group #plot subgroups res.group &lt;- rma(yi, vi, mods = ~ group, data=datO3) res.wide&lt;- rma(yi, vi, subset=(group==&quot;wide&quot;), data=datO3) res.wide res.narrow&lt;- rma(yi, vi, subset=(group==&quot;narrow&quot;), data=datO3) res.narrow #https://stackoverflow.com/questions/39392706/using-the-r-forestplot-package- #is-there-a-way-to-assign-variable-colors-to-boxe #confidence interval rmeta_conf &lt;- structure(list( mean = c(NA, NA, exp(-0.0676), exp(-0.0266), NA, exp(-0.0352)), lower = c(NA, NA, exp(-0.1873), exp(-0.0715), NA, exp(-0.0753)), upper = c(NA, NA, exp(0.0520), exp(0.0183), NA, exp(0.0049))), .Names = c(&quot;mean&quot;, &quot;lower&quot;, &quot;upper&quot;), row.names = c(NA, -6L), class = &quot;data.frame&quot;) #table data tabletext&lt;-cbind(c(&quot;&quot;,&quot;Trials&quot;,&quot;wide&quot;,&quot;narrow&quot;,NA,&quot;Summary&quot;), c(&quot;Events&quot;,&quot;(Drugs)&quot;,sum(filter(datO3,group==&quot;wide&quot;)$ai), sum(filter(datO3,group==&quot;narrow&quot;)$ai),NA,NA), c(&quot;Events&quot;,&quot;(Control)&quot;,sum(filter(datO3,group==&quot;wide&quot;)$ci), sum(filter(datO3,group==&quot;narrow&quot;)$ci),NA,NA), c(&quot;&quot;,&quot;OR&quot;,&quot;0.934&quot;,&quot;0.974&quot;,NA,&quot;0.965&quot;) ) #use forestplot as another library to perform forest plot library(forestplot) forestplot(tabletext, rmeta_conf,new_page = TRUE, is.summary=c(TRUE,TRUE,rep(FALSE,8),TRUE), clip=c(0.1,2.5), xlog=TRUE, col=fpColors(box=&quot;royalblue&quot;,line=&quot;darkblue&quot;, summary=&quot;royalblue&quot;)) Baujat plot is another method in addition to funnel plot explore heterogeneity # adjust margins so the space is better used par(mar=c(5,4,2,2)) # create Baujat plot to explore source of heterogeneity baujat(res.DL, xlim=c(0,20), ylim=c(0,0.2)) GOSH plot explores study heterogeneity using output of fixed effect model for all possible subsets ## fit FE model to all possible subsets # uses output from DL analysis sav &lt;- gosh(res.DL) ## Fitting 1023 models (based on all possible subsets). ### create GOSH plot ### red points for subsets that include and blue points ### for subsets that exclude study 16 (the ISIS-4 trial) plot(sav, out=dim(rct)[1], breaks=100) 4.9.9 Metaregression #separate metaregression for tsens and tfpr #tsens - setting single or multicentre for spot sign metaanalysis dat1&lt;-subset(Dat, Dat$result_id==1 ) (metass&lt;-reitsma(dat1, formula = cbind(tsens,tfpr)~PubYear+Study.type+Setting+Quality.assessment)) ## Call: reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + ## Study.type + Setting + Quality.assessment) ## ## Fixed-effects coefficients: ## tsens tfpr ## (Intercept) 298.7348 -164.8603 ## PubYear -0.1484 0.0828 ## Study.typeRetro 0.3554 -0.1489 ## SettingSingle 0.2283 -0.1301 ## Quality.assessment -0.0040 -0.0834 ## ## 26 studies, 10 fixed and 3 random-effects parameters ## logLik AIC BIC ## 48.5386 -71.0771 -45.7109 summary(metass) ## Call: reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + ## Study.type + Setting + Quality.assessment) ## ## Bivariate diagnostic random-effects meta-regression ## Estimation method: REML ## ## Fixed-effects coefficients ## Estimate Std. Error z Pr(&gt;|z|) 95%ci.lb 95%ci.ub ## tsens.(Intercept) 298.735 143.547 2.081 0.037 17.387 580.083 * ## tsens.PubYear -0.148 0.071 -2.078 0.038 -0.288 -0.008 * ## tsens.Study.typeRetro 0.355 0.310 1.146 0.252 -0.253 0.963 ## tsens.SettingSingle 0.228 0.495 0.462 0.644 -0.741 1.198 ## tsens.Quality.assessment -0.004 0.050 -0.081 0.936 -0.102 0.094 ## tfpr.(Intercept) -164.860 80.574 -2.046 0.041 -322.783 -6.938 * ## tfpr.PubYear 0.083 0.040 2.066 0.039 0.004 0.161 * ## tfpr.Study.typeRetro -0.149 0.172 -0.865 0.387 -0.486 0.188 ## tfpr.SettingSingle -0.130 0.276 -0.471 0.638 -0.672 0.412 ## tfpr.Quality.assessment -0.083 0.026 -3.190 0.001 -0.135 -0.032 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Variance components: between-studies Std. Dev and correlation matrix ## Std. Dev tsens tfpr ## tsens 0.657 1.000 . ## tfpr 0.297 0.312 1.000 ## ## logLik AIC BIC ## 48.539 -71.077 -45.711 ## ## ## I2 estimates ## Zhou and Dendukuri approach: 0 % ## Holling sample size unadjusted approaches: 66.8 - 80.5 % ## Holling sample size adjusted approaches: 5.4 - 5.8 % #tfpr - setting single or multicentre for spot sign metaanalysis dat1&lt;-subset(Dat, Dat$result_id==1 ) (metassfull&lt;-reitsma(dat1, formula = cbind(tsens,tfpr)~ PubYear+clinical+Study.type+CTA6hrs+Setting+Quality.assessment)) ## Call: reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + ## clinical + Study.type + CTA6hrs + Setting + Quality.assessment) ## ## Fixed-effects coefficients: ## tsens tfpr ## (Intercept) 297.5416 -144.2754 ## PubYear -0.1480 0.0724 ## clinical -0.4703 -0.1810 ## Study.typeRetro 0.4884 -0.0989 ## CTA6hrsyes -0.2141 -0.2342 ## SettingSingle 0.2568 -0.1435 ## Quality.assessment 0.0130 -0.0738 ## ## 26 studies, 14 fixed and 3 random-effects parameters ## logLik AIC BIC ## 48.4477 -62.8955 -29.7243 summary(metassfull) ## Call: reitsma.default(data = dat1, formula = cbind(tsens, tfpr) ~ PubYear + ## clinical + Study.type + CTA6hrs + Setting + Quality.assessment) ## ## Bivariate diagnostic random-effects meta-regression ## Estimation method: REML ## ## Fixed-effects coefficients ## Estimate Std. Error z Pr(&gt;|z|) 95%ci.lb 95%ci.ub ## tsens.(Intercept) 297.542 150.543 1.976 0.048 2.482 592.601 * ## tsens.PubYear -0.148 0.075 -1.977 0.048 -0.295 -0.001 * ## tsens.clinical -0.470 0.333 -1.411 0.158 -1.124 0.183 ## tsens.Study.typeRetro 0.488 0.321 1.523 0.128 -0.140 1.117 ## tsens.CTA6hrsyes -0.214 0.322 -0.665 0.506 -0.845 0.417 ## tsens.SettingSingle 0.257 0.495 0.519 0.604 -0.714 1.228 ## tsens.Quality.assessment 0.013 0.051 0.255 0.799 -0.087 0.113 ## tfpr.(Intercept) -144.275 82.720 -1.744 0.081 -306.403 17.852 . ## tfpr.PubYear 0.072 0.041 1.761 0.078 -0.008 0.153 . ## tfpr.clinical -0.181 0.187 -0.969 0.333 -0.547 0.185 ## tfpr.Study.typeRetro -0.099 0.177 -0.558 0.577 -0.446 0.249 ## tfpr.CTA6hrsyes -0.234 0.177 -1.322 0.186 -0.581 0.113 ## tfpr.SettingSingle -0.144 0.274 -0.524 0.600 -0.680 0.393 ## tfpr.Quality.assessment -0.074 0.027 -2.774 0.006 -0.126 -0.022 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Variance components: between-studies Std. Dev and correlation matrix ## Std. Dev tsens tfpr ## tsens 0.650 1.000 . ## tfpr 0.285 0.188 1.000 ## ## logLik AIC BIC ## 48.448 -62.895 -29.724 ## ## ## I2 estimates ## Zhou and Dendukuri approach: 0 % ## Holling sample size unadjusted approaches: 66.8 - 80.5 % ## Holling sample size adjusted approaches: 5.4 - 5.8 % Plot year against tsens from metaregression #using output from spot sign library(ggplot2) library(lubridate) ssr&lt;-as.data.frame(ss$residuals) #convert character to year ssr$Year&lt;-as.Date(as.character(Dat$PubYear),&quot;%Y&quot;) ssr$Quality&lt;-Dat$Quality.assessment ggplot(ssr, aes(x=ssr$Year,y=ssr$tsens))+geom_point()+ scale_x_date()+geom_smooth(method=&quot;lm&quot;)+ ggtitle(&quot;Relationship between transformed sensitivity and Publication Year&quot;)+ labs(x=&quot;Year&quot;,y=&quot;transformed sensitivity&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 4.9.9.1 summary Positive and Negative Likelihood Ratio Positive likelihood ratio (PLR) is the ratio of sensitivity to false positive rate (FPR); the negative (NLR) likelihood ratio is the ratio of 1-sensitivity to specificity. A PLR indicates the likelihood that a positive spot sign (test) would be expected in a patient with target disorder compared with the likelihood that the same result would be expected in a patient without target disorder. Using the recommendation by Jaeschke et al(Jaeschke, Guyatt, and Sackett 1994) a high PLR (&gt;5) and low NLR (&lt;0.2) indicate that the test results would make moderate changes in the likelihood of hematoma growth from baseline risk. PLRs of &gt;10 and NLRs of &lt;0.1 would confer very large changes from baseline risk. The pooled likelihood ratios were used to calculate post-test odds according to Bayes’ Theorem and post-test probabilities of outcome after a positive test result for a range of possible values of baseline risk. Data from likelihood ratio can be used to create Fagan’s normogram. #The code for this is available at #&quot;https://raw.githubusercontent.com/achekroud/nomogrammer/master/nomogrammer.r&quot; library(nomogrammer) p&lt;-nomogrammer(Prevalence = .234, Plr = 4.85, Nlr = 0.49) p+ggtitle(&quot;Fagan&#39;s normogram for Spot Sign and ICH growth&quot;) #ggsave(p,file=&quot;Fagan_SpotSign.png&quot;,width=5.99,height=3.99,units=&quot;in&quot;) 4.10 Data simulation Data simulation is an important aspects of data science. The example below is taken from our experience trying to simulate data from recent clot retrieval trials in stroke (Berkhemer et al. 2015) (Campbell et al. 2015). Simulation is performed using simstudy library. library (simstudy) library(tidyverse) #T is Trial def &lt;- defData(varname = &quot;T&quot;, dist = &quot;binary&quot;, formula = 0.5) #early neurological improvement (ENI) .37 in TPA and .8 in ECR #baseline NIHSS 13 in TPA and 17 in ECR def &lt;- defData(def, varname = &quot;ENI&quot;, dist = &quot;normal&quot;, formula = .8-.53*T, variance = .1) #baseline NIHSS 13 in TPA and 17 in ECR def &lt;- defData(def, varname = &quot;Y1&quot;, dist = &quot;normal&quot;, formula=13, variance = 1) def &lt;- defData(def, varname = &quot;Y2&quot;, dist = &quot;normal&quot;, formula = &quot;Y1*ENI - 5 * T &gt;5&quot;,variance = 1) def &lt;- defData(def, varname = &quot;Y3&quot;, dist = &quot;normal&quot;, formula = &quot;Y2- 4*T&gt;2&quot;,variance = 1) def &lt;- defData(def, varname = &quot;Y4&quot;, dist = &quot;normal&quot;, formula = &quot;Y3- 2*T&gt;0&quot;, variance = 1) #male def &lt;- defData(def,varname = &quot;Male&quot;, dist = &quot;binary&quot;, formula = 0.49*T) #diabetes .23 in TPA and .06 in ECR def &lt;- defData(def,varname = &quot;Diabetes&quot;, dist = &quot;binary&quot;, formula = .23-.17*T) #HT .66 TPA vs .6 ECR def &lt;- defData(def,varname = &quot;HT&quot;, dist = &quot;binary&quot;, formula = .66-.06*T) #generate data frame dtTrial &lt;- genData(500, def) #define parameter for mRS #Add conditional column with field name &quot;mRS&quot; dtTime &lt;- addPeriods(dtTrial, nPeriods = 4, idvars = &quot;id&quot;, timevars = c(&quot;Y1&quot;, &quot;Y2&quot;, &quot;Y3&quot;,&quot;Y4&quot;), timevarName = &quot;Y&quot;) dtTime ## id period T ENI Male Diabetes HT Y timeID ## 1: 1 0 0 -0.07917888 0 0 1 13.1982656 1 ## 2: 1 1 0 -0.07917888 0 0 1 -0.6170137 2 ## 3: 1 2 0 -0.07917888 0 0 1 1.9725600 3 ## 4: 1 3 0 -0.07917888 0 0 1 -0.5244530 4 ## 5: 2 0 0 0.63132133 1 0 1 12.6286712 5 ## --- ## 1996: 499 3 0 0.14824903 0 0 1 0.5098237 1996 ## 1997: 500 0 0 0.80704138 1 0 1 12.3427561 1997 ## 1998: 500 1 0 0.80704138 1 0 1 1.3319651 1998 ## 1999: 500 2 0 0.80704138 1 0 1 -2.4721920 1999 ## 2000: 500 3 0 0.80704138 1 0 1 -1.7111199 2000 #check that 2 groups are similar at start but not at finish t.test(Y1~T,data=dtTrial) ## ## Welch Two Sample t-test ## ## data: Y1 by T ## t = 0.54733, df = 497.91, p-value = 0.5844 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -0.1230718 0.2181206 ## sample estimates: ## mean in group 0 mean in group 1 ## 12.96705 12.91953 t.test(Y4~T,data=dtTrial) ## ## Welch Two Sample t-test ## ## data: Y4 by T ## t = 6.275, df = 484.36, p-value = 7.765e-10 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 0.4187510 0.8005452 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.58561980 -0.02402832 t.test(Male~T,data=dtTrial) ## ## Welch Two Sample t-test ## ## data: Male by T ## t = 0.53764, df = 497.21, p-value = 0.5911 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -0.06394389 0.11212316 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.5142857 0.4901961 #putting the 4 time periods together - long format dtTime &lt;- addPeriods(dtTrial, nPeriods = 3, idvars = &quot;id&quot;, timevars = c(&quot;Y1&quot;, &quot;Y2&quot;, &quot;Y3&quot;,&quot;Y4&quot;), timevarName = &quot;Y&quot;) #summarise data using group_by dtTime2&lt;-dtTime %&gt;% group_by(period, T) %&gt;% summarise(meanY=mean(Y), sdY=sd(Y), upperY=meanY+sdY, lowerY=meanY-sdY) ## `summarise()` has grouped output by &#39;period&#39;. You can override using the `.groups` ## argument. #write.csv(dtTime, file=&quot;./Data-Use/dtTime_simulated.csv&quot;) #write.csv(dtTrial, file=&quot;./Data-Use/dtTrial_simulated.csv&quot;) References "],["multivariate-analysis.html", "Chapter 5 Multivariate Analysis 5.1 Multivariate regression 5.2 Principal component analysis 5.3 Independent component analysis 5.4 Partial least squares", " Chapter 5 Multivariate Analysis The following section illustrates the different methods in multivariate analyses. These methods are not to be confused with the more multivariable analyses discussed under Statistics. 5.1 Multivariate regression Multivariable and multivariate regression are often used interchangeably. Some use the term multivariate when there are more than one dependent variables. Multivariable regression refers to linear, logistic or survival curve analysis in the previous chapter. Multivariate regression refers to nested models or longitudinal models or more complex type of analyses described below. 5.1.1 Penalised regression We used penalised logistic regression (PLR) to assess the relationship between the ASPECTS regions and stroke disability (binary outcome) (Phan et al. 2013). PLR can be conceptualized as a modification of logistic regression. In logistic regression, there is no algebraic solution to determine the parameter estimate (β coefficient) and a numerical method (trial and error approach) such as maximum likelihood estimate is used to determine the parameter estimate. In certain situations overfitting of the model may occur with the maximum likelihood method. This situation occurs when there is collinearity (relatedness) of the data. To circumvent this, a bias factor is introduced into the calculation to prevent overfitting of the model. The tuning (regularization) parameter for the bias factor is chosen from the quadratic of the norms of the parameter estimate. This method is known as PLR. This method also allows handling of a large number of interaction terms in the model. We employed a forward and backward stepwise PLR that used all the ASPECTS regions in the analysis, calling on the penalized function in R programming environment. This program automatically assessed the interaction of factors in the regression model in the following manner. The choice of factors to be added/deleted to the stepwise regression was based on the cost complexity statistic. The asymmetric hierarchy principle was used to determine the choice of interaction of factors. In this case, any factor retained in the model can form interactions with others that are already in the model and those that are not yet in the model. In this analysis, we have specified a maximum of 5 terms to be added to the selection procedure. The significance of the interactions was plotted using a previously described method. We regressed the dichotomized mRS score against ASPECTS regions, demographic variables (such as age and sex), physiological variables (such as blood pressure and serum glucose level) and treatment (rt-PA). The results are expressed as β coefficients rather than as odds ratio for consistency due to the presence of interaction terms. library(mice) ## ## Attaching package: &#39;mice&#39; ## The following object is masked from &#39;package:kernlab&#39;: ## ## convergence ## The following object is masked from &#39;package:BiocManager&#39;: ## ## version ## The following object is masked from &#39;package:signal&#39;: ## ## filter ## The following object is masked from &#39;package:pillar&#39;: ## ## squeeze ## The following objects are masked from &#39;package:BiocGenerics&#39;: ## ## cbind, rbind ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; &quot;Cell.shape&quot; ## [5] &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; ## [9] &quot;Normal.nucleoli&quot; &quot;Mitoses&quot; &quot;Class&quot; #check for duplicates sum(duplicated(BreastCancer)) ## [1] 8 #remove duplicates #keep Id to avoid creation of new duplicates #BreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows #impute missing data #m is number of multiple imputation, default is 5 #output is a list imputed_Data &lt;- mice(BreastCancer, m=5, maxit = 5, method = &#39;pmm&#39;, seed = 500) ## ## iter imp variable ## 1 1 Bare.nuclei ## 1 2 Bare.nuclei ## 1 3 Bare.nuclei ## 1 4 Bare.nuclei ## 1 5 Bare.nuclei ## 2 1 Bare.nuclei ## 2 2 Bare.nuclei ## 2 3 Bare.nuclei ## 2 4 Bare.nuclei ## 2 5 Bare.nuclei ## 3 1 Bare.nuclei ## 3 2 Bare.nuclei ## 3 3 Bare.nuclei ## 3 4 Bare.nuclei ## 3 5 Bare.nuclei ## 4 1 Bare.nuclei ## 4 2 Bare.nuclei ## 4 3 Bare.nuclei ## 4 4 Bare.nuclei ## 4 5 Bare.nuclei ## 5 1 Bare.nuclei ## 5 2 Bare.nuclei ## 5 3 Bare.nuclei ## 5 4 Bare.nuclei ## 5 5 Bare.nuclei #choose among the 5 imputed dataset completeData &lt;- complete(imputed_Data,2) #convert multiple columns to numeric #lapply output a list BreastCancer2&lt;-lapply(completeData[,-c(11)], as.numeric) #list BreastCancer2&lt;-as.data.frame(BreastCancer2) BreastCancer2$Class&lt;-BreastCancer$Class #convert factor to numeric for calculatin of vif BreastCancer2$Class&lt;-as.character(BreastCancer2$Class) BreastCancer2$Class[BreastCancer2$Class==&quot;benign&quot;]&lt;-0 BreastCancer2$Class[BreastCancer2$Class==&quot;malignant&quot;]&lt;-1 BreastCancer2$Class&lt;-as.numeric(BreastCancer2$Class) BC &lt;- unique(BreastCancer2) # Remove duplicates #check correlation library(ggcorrplot) ## ## Attaching package: &#39;ggcorrplot&#39; ## The following object is masked from &#39;package:rstatix&#39;: ## ## cor_pmat ggcorrplot(cor(BC), p.mat=cor_pmat(BC),hc.order=T, type=&quot;lower&quot;, colors=c(&quot;red&quot;,&quot;white&quot;,&quot;blue&quot;),tl.cex = 8) 5.1.2 MARS Multivariate adaptive regression spline (MARS) is a non-linear regression method that fits a set of splines (hinge functions) to each of the predictor variables i.e. different hinge function for different variables (Friedman and Roosen 1995). As such, the method can be used to plot the relationship between each variable and outcome. Use in this way, the presence of any threshold effect on the predictors can be graphically visualized. The MARS method is implemented in R programming environment in the earth package. library(earth) BC&lt;-BC[-1] Fit&lt;-earth(Class ~.,data= BC, nfold=10,ncross=30, varmod.method = &quot;none&quot;, glm=list(family=binomial)) plotmo(Fit) ## plotmo grid: Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size ## 4 1 1 1 2 ## Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses ## 1 3 1 1 summary(Fit) ## Call: earth(formula=Class~., data=BC, glm=list(family=binomial), nfold=10, ncross=30, ## varmod.method=&quot;none&quot;) ## ## GLM coefficients ## Class ## (Intercept) -3.0367094 ## h(Cl.thickness-4) 0.7610206 ## h(5-Cell.size) -0.4035770 ## h(5-Cell.shape) -0.6270418 ## h(6-Bare.nuclei) -0.5975594 ## h(Bl.cromatin-3) -0.6211132 ## h(5-Bl.cromatin) -0.8983521 ## h(Bl.cromatin-5) 0.9870000 ## h(Normal.nucleoli-2) 5.5785046 ## h(4-Normal.nucleoli) 2.9930778 ## h(Normal.nucleoli-4) -6.7265575 ## h(Normal.nucleoli-8) 3.9273365 ## h(3-Mitoses) -1.1559421 ## ## GLM (family binomial, link logit): ## nulldev df dev df devratio AIC iters converged ## 889.065 689 101.133 677 0.886 127.1 8 1 ## ## Earth selected 13 of 18 terms, and 7 of 9 predictors ## Termination condition: Reached nk 21 ## Importance: Cell.size, Bare.nuclei, Cl.thickness, Normal.nucleoli, Cell.shape, ... ## Number of terms at each degree of interaction: 1 12 (additive model) ## Earth GCV 0.03174149 RSS 20.3433 GRSq 0.8599283 RSq 0.8695166 CVRSq 0.8481011 ## ## Note: the cross-validation sd&#39;s below are standard deviations across folds ## ## Cross validation: nterms 12.38 sd 0.75 nvars 7.22 sd 0.50 ## ## CVRSq sd ClassRate sd MaxErr sd AUC sd MeanDev sd CalibInt sd ## 0.848 0.05 0.963 0.02 -1 0.859 0.991 0.008 0.252 0.162 6.01 46.7 ## CalibSlope sd ## 13.6 57.5 5.1.3 Mixed modelling In a standard regression analysis, the data is assumed to be random. Mixed models assume that there are more than one source of random variability in the data. This is expressed in terms of fixed and random effects. Mixed modeling is a useful technique for handling multilevel or group data. The intraclass correlation (ICC) is used to determine if a multilevel analysis is necessary ie if the infarct volume varies among the surgeon or not. ICC is the between group variance to the total variance. If the ICC approaches zero then a simple regression model would suffice. There are several R packages for performing mixed modeling such as lme4. Mixed modeling in meta-regression is illustrated in the section on Metaanalysis. An example of mixed model using Bayesian approach with INLA is provided in the Bayesian section 5.1.3.1 Random intercept model In a random intercept or fixed slope multilevel model the slope or gradient of the fitted lines are assumed to be parallel to each other and the intercept varies for different groups. This can be the case of same treatment effect on animal experiments performed by different technician or same treatment in different clusters of hospitals. There are several approached to performing analysis with random intercept model. The choice of the model depends on the reason for performing the analysis. For example, the maximum likelihood estimation (MLE) method is better than restricted maximum likelihood (RMLE) in that it generates estimates for fixed effects and model comparison. RMLE is preferrred if there are outliers. 5.1.3.2 Random slope model In a random slope model, the slopes are not paralleled 5.1.4 Trajectory modelling Trajectory analysis attempts to group the behaviour of the subject of interest over time. There are several different approaches to trajectory analysis: data in raw form or after orthonal transformation of the data in principal component analysis. Trajectory analysis is different from mixed modelling in that it examines group behaviour. The output of trajectory analysis is only the beginning of the modeling analysis. For example, the analysis may identify that there are 3 groups. These groups are labelled as group A, B and C. The next step would be to use the results in a modelling analysis of your choice. A useful library for performing trajectory analysis is akmedoids. This library anchored the analysis around the median value. The analysis requires the data in long format. The traj library is similar to the one in Stata. It uses several steps including factor and cluster analyses to idetify groups. The traj model prefers data in wide format. 5.1.5 Generalized estimating equation (GEE) GEE is used for analysis of longitudinal or clustered data. GEE is preferred when the idea is to discover the group effect or population average (marginal) log odds (Hubbard AE 2010). This is contrast with the mixed model approach to evaluate the average subject via maximum likelihood estimation. The fitting for mixed model is complex compare to GEE and can breakdown. The library for performing GEE is gee or geepack. library(tidyverse) library(gee) #open simulated data from previous chapter dtTime&lt;-read.csv(&quot;./Data-Use/dtTime_simulated.csv&quot;) %&gt;% rename(NIHSS=Y) %&gt;% mutate (NIHSS=abs(NIHSS)) (fit&lt;-gee(ENI~T+Diabetes+NIHSS, id=id, corstr = &quot;unstructured&quot;, tol = 0.001, maxiter = 25, #data=dtTrial_long) data=dtTime)) summary(fit) 5.2 Principal component analysis Principal component analysis (PCA) is a data dimension reduction method which can be applied to a large dataset to determine the latent variables (principal components) which best represent that set of data. A brief description of the method is described here and a more detailed description of the method can be found in review (Friston et al. 2000). The usual approach to PCA involves eigen analysis of a covariance matrix or singular value decomposition. PCA estimates an orthogonal transformation (variance maximising) to convert a set of observations of correlated variables into a set of values of uncorrelated (orthogonal) variables called principal components. The first extracted principal component aligns in the direction that contains most of the variance of observed variables. The next principal component is orthogonal to the first principle component and contains the second most of spread of variance. The next component contains the third most of spread, and so on. The latter principal components are likely to represent noise and are discarded. Expressing this in terms of our imaging data, each component yields a linear combination of ‘ischemic’ voxels that covary with each other. These components can be interpreted as patterns of ischemic injury. The unit of measurement in PCA images is the covariance of the data. In the case of MR images, each voxel is a variable, leading to tens of thousands of variables with relatively small numbers of samples. Specialised methods are required to compute principal components. There are situations in which PCA may not work well if there is non-linear relationship in the data. Based on cosine rule, principal components from different data are similar if values approach 1 and dissimilar if values approach 0 (Singhal et al. 2012). 5.2.1 PCA with MRI Here we illustrate multivariate analysis with mand library. #modified commands from mand vignette library(mand) ## Loading required package: msma data(&quot;atlas&quot;) data(&quot;atlasdatasets&quot;) data(&quot;template&quot;) atlasdataset=atlasdatasets$aal tmpatlas=atlas$aal #overlay images coat(template, tmpatlas, regionplot=TRUE, atlasdataset=atlasdataset, ROIids = c(1:2, 41:44), regionlegend=TRUE) Simulate a dataset using mand. data(&quot;diffimg&quot;) #mand data(&quot;baseimg&quot;) data(&quot;mask&quot;) data(&quot;sdevimg&quot;) diffimg2 = diffimg * (tmpatlas %in% 41:44) img1 = simbrain(baseimg = baseimg, diffimg = diffimg2, sdevimg=sdevimg, mask=mask, n0=20, c1=0.01, sd1=0.05) #dim(img1$S) #40 6422 #mand takes matrix array as argument fit=msma(img1$S,comp=2) plot(fit, v=&quot;score&quot;, axes = 1:2, plottype=&quot;scatter&quot;) midx = 1 ## the index for the modality vidx = 1 ## the index for the component Q = fit$wbX[[midx]][,vidx] outstat1 = rec(Q, img1$imagedim, mask=img1$brainpos) coat(template, outstat1) #https://rdrr.io/cran/mand/f/vignettes/a_overview.Rmd #https://rdrr.io/cran/caret/man/plsda.html Let’s apply mand on imaging data. First we use pattern matching to extract the relevant nii files into a list. #remotes::install_github(&quot;neuroconductor/MNITemplate&quot;) library(MNITemplate) #MNI choose resolution MNI = readMNI(res = &quot;2mm&quot;) #create a list using pattern matching ending in .nii #regular expression | indicates matching for ica.nii or mca_blur.nii mca.list&lt;-list.files(path=&quot;./Ext-Data/&quot;, pattern = &quot;*ica.nii|*mca_blur.nii&quot;, full.names = TRUE) We use the imgdatamat function to read in the files with patients as rows and imaging voxels in columns. #use imgdatamat to organise data as rows of patients and columns of voxels #simscale reduces the size of the voxel to quarter of its size m.list.dat&lt;-imgdatamat(mca.list, simscale=1/4) ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! ## Malformed NIfTI - not reading NIfTI extension, use at own risk! #check that the dimension is correct dim(m.list.dat$S) #42 902629 ## [1] 29 902629 fit1=msma(m.list.dat$S,comp=2) plot(fit1, v=&quot;score&quot;, axes = 1:2, plottype=&quot;scatter&quot;) midx = 1 ## the index for the modality vidx = 1 ## the index for the component Q = fit1$wbX[[midx]][,vidx] outstat_fit1 = rec(Q, m.list.dat$imagedim, mask=m.list.dat$brainpos) coat(MNI, outstat_fit1) 5.3 Independent component analysis Independent component analysis is different from PCA in that it seeks components which are statistically independent.It separates signal from a multivariate distribution into additive components which as statistically independent. It is used in separating components of noise signal or blind source localisation. library(fastICA) a &lt;- fastICA(img1$S, 2, alg.typ = &quot;deflation&quot;, fun = &quot;logcosh&quot;, alpha = 1, method = &quot;R&quot;, row.norm = FALSE, maxit = 200, tol = 0.0001, verbose = TRUE) ## Centering ## Whitening ## Deflation FastICA using logcosh approx. to neg-entropy function ## Component 1 ## Iteration 1 tol = 0.002084111 ## Iteration 2 tol = 4.374718e-05 ## Component 2 ## Iteration 1 tol = 0 5.4 Partial least squares There are several versions of partial least squares (PLS). A detailed mathematical exposition of the PLS-PLR technique used here can be found in the paper by Fort and Lambert-Lacroix (Fort and Lambert-Lacroix 2005). PLS is a multiple regression method that is suited to datasets comprising large sets of independent predictor variables (voxels in an image) and smaller sets of dependent variables (neurological outcome scores). Each voxel can take on a value of 1 (representing involvement by infarction) or 0 (representing absence of involvement) in the MR image of each patient. PLS employs a data reduction method which generates latent variables, linear combinations of independent and dependent variables which explain as much of their covariance as possible. Linear least squares regression of the latent variables produces coefﬁcients or beta weights for the latent variables at each voxel location in the brain in stereotaxic coordinate space.(Phan et al. 2010) The colon dataset containing microarray data comes with the plsgenomics library (Durif et al. 2018). The analysis involves partitioning the data into training and test set. The classification data is in the Y column. This example is provided by the plsgenomics library library(plsgenomics) ## For any news related to the &#39;plsgenomics&#39; package (update, corrected bugs), please check http://thoth.inrialpes.fr/people/gdurif/ ## C++ based sparse PLS routines will soon be available on the CRAN in the new &#39;fastPLS&#39; package. data(&quot;Colon&quot;) class(Colon) #list ## [1] &quot;list&quot; #62 samples 2000 genes #Outcome is in Y column as 1 and 2. 62 rows #2000 gene names dim(Colon$X) ## [1] 62 2000 #heatmap matrix.heatmap(cbind(Colon$X,Colon$y)) # IndexLearn &lt;- c(sample(which(Colon$Y==2),12),sample(which(Colon$Y==1),8)) Xtrain &lt;- Colon$X[IndexLearn,] Ytrain &lt;- Colon$Y[IndexLearn] Xtest &lt;- Colon$X[-IndexLearn,] # preprocess data resP &lt;- preprocess(Xtrain= Xtrain, Xtest=Xtest,Threshold = c(100,16000),Filtering=c(5,500), log10.scale=TRUE,row.stand=TRUE) # Determine optimum h and lambda hlam &lt;- gsim.cv(Xtrain=resP$pXtrain,Ytrain=Ytrain,hARange=c(7,20), LambdaRange=c(0.1,1),hB=NULL) # perform prediction by GSIM # lambda is the ridge regularization parameter from the cross validation res &lt;- gsim(Xtrain=resP$pXtrain, Ytrain= Ytrain,Xtest=resP$pXtest, Lambda=hlam$Lambda,hA=hlam$hA,hB=NULL) res$Cvg ## [1] 1 #difference between predicted and observed sum(res$Ytest!=Colon$Y[-IndexLearn]) ## [1] 6 References "],["machine-learning.html", "Chapter 6 Machine learning 6.1 Decision tree analysis 6.2 Ensemble tree methods 6.3 KNN 6.4 Support vector machine 6.5 Non-negative matrix factorisation 6.6 Formal concept analysis 6.7 Evolutionary Algorithm 6.8 Manifold learning 6.9 Deep learning", " Chapter 6 Machine learning A key aspect of machine learning is cross validation to evaluate the model. It repeatedly evaluate the model based on different subsets of the model and using different parameters to select the optimal parameters. The models are compared against subset of the data. The caret library is an excellent tool for performing model selection. 6.1 Decision tree analysis Decision tree method generates a logical flow diagram that resembles a tree. This triangulated diagram, with repeated partitioning of the original data into smaller groups (nodes) on a yes or no basis, resembles clinical reasoning. By way of contrast, regression methods generate significant predictors but it’s not clear how those predictors enter the sequential nature of clinical reasoning. Regression models assume that all of the variables are required at once to formulate an accurate prediction. This would make some of the elements of any model from regression analysis superfluous. There are several different approaches to performing decision tree analyses. The most famous method CART is implemented in R as rpart. The second approaches uses chi-square test to partition the tree, available from the party library. Decision tree may also reveal complex intreactions (relationship) among the predictors in a way that regression analyses do not easily reveal. 6.1.1 Information theory driven The tree is grown using a “divide and conquer” strategy, with repeated partitioning of the original data into smaller groups (nodes) on a yes or no basis. The method uses a splitting rule built around the notion of “purity.” A node in the tree is defined as pure when all the elements belong to one class. When there is impurity in the node, a split occurs to maximize reduction in “impurity.” In some cases, the split may be biased toward attributes that contain many different ordinal levels or scales. Thus, the selection of an attribute as the root node may vary according to the splitting rule and the scaling of the attribute. The decision tree package rpart does tolerate certain degree of missing number because the data are split using the available data for that attribute to calculate the Gini index (rather than the entire cohort). One major advantage of rpart is the presentation of the classification rules in the easily interpretable form of a tree. The hierarchical nature of the decision tree is similar to many decision processes (Phan et al. 2018). A criticism of decision tree is that it’s prone to overfitting and or preference for variable with many levels. Decision tree do not handle collinearity issues well. library(rpart) library(rattle) ## Rattle: A free graphical interface for data science with R. ## Version 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd. ## Type &#39;rattle()&#39; to shake, rattle, and roll your data. library(rpart.plot) data(&quot;Leukemia&quot;, package = &quot;Stat2Data&quot;) colnames(Leukemia) ## [1] &quot;Age&quot; &quot;Smear&quot; &quot;Infil&quot; &quot;Index&quot; &quot;Blasts&quot; &quot;Temp&quot; &quot;Resp&quot; &quot;Time&quot; &quot;Status&quot; #decision tree model for AML treatment treLeukemia&lt;-rpart(Status~., data=Leukemia) fancyRpartPlot(treLeukemia) 6.1.2 Conditional decision tree The conditional decision tree approach has been proposed to be superior to CART method because that method uses information criterion for partitioning and which can lead to overﬁtting.The scenario of overﬁtting describes model which works well on training data but less so with new data.The conditional approach by party is less prone to overﬁtting as it includes signiﬁcance testing (Phan, Kooblal, et al. 2019). library(party) data(&quot;aSAH&quot;, package = &quot;pROC&quot;) colnames(aSAH) ## [1] &quot;gos6&quot; &quot;outcome&quot; &quot;gender&quot; &quot;age&quot; &quot;wfns&quot; &quot;s100b&quot; &quot;ndka&quot; #decision tree model treeSAH&lt;-ctree(outcome~., data=aSAH, control = ctree_control(mincriterion=0.95, minsplit=50)) plot(treeSAH,type = &quot;simple&quot;,main = &quot;Conditional Inference for aSAH&quot;) 6.2 Ensemble tree methods 6.2.1 Bagging trees Both gradient boost machine and random forest are examples of tree-based method with the former based on boosting of the residuals of the model and the latter based on bagging with random selection (rows and columns) of multiple subsets of the data. As such random forest regression ensembles the model from multiple decision trees. The trees are created by obtaining multiple subset of the data without replacement (random selection of data by rows and columns). Decision tree comes at certain disadvantage such as overfitting. Random forest avoids the problems of single decision tree analyses by aggregating the results of multiple trees obtained by performing analysis on random subsets of the original data. This method is different from the bootstrapping procedure in which the data is subsetted with replacement. Theoretically, decision tree can look very similar as the data structure is not significantly changed. There is a theoretical risk of overfitting with random forest and underfitting with boosting tree methods. 6.2.1.1 Random Forest Random forest is available as randomForest or ranger or via caret. A major drawback to random forest is that the hierarchical nature of the trees is lost. As such this method is seen as a black box tool and is less commonly embraced in the medical literature. One way us to use an interpretable machine learning tool iml (Molnar, Bischl, and Casalicchio 2018) (Shapley values) tool to aid interpretation of the model. This method uses ideas from coalition game theory to fairly distribute the contribution of the coalition of covariates to the random forest model. The machine learning models are tuned using caret library. #https://topepo.github.io/caret/index.html library(caret) data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) #The Breast Cancer data contains NA as well as factors #note Class is benign or malignant of class factor #column Bare.nuclei removed due to NA BreastCancer&lt;-BreastCancer[,-c(1,7)] #split data using caTools. #The next example will use createDataPartition from caret set.seed(123) split = caTools::sample.split(BreastCancer$Class, SplitRatio = 0.75) Train = subset(BreastCancer, split == TRUE) Test = subset(BreastCancer, split == FALSE) # specify that resampling method is rf_control &lt;- trainControl(## 10-fold CV method = &quot;cv&quot;, number = 10) #scaling data is performed here under preProcess #note that ranger handles the outcome variable as factor rf &lt;- caret::train(Class ~ ., data = Train, method = &quot;ranger&quot;, trControl=rf_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10, verbose=F) summary(rf) ## Length Class Mode ## predictions 525 factor numeric ## num.trees 1 -none- numeric ## num.independent.variables 1 -none- numeric ## mtry 1 -none- numeric ## min.node.size 1 -none- numeric ## prediction.error 1 -none- numeric ## forest 9 ranger.forest list ## confusion.matrix 4 table numeric ## splitrule 1 -none- character ## num.random.splits 1 -none- numeric ## treetype 1 -none- character ## call 9 -none- call ## importance.mode 1 -none- character ## num.samples 1 -none- numeric ## replace 1 -none- logical ## xNames 71 -none- character ## problemType 1 -none- character ## tuneValue 3 data.frame list ## obsLevels 2 -none- character ## param 1 -none- list pred_rf&lt;-predict(rf,BreastCancer) confusionMatrix(pred_rf, BreastCancer$Class) ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 461 roc_rf&lt;-pROC::roc(BreastCancer$Class, as.numeric(pred_rf)) ## Setting levels: control = benign, case = malignant ## Setting direction: controls &lt; cases roc_rf ## ## Call: ## roc.default(response = BreastCancer$Class, predictor = as.numeric(pred_rf)) ## ## Data: as.numeric(pred_rf) in 458 controls (BreastCancer$Class benign) &lt; 241 cases (BreastCancer$Class malignant). ## Area under the curve: 0.9843 6.2.1.2 Random survival forest with rfsrc Random survival forest example is provided below using rfsrc library. The survex library is used for explanation on the model. This library is also available as a learner in the mlr3verse. library(survival) library(survminer) library(randomForestSRC) library(survex) library(dplyr) #data from survival package on NCCTG lung cancer trial #https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html data(cancer, package=&quot;survival&quot;) #time in days #status censored=1, dead=2 #sex:Male=1 Female=2 cancer2&lt;- cancer %&gt;% mutate( status=ifelse(status==1,0,1)) %&gt;% rename(Dead=status, Days=time) time=cancer2$Days status=cancer2$Dead RF&lt;- rfsrc(Surv(Days, Dead) ~ age+sex+ph.ecog+ph.karno+wt.loss, data = cancer2) #specify library to avoid confusion with dplyr explainer&lt;-survex::explain(RF) ## Preparation of a new explainer is initiated ## -&gt; model label : rfsrc ( [33m default [39m ) ## -&gt; data : 213 rows 5 cols ( extracted from the model ) ## -&gt; target variable : 213 values ( 151 events and 62 censored , censoring rate = 0.291 ) ( extracted from the model ) ## -&gt; times : 50 unique time points , min = 5 , mean = 304.1224 , median = 263.16 , max = 835.44 ## -&gt; times : ( generated from y as 50 time points being consecutive quantiles (0.00, 0.02, ..., 0.98) ) ## -&gt; predict function : sum over the predict_cumulative_hazard_function will be used ( [33m default [39m ) ## -&gt; predict survival function : stepfun based on predict.rfsrc()$survival will be used ( [33m default [39m ) ## -&gt; predict cumulative hazard function : stepfun based on predict.rfsrc()$chf will be used ( [33m default [39m ) ## -&gt; model_info : package randomForestSRC , ver. 3.2.2 , task survival ( [33m default [39m ) ## A new explainer has been created! Plot a single tree from the random survival forest model. plot(get.tree(RF,4)) Dynamic AUC y &lt;- explainer$y times &lt;- explainer$times surv &lt;- explainer$predict_survival_function(RF, explainer$data, times) cd_auc(y, surv = surv, times = times) ## [1] 1.0000000 0.9307568 0.9112200 0.9096154 0.8829532 0.8786292 0.8560335 0.8517304 ## [9] 0.8459069 0.8241969 0.8231190 0.8146465 0.7873396 0.7604641 0.7722174 0.7696099 ## [17] 0.7667343 0.7605675 0.7514324 0.7500935 0.7638787 0.7621149 0.7673878 0.7606034 ## [25] 0.7352058 0.7093474 0.7040276 0.7112689 0.7135705 0.7084011 0.7085640 0.7135241 ## [33] 0.7028266 0.6921722 0.6955375 0.7029153 0.6907407 0.6919927 0.7015251 0.7019995 ## [41] 0.7273598 0.7425582 0.7499178 0.7495446 0.7548334 0.7822465 0.8160264 0.8361538 ## [49] 0.8801743 0.9048077 Plot variable importance for random survival forest using permutation of features and measure impact on Brier score. ModelRF&lt;-survex::model_parts(explainer) plot(ModelRF) Plot partial dependence Model_PD&lt;-model_profile(explainer) ## Registered S3 method overwritten by &#39;ingredients&#39;: ## method from ## plot.feature_importance_explainer survex plot(Model_PD) 6.2.1.3 Random survival forest with ranger Random forest can be used for performing survival analysis using ranger, randomforestSRC. The example below is an example using the lung cancer trial. #data from survival package on NCCTG lung cancer trial #https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html data(cancer, package=&quot;survival&quot;) #time in days #status censored=1, dead=2 #sex:Male=1 Female=2 library(ranger) ## ## Attaching package: &#39;ranger&#39; ## The following object is masked from &#39;package:rattle&#39;: ## ## importance library(tidyverse) library(survival) cancer2&lt;-cancer %&gt;% dplyr::select(time, status, age,sex, ph.ecog) %&gt;% na.omit() survival_formula&lt;-formula(paste(&#39;Surv(&#39;, &#39;time&#39;, &#39;,&#39;, &#39;status&#39;, &#39;) ~ &#39;,&#39;age+sex+ph.ecog&#39;)) survival_forest &lt;- ranger(survival_formula, data = cancer2, seed = 1234, importance = &#39;permutation&#39;, mtry = 2, verbose = TRUE, num.trees = 200, write.forest=TRUE) print(&quot;error:&quot;); print(survival_forest$prediction.error) ## [1] &quot;error:&quot; ## [1] 0.391924 Print variable importance sort(survival_forest$variable.importance) ## age sex ph.ecog ## 0.008386083 0.030801816 0.065970251 Probability of survival plot(survival_forest$unique.death.times, survival_forest$survival[1,], type=&#39;l&#39;, col=&#39;orange&#39;, ylim=c(0.01,1)) lines(survival_forest$unique.death.times, survival_forest$survival[56,], col=&#39;blue&#39;) plot(survival_forest$unique.death.times, survival_forest$survival[1,], type=&#39;l&#39;, col=&#39;orange&#39;, ylim=c(0.01,1)) for (x in c(2:100)) { lines(survival_forest$unique.death.times, survival_forest$survival[x,], col=&#39;red&#39;) } 6.2.2 Boosting trees 6.2.2.1 Gradient Boost Machine Gradient boost machine is available as gradient boost machine_gbm. #the breast cancer data from random forest is used here # specify that the resampling method is gbm_control &lt;- trainControl(## 10-fold CV method = &quot;repeatedcv&quot;, number = 10) #scaling data is performed here under preProcess #note that ranger handles the outcome variable as factor gbm &lt;- caret::train(Class ~ ., data = Train, method = &quot;gbm&quot;, trControl=gbm_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10) ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1427 nan 0.1000 0.0725 ## 2 1.0265 nan 0.1000 0.0564 ## 3 0.9280 nan 0.1000 0.0462 ## 4 0.8512 nan 0.1000 0.0377 ## 5 0.7835 nan 0.1000 0.0319 ## 6 0.7269 nan 0.1000 0.0280 ## 7 0.6797 nan 0.1000 0.0211 ## 8 0.6261 nan 0.1000 0.0225 ## 9 0.5811 nan 0.1000 0.0192 ## 10 0.5462 nan 0.1000 0.0161 ## 20 0.3514 nan 0.1000 0.0047 ## 40 0.2348 nan 0.1000 0.0009 ## 60 0.1887 nan 0.1000 -0.0002 ## 80 0.1648 nan 0.1000 -0.0007 ## 100 0.1492 nan 0.1000 -0.0017 ## 120 0.1416 nan 0.1000 -0.0002 ## 140 0.1289 nan 0.1000 -0.0007 ## 160 0.1239 nan 0.1000 -0.0007 ## 180 0.1199 nan 0.1000 -0.0006 ## 200 0.1155 nan 0.1000 -0.0004 ## 220 0.1131 nan 0.1000 -0.0015 ## 240 0.1085 nan 0.1000 -0.0005 ## 260 0.1074 nan 0.1000 -0.0009 ## 280 0.1013 nan 0.1000 0.0001 ## 300 0.1001 nan 0.1000 -0.0007 ## 320 0.0970 nan 0.1000 -0.0008 ## 340 0.0940 nan 0.1000 -0.0004 ## 360 0.0940 nan 0.1000 -0.0012 ## 380 0.0926 nan 0.1000 -0.0005 ## 400 0.0902 nan 0.1000 -0.0008 ## 420 0.0885 nan 0.1000 -0.0012 ## 440 0.0871 nan 0.1000 -0.0004 ## 460 0.0859 nan 0.1000 -0.0010 ## 480 0.0848 nan 0.1000 -0.0008 ## 500 0.0839 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1404 nan 0.1000 0.0706 ## 2 1.0186 nan 0.1000 0.0602 ## 3 0.9203 nan 0.1000 0.0470 ## 4 0.8335 nan 0.1000 0.0422 ## 5 0.7527 nan 0.1000 0.0384 ## 6 0.6890 nan 0.1000 0.0303 ## 7 0.6323 nan 0.1000 0.0267 ## 8 0.5864 nan 0.1000 0.0216 ## 9 0.5459 nan 0.1000 0.0175 ## 10 0.5093 nan 0.1000 0.0178 ## 20 0.2971 nan 0.1000 0.0053 ## 40 0.1873 nan 0.1000 0.0010 ## 60 0.1372 nan 0.1000 -0.0002 ## 80 0.1117 nan 0.1000 -0.0006 ## 100 0.0929 nan 0.1000 -0.0014 ## 120 0.0825 nan 0.1000 -0.0004 ## 140 0.0723 nan 0.1000 -0.0007 ## 160 0.0657 nan 0.1000 -0.0009 ## 180 0.0592 nan 0.1000 -0.0009 ## 200 0.0534 nan 0.1000 -0.0004 ## 220 0.0501 nan 0.1000 -0.0006 ## 240 0.0432 nan 0.1000 -0.0005 ## 260 0.0409 nan 0.1000 -0.0005 ## 280 0.0358 nan 0.1000 -0.0004 ## 300 0.0338 nan 0.1000 -0.0001 ## 320 0.0310 nan 0.1000 -0.0004 ## 340 0.0285 nan 0.1000 -0.0001 ## 360 0.0256 nan 0.1000 -0.0000 ## 380 0.0230 nan 0.1000 -0.0002 ## 400 0.0207 nan 0.1000 -0.0001 ## 420 0.0185 nan 0.1000 -0.0002 ## 440 0.0173 nan 0.1000 -0.0002 ## 460 0.0153 nan 0.1000 -0.0002 ## 480 0.0139 nan 0.1000 -0.0002 ## 500 0.0126 nan 0.1000 -0.0002 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1317 nan 0.1000 0.0753 ## 2 1.0143 nan 0.1000 0.0530 ## 3 0.9039 nan 0.1000 0.0534 ## 4 0.8172 nan 0.1000 0.0429 ## 5 0.7454 nan 0.1000 0.0339 ## 6 0.6799 nan 0.1000 0.0335 ## 7 0.6212 nan 0.1000 0.0261 ## 8 0.5719 nan 0.1000 0.0244 ## 9 0.5298 nan 0.1000 0.0205 ## 10 0.4910 nan 0.1000 0.0179 ## 20 0.2803 nan 0.1000 0.0041 ## 40 0.1490 nan 0.1000 -0.0011 ## 60 0.1038 nan 0.1000 -0.0008 ## 80 0.0793 nan 0.1000 -0.0004 ## 100 0.0626 nan 0.1000 -0.0004 ## 120 0.0511 nan 0.1000 -0.0009 ## 140 0.0421 nan 0.1000 -0.0004 ## 160 0.0342 nan 0.1000 -0.0006 ## 180 0.0305 nan 0.1000 -0.0004 ## 200 0.0256 nan 0.1000 -0.0002 ## 220 0.0210 nan 0.1000 -0.0000 ## 240 0.0173 nan 0.1000 -0.0001 ## 260 0.0150 nan 0.1000 -0.0001 ## 280 0.0122 nan 0.1000 -0.0001 ## 300 0.0108 nan 0.1000 -0.0002 ## 320 0.0090 nan 0.1000 -0.0000 ## 340 0.0083 nan 0.1000 -0.0001 ## 360 0.0073 nan 0.1000 -0.0001 ## 380 0.0060 nan 0.1000 -0.0001 ## 400 0.0051 nan 0.1000 -0.0001 ## 420 0.0044 nan 0.1000 -0.0000 ## 440 0.0038 nan 0.1000 -0.0000 ## 460 0.0033 nan 0.1000 -0.0000 ## 480 0.0029 nan 0.1000 -0.0000 ## 500 0.0024 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1353 nan 0.1000 0.0720 ## 2 1.0104 nan 0.1000 0.0585 ## 3 0.9082 nan 0.1000 0.0494 ## 4 0.8160 nan 0.1000 0.0445 ## 5 0.7366 nan 0.1000 0.0358 ## 6 0.6760 nan 0.1000 0.0278 ## 7 0.6191 nan 0.1000 0.0264 ## 8 0.5679 nan 0.1000 0.0237 ## 9 0.5251 nan 0.1000 0.0188 ## 10 0.4870 nan 0.1000 0.0190 ## 20 0.2760 nan 0.1000 0.0037 ## 40 0.1335 nan 0.1000 -0.0004 ## 60 0.0908 nan 0.1000 -0.0011 ## 80 0.0640 nan 0.1000 -0.0009 ## 100 0.0491 nan 0.1000 -0.0003 ## 120 0.0382 nan 0.1000 -0.0003 ## 140 0.0283 nan 0.1000 -0.0004 ## 160 0.0213 nan 0.1000 -0.0002 ## 180 0.0172 nan 0.1000 0.0000 ## 200 0.0139 nan 0.1000 -0.0001 ## 220 0.0114 nan 0.1000 -0.0002 ## 240 0.0089 nan 0.1000 -0.0000 ## 260 0.0071 nan 0.1000 -0.0001 ## 280 0.0060 nan 0.1000 -0.0001 ## 300 0.0048 nan 0.1000 -0.0001 ## 320 0.0037 nan 0.1000 -0.0000 ## 340 0.0029 nan 0.1000 -0.0000 ## 360 0.0024 nan 0.1000 -0.0000 ## 380 0.0020 nan 0.1000 -0.0001 ## 400 0.0017 nan 0.1000 -0.0000 ## 420 0.0014 nan 0.1000 -0.0000 ## 440 0.0011 nan 0.1000 -0.0000 ## 460 0.0009 nan 0.1000 -0.0000 ## 480 0.0008 nan 0.1000 -0.0000 ## 500 0.0006 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1263 nan 0.1000 0.0779 ## 2 1.0050 nan 0.1000 0.0553 ## 3 0.9025 nan 0.1000 0.0489 ## 4 0.8142 nan 0.1000 0.0412 ## 5 0.7407 nan 0.1000 0.0350 ## 6 0.6754 nan 0.1000 0.0294 ## 7 0.6170 nan 0.1000 0.0271 ## 8 0.5667 nan 0.1000 0.0230 ## 9 0.5237 nan 0.1000 0.0188 ## 10 0.4848 nan 0.1000 0.0183 ## 20 0.2694 nan 0.1000 0.0050 ## 40 0.1239 nan 0.1000 -0.0009 ## 60 0.0745 nan 0.1000 -0.0007 ## 80 0.0505 nan 0.1000 -0.0004 ## 100 0.0386 nan 0.1000 -0.0006 ## 120 0.0289 nan 0.1000 -0.0006 ## 140 0.0219 nan 0.1000 -0.0000 ## 160 0.0175 nan 0.1000 -0.0004 ## 180 0.0122 nan 0.1000 -0.0001 ## 200 0.0090 nan 0.1000 -0.0001 ## 220 0.0070 nan 0.1000 0.0000 ## 240 0.0057 nan 0.1000 -0.0001 ## 260 0.0043 nan 0.1000 -0.0001 ## 280 0.0036 nan 0.1000 -0.0001 ## 300 0.0025 nan 0.1000 -0.0000 ## 320 0.0021 nan 0.1000 -0.0000 ## 340 0.0019 nan 0.1000 -0.0001 ## 360 0.0015 nan 0.1000 -0.0001 ## 380 0.0011 nan 0.1000 -0.0000 ## 400 0.0010 nan 0.1000 -0.0000 ## 420 0.0008 nan 0.1000 -0.0000 ## 440 0.0006 nan 0.1000 0.0000 ## 460 0.0005 nan 0.1000 -0.0000 ## 480 0.0005 nan 0.1000 -0.0000 ## 500 0.0004 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1278 nan 0.1000 0.0764 ## 2 1.0050 nan 0.1000 0.0633 ## 3 0.9025 nan 0.1000 0.0509 ## 4 0.8140 nan 0.1000 0.0421 ## 5 0.7402 nan 0.1000 0.0339 ## 6 0.6744 nan 0.1000 0.0312 ## 7 0.6183 nan 0.1000 0.0251 ## 8 0.5694 nan 0.1000 0.0216 ## 9 0.5268 nan 0.1000 0.0180 ## 10 0.4886 nan 0.1000 0.0174 ## 20 0.2618 nan 0.1000 0.0054 ## 40 0.1126 nan 0.1000 -0.0006 ## 60 0.0650 nan 0.1000 -0.0005 ## 80 0.0418 nan 0.1000 -0.0002 ## 100 0.0305 nan 0.1000 -0.0006 ## 120 0.0197 nan 0.1000 -0.0004 ## 140 0.0148 nan 0.1000 -0.0002 ## 160 0.0112 nan 0.1000 -0.0001 ## 180 0.0080 nan 0.1000 -0.0001 ## 200 0.0057 nan 0.1000 -0.0001 ## 220 0.0047 nan 0.1000 -0.0001 ## 240 0.0035 nan 0.1000 -0.0001 ## 260 0.0024 nan 0.1000 -0.0000 ## 280 0.0018 nan 0.1000 0.0000 ## 300 0.0013 nan 0.1000 -0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1250 nan 0.1000 0.0753 ## 2 0.9960 nan 0.1000 0.0631 ## 3 0.8924 nan 0.1000 0.0494 ## 4 0.8091 nan 0.1000 0.0411 ## 5 0.7351 nan 0.1000 0.0342 ## 6 0.6707 nan 0.1000 0.0309 ## 7 0.6129 nan 0.1000 0.0269 ## 8 0.5629 nan 0.1000 0.0229 ## 9 0.5179 nan 0.1000 0.0207 ## 10 0.4805 nan 0.1000 0.0161 ## 20 0.2540 nan 0.1000 0.0044 ## 40 0.1119 nan 0.1000 -0.0003 ## 60 0.0606 nan 0.1000 -0.0013 ## 80 0.0402 nan 0.1000 -0.0008 ## 100 0.0262 nan 0.1000 -0.0003 ## 120 0.0165 nan 0.1000 -0.0003 ## 140 0.0115 nan 0.1000 -0.0002 ## 160 0.0074 nan 0.1000 -0.0001 ## 180 0.0054 nan 0.1000 -0.0001 ## 200 0.0041 nan 0.1000 -0.0001 ## 220 0.0033 nan 0.1000 -0.0001 ## 240 0.0026 nan 0.1000 -0.0000 ## 260 0.0017 nan 0.1000 -0.0000 ## 280 0.0012 nan 0.1000 0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1322 nan 0.1000 0.0773 ## 2 1.0083 nan 0.1000 0.0644 ## 3 0.9031 nan 0.1000 0.0521 ## 4 0.8129 nan 0.1000 0.0427 ## 5 0.7366 nan 0.1000 0.0359 ## 6 0.6681 nan 0.1000 0.0318 ## 7 0.6107 nan 0.1000 0.0254 ## 8 0.5587 nan 0.1000 0.0240 ## 9 0.5183 nan 0.1000 0.0177 ## 10 0.4820 nan 0.1000 0.0155 ## 20 0.2625 nan 0.1000 0.0036 ## 40 0.1160 nan 0.1000 0.0000 ## 60 0.0647 nan 0.1000 -0.0010 ## 80 0.0391 nan 0.1000 -0.0010 ## 100 0.0254 nan 0.1000 -0.0005 ## 120 0.0179 nan 0.1000 -0.0003 ## 140 0.0115 nan 0.1000 -0.0002 ## 160 0.0085 nan 0.1000 0.0000 ## 180 0.0068 nan 0.1000 -0.0002 ## 200 0.0043 nan 0.1000 -0.0001 ## 220 0.0036 nan 0.1000 -0.0002 ## 240 0.0023 nan 0.1000 -0.0000 ## 260 0.0017 nan 0.1000 -0.0000 ## 280 0.0015 nan 0.1000 -0.0000 ## 300 0.0013 nan 0.1000 -0.0001 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1300 nan 0.1000 0.0741 ## 2 1.0023 nan 0.1000 0.0610 ## 3 0.8978 nan 0.1000 0.0495 ## 4 0.8107 nan 0.1000 0.0434 ## 5 0.7360 nan 0.1000 0.0348 ## 6 0.6670 nan 0.1000 0.0315 ## 7 0.6116 nan 0.1000 0.0256 ## 8 0.5640 nan 0.1000 0.0211 ## 9 0.5222 nan 0.1000 0.0192 ## 10 0.4862 nan 0.1000 0.0161 ## 20 0.2622 nan 0.1000 0.0043 ## 40 0.1118 nan 0.1000 -0.0002 ## 60 0.0655 nan 0.1000 -0.0012 ## 80 0.0416 nan 0.1000 -0.0009 ## 100 0.0246 nan 0.1000 -0.0006 ## 120 0.0170 nan 0.1000 -0.0005 ## 140 0.0114 nan 0.1000 -0.0001 ## 160 0.0089 nan 0.1000 -0.0003 ## 180 0.0063 nan 0.1000 -0.0003 ## 200 0.0053 nan 0.1000 -0.0002 ## 220 0.0035 nan 0.1000 -0.0000 ## 240 0.0029 nan 0.1000 -0.0001 ## 260 0.0025 nan 0.1000 -0.0001 ## 280 0.0021 nan 0.1000 -0.0001 ## 300 0.0015 nan 0.1000 -0.0001 ## 320 0.0013 nan 0.1000 -0.0001 ## 340 0.0009 nan 0.1000 -0.0000 ## 360 0.0009 nan 0.1000 -0.0000 ## 380 0.0007 nan 0.1000 -0.0000 ## 400 0.0005 nan 0.1000 -0.0000 ## 420 0.0004 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1290 nan 0.1000 0.0814 ## 2 1.0029 nan 0.1000 0.0630 ## 3 0.8975 nan 0.1000 0.0507 ## 4 0.8085 nan 0.1000 0.0428 ## 5 0.7330 nan 0.1000 0.0374 ## 6 0.6735 nan 0.1000 0.0277 ## 7 0.6214 nan 0.1000 0.0244 ## 8 0.5721 nan 0.1000 0.0227 ## 9 0.5293 nan 0.1000 0.0185 ## 10 0.4898 nan 0.1000 0.0172 ## 20 0.2701 nan 0.1000 0.0041 ## 40 0.1231 nan 0.1000 0.0001 ## 60 0.0643 nan 0.1000 -0.0006 ## 80 0.0365 nan 0.1000 -0.0010 ## 100 0.0260 nan 0.1000 -0.0003 ## 120 0.0180 nan 0.1000 -0.0001 ## 140 0.0123 nan 0.1000 -0.0001 ## 160 0.0090 nan 0.1000 -0.0002 ## 180 0.0074 nan 0.1000 -0.0003 ## 200 0.0065 nan 0.1000 -0.0001 ## 220 0.0041 nan 0.1000 -0.0001 ## 240 0.0038 nan 0.1000 -0.0001 ## 260 0.0027 nan 0.1000 -0.0000 ## 280 0.0024 nan 0.1000 -0.0001 ## 300 0.0019 nan 0.1000 -0.0001 ## 320 0.0016 nan 0.1000 -0.0000 ## 340 0.0011 nan 0.1000 -0.0000 ## 360 0.0009 nan 0.1000 -0.0000 ## 380 0.0008 nan 0.1000 0.0000 ## 400 0.0007 nan 0.1000 -0.0000 ## 420 0.0005 nan 0.1000 -0.0000 ## 440 0.0005 nan 0.1000 0.0000 ## 460 0.0004 nan 0.1000 -0.0000 ## 480 0.0003 nan 0.1000 -0.0000 ## 500 0.0002 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1580 nan 0.1000 0.0611 ## 2 1.0377 nan 0.1000 0.0574 ## 3 0.9452 nan 0.1000 0.0466 ## 4 0.8664 nan 0.1000 0.0386 ## 5 0.7939 nan 0.1000 0.0362 ## 6 0.7283 nan 0.1000 0.0299 ## 7 0.6776 nan 0.1000 0.0254 ## 8 0.6344 nan 0.1000 0.0205 ## 9 0.5950 nan 0.1000 0.0191 ## 10 0.5567 nan 0.1000 0.0189 ## 20 0.3469 nan 0.1000 0.0059 ## 40 0.2207 nan 0.1000 0.0011 ## 60 0.1758 nan 0.1000 0.0000 ## 80 0.1545 nan 0.1000 -0.0008 ## 100 0.1393 nan 0.1000 -0.0005 ## 120 0.1251 nan 0.1000 -0.0004 ## 140 0.1143 nan 0.1000 -0.0004 ## 160 0.1082 nan 0.1000 -0.0011 ## 180 0.1027 nan 0.1000 -0.0004 ## 200 0.0993 nan 0.1000 -0.0007 ## 220 0.0929 nan 0.1000 -0.0006 ## 240 0.0911 nan 0.1000 -0.0007 ## 260 0.0881 nan 0.1000 -0.0003 ## 280 0.0848 nan 0.1000 -0.0003 ## 300 0.0825 nan 0.1000 -0.0007 ## 320 0.0813 nan 0.1000 -0.0007 ## 340 0.0818 nan 0.1000 -0.0008 ## 360 0.0789 nan 0.1000 -0.0006 ## 380 0.0762 nan 0.1000 -0.0004 ## 400 0.0743 nan 0.1000 -0.0004 ## 420 0.0715 nan 0.1000 -0.0005 ## 440 0.0707 nan 0.1000 -0.0003 ## 460 0.0699 nan 0.1000 -0.0007 ## 480 0.0671 nan 0.1000 -0.0007 ## 500 0.0663 nan 0.1000 -0.0011 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1434 nan 0.1000 0.0754 ## 2 1.0214 nan 0.1000 0.0553 ## 3 0.9171 nan 0.1000 0.0528 ## 4 0.8311 nan 0.1000 0.0430 ## 5 0.7546 nan 0.1000 0.0369 ## 6 0.6928 nan 0.1000 0.0298 ## 7 0.6388 nan 0.1000 0.0265 ## 8 0.5956 nan 0.1000 0.0201 ## 9 0.5531 nan 0.1000 0.0191 ## 10 0.5121 nan 0.1000 0.0177 ## 20 0.2979 nan 0.1000 0.0071 ## 40 0.1715 nan 0.1000 -0.0003 ## 60 0.1296 nan 0.1000 -0.0004 ## 80 0.1005 nan 0.1000 -0.0008 ## 100 0.0831 nan 0.1000 -0.0011 ## 120 0.0710 nan 0.1000 -0.0010 ## 140 0.0625 nan 0.1000 -0.0002 ## 160 0.0525 nan 0.1000 -0.0001 ## 180 0.0466 nan 0.1000 -0.0004 ## 200 0.0416 nan 0.1000 -0.0003 ## 220 0.0377 nan 0.1000 -0.0001 ## 240 0.0334 nan 0.1000 -0.0005 ## 260 0.0292 nan 0.1000 -0.0000 ## 280 0.0264 nan 0.1000 -0.0003 ## 300 0.0244 nan 0.1000 -0.0003 ## 320 0.0217 nan 0.1000 -0.0003 ## 340 0.0192 nan 0.1000 -0.0001 ## 360 0.0178 nan 0.1000 -0.0001 ## 380 0.0163 nan 0.1000 -0.0001 ## 400 0.0150 nan 0.1000 -0.0001 ## 420 0.0136 nan 0.1000 -0.0001 ## 440 0.0127 nan 0.1000 -0.0001 ## 460 0.0114 nan 0.1000 -0.0001 ## 480 0.0104 nan 0.1000 -0.0000 ## 500 0.0094 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1385 nan 0.1000 0.0672 ## 2 1.0083 nan 0.1000 0.0636 ## 3 0.9009 nan 0.1000 0.0501 ## 4 0.8127 nan 0.1000 0.0400 ## 5 0.7386 nan 0.1000 0.0361 ## 6 0.6771 nan 0.1000 0.0278 ## 7 0.6206 nan 0.1000 0.0270 ## 8 0.5651 nan 0.1000 0.0260 ## 9 0.5235 nan 0.1000 0.0191 ## 10 0.4884 nan 0.1000 0.0157 ## 20 0.2743 nan 0.1000 0.0061 ## 40 0.1431 nan 0.1000 -0.0017 ## 60 0.0981 nan 0.1000 -0.0001 ## 80 0.0772 nan 0.1000 -0.0006 ## 100 0.0582 nan 0.1000 -0.0006 ## 120 0.0462 nan 0.1000 -0.0005 ## 140 0.0366 nan 0.1000 -0.0005 ## 160 0.0302 nan 0.1000 -0.0003 ## 180 0.0251 nan 0.1000 -0.0004 ## 200 0.0211 nan 0.1000 -0.0001 ## 220 0.0179 nan 0.1000 -0.0001 ## 240 0.0158 nan 0.1000 -0.0002 ## 260 0.0125 nan 0.1000 -0.0001 ## 280 0.0104 nan 0.1000 -0.0001 ## 300 0.0087 nan 0.1000 -0.0002 ## 320 0.0076 nan 0.1000 -0.0000 ## 340 0.0062 nan 0.1000 -0.0000 ## 360 0.0051 nan 0.1000 0.0000 ## 380 0.0044 nan 0.1000 -0.0000 ## 400 0.0037 nan 0.1000 -0.0000 ## 420 0.0031 nan 0.1000 -0.0000 ## 440 0.0027 nan 0.1000 -0.0000 ## 460 0.0023 nan 0.1000 -0.0000 ## 480 0.0019 nan 0.1000 -0.0000 ## 500 0.0016 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1259 nan 0.1000 0.0798 ## 2 1.0046 nan 0.1000 0.0586 ## 3 0.9048 nan 0.1000 0.0443 ## 4 0.8163 nan 0.1000 0.0404 ## 5 0.7430 nan 0.1000 0.0351 ## 6 0.6765 nan 0.1000 0.0327 ## 7 0.6222 nan 0.1000 0.0226 ## 8 0.5747 nan 0.1000 0.0222 ## 9 0.5300 nan 0.1000 0.0202 ## 10 0.4897 nan 0.1000 0.0192 ## 20 0.2680 nan 0.1000 0.0036 ## 40 0.1345 nan 0.1000 0.0003 ## 60 0.0871 nan 0.1000 -0.0004 ## 80 0.0633 nan 0.1000 -0.0002 ## 100 0.0474 nan 0.1000 -0.0003 ## 120 0.0342 nan 0.1000 -0.0001 ## 140 0.0270 nan 0.1000 -0.0003 ## 160 0.0210 nan 0.1000 -0.0000 ## 180 0.0152 nan 0.1000 -0.0001 ## 200 0.0122 nan 0.1000 -0.0002 ## 220 0.0101 nan 0.1000 -0.0001 ## 240 0.0075 nan 0.1000 -0.0001 ## 260 0.0058 nan 0.1000 -0.0000 ## 280 0.0047 nan 0.1000 -0.0001 ## 300 0.0038 nan 0.1000 -0.0000 ## 320 0.0030 nan 0.1000 -0.0000 ## 340 0.0025 nan 0.1000 -0.0000 ## 360 0.0020 nan 0.1000 -0.0001 ## 380 0.0016 nan 0.1000 -0.0000 ## 400 0.0013 nan 0.1000 -0.0000 ## 420 0.0010 nan 0.1000 -0.0000 ## 440 0.0009 nan 0.1000 -0.0000 ## 460 0.0006 nan 0.1000 -0.0000 ## 480 0.0005 nan 0.1000 -0.0000 ## 500 0.0004 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1225 nan 0.1000 0.0872 ## 2 0.9954 nan 0.1000 0.0610 ## 3 0.8951 nan 0.1000 0.0487 ## 4 0.8114 nan 0.1000 0.0412 ## 5 0.7393 nan 0.1000 0.0336 ## 6 0.6786 nan 0.1000 0.0281 ## 7 0.6211 nan 0.1000 0.0256 ## 8 0.5740 nan 0.1000 0.0202 ## 9 0.5319 nan 0.1000 0.0186 ## 10 0.4951 nan 0.1000 0.0153 ## 20 0.2632 nan 0.1000 0.0040 ## 40 0.1170 nan 0.1000 -0.0004 ## 60 0.0701 nan 0.1000 -0.0001 ## 80 0.0441 nan 0.1000 -0.0002 ## 100 0.0303 nan 0.1000 0.0001 ## 120 0.0217 nan 0.1000 -0.0002 ## 140 0.0148 nan 0.1000 -0.0002 ## 160 0.0104 nan 0.1000 -0.0001 ## 180 0.0076 nan 0.1000 -0.0000 ## 200 0.0059 nan 0.1000 -0.0001 ## 220 0.0044 nan 0.1000 -0.0000 ## 240 0.0034 nan 0.1000 -0.0001 ## 260 0.0024 nan 0.1000 -0.0000 ## 280 0.0019 nan 0.1000 -0.0000 ## 300 0.0014 nan 0.1000 -0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1352 nan 0.1000 0.0697 ## 2 1.0150 nan 0.1000 0.0587 ## 3 0.9080 nan 0.1000 0.0514 ## 4 0.8184 nan 0.1000 0.0400 ## 5 0.7429 nan 0.1000 0.0372 ## 6 0.6776 nan 0.1000 0.0303 ## 7 0.6209 nan 0.1000 0.0270 ## 8 0.5715 nan 0.1000 0.0232 ## 9 0.5285 nan 0.1000 0.0201 ## 10 0.4902 nan 0.1000 0.0161 ## 20 0.2575 nan 0.1000 0.0046 ## 40 0.1136 nan 0.1000 0.0002 ## 60 0.0683 nan 0.1000 -0.0020 ## 80 0.0425 nan 0.1000 -0.0002 ## 100 0.0296 nan 0.1000 -0.0003 ## 120 0.0201 nan 0.1000 -0.0002 ## 140 0.0134 nan 0.1000 -0.0001 ## 160 0.0097 nan 0.1000 -0.0001 ## 180 0.0067 nan 0.1000 -0.0000 ## 200 0.0045 nan 0.1000 -0.0000 ## 220 0.0036 nan 0.1000 -0.0001 ## 240 0.0024 nan 0.1000 0.0000 ## 260 0.0019 nan 0.1000 -0.0000 ## 280 0.0015 nan 0.1000 -0.0000 ## 300 0.0012 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0006 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1333 nan 0.1000 0.0754 ## 2 1.0020 nan 0.1000 0.0638 ## 3 0.8969 nan 0.1000 0.0513 ## 4 0.8124 nan 0.1000 0.0398 ## 5 0.7417 nan 0.1000 0.0335 ## 6 0.6767 nan 0.1000 0.0309 ## 7 0.6196 nan 0.1000 0.0253 ## 8 0.5642 nan 0.1000 0.0256 ## 9 0.5184 nan 0.1000 0.0203 ## 10 0.4820 nan 0.1000 0.0157 ## 20 0.2573 nan 0.1000 0.0052 ## 40 0.1094 nan 0.1000 -0.0014 ## 60 0.0573 nan 0.1000 -0.0003 ## 80 0.0345 nan 0.1000 -0.0004 ## 100 0.0229 nan 0.1000 -0.0001 ## 120 0.0170 nan 0.1000 -0.0004 ## 140 0.0124 nan 0.1000 -0.0003 ## 160 0.0085 nan 0.1000 -0.0002 ## 180 0.0064 nan 0.1000 -0.0001 ## 200 0.0045 nan 0.1000 0.0000 ## 220 0.0032 nan 0.1000 -0.0000 ## 240 0.0024 nan 0.1000 -0.0001 ## 260 0.0017 nan 0.1000 -0.0000 ## 280 0.0016 nan 0.1000 -0.0000 ## 300 0.0013 nan 0.1000 -0.0000 ## 320 0.0012 nan 0.1000 -0.0001 ## 340 0.0010 nan 0.1000 -0.0000 ## 360 0.0011 nan 0.1000 -0.0000 ## 380 0.0007 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1353 nan 0.1000 0.0748 ## 2 1.0077 nan 0.1000 0.0615 ## 3 0.9042 nan 0.1000 0.0505 ## 4 0.8200 nan 0.1000 0.0388 ## 5 0.7463 nan 0.1000 0.0333 ## 6 0.6782 nan 0.1000 0.0318 ## 7 0.6199 nan 0.1000 0.0287 ## 8 0.5685 nan 0.1000 0.0246 ## 9 0.5259 nan 0.1000 0.0161 ## 10 0.4856 nan 0.1000 0.0194 ## 20 0.2593 nan 0.1000 0.0047 ## 40 0.1079 nan 0.1000 -0.0001 ## 60 0.0608 nan 0.1000 -0.0012 ## 80 0.0391 nan 0.1000 -0.0007 ## 100 0.0248 nan 0.1000 -0.0003 ## 120 0.0191 nan 0.1000 -0.0004 ## 140 0.0126 nan 0.1000 -0.0002 ## 160 0.0088 nan 0.1000 -0.0003 ## 180 0.0059 nan 0.1000 -0.0001 ## 200 0.0045 nan 0.1000 -0.0001 ## 220 0.0036 nan 0.1000 -0.0001 ## 240 0.0029 nan 0.1000 -0.0001 ## 260 0.0022 nan 0.1000 -0.0000 ## 280 0.0017 nan 0.1000 -0.0001 ## 300 0.0011 nan 0.1000 -0.0000 ## 320 0.0009 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1282 nan 0.1000 0.0818 ## 2 1.0091 nan 0.1000 0.0571 ## 3 0.9103 nan 0.1000 0.0479 ## 4 0.8243 nan 0.1000 0.0384 ## 5 0.7461 nan 0.1000 0.0363 ## 6 0.6807 nan 0.1000 0.0299 ## 7 0.6239 nan 0.1000 0.0259 ## 8 0.5742 nan 0.1000 0.0234 ## 9 0.5260 nan 0.1000 0.0208 ## 10 0.4871 nan 0.1000 0.0186 ## 20 0.2600 nan 0.1000 0.0038 ## 40 0.0998 nan 0.1000 -0.0002 ## 60 0.0490 nan 0.1000 -0.0004 ## 80 0.0296 nan 0.1000 0.0001 ## 100 0.0181 nan 0.1000 -0.0002 ## 120 0.0153 nan 0.1000 -0.0006 ## 140 0.0113 nan 0.1000 -0.0004 ## 160 0.0079 nan 0.1000 -0.0000 ## 180 0.0054 nan 0.1000 -0.0001 ## 200 0.0038 nan 0.1000 -0.0000 ## 220 0.0025 nan 0.1000 -0.0001 ## 240 0.0018 nan 0.1000 -0.0000 ## 260 0.0014 nan 0.1000 -0.0000 ## 280 0.0009 nan 0.1000 -0.0000 ## 300 0.0007 nan 0.1000 -0.0000 ## 320 0.0005 nan 0.1000 -0.0000 ## 340 0.0004 nan 0.1000 -0.0000 ## 360 0.0003 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1378 nan 0.1000 0.0711 ## 2 1.0160 nan 0.1000 0.0605 ## 3 0.9141 nan 0.1000 0.0498 ## 4 0.8268 nan 0.1000 0.0398 ## 5 0.7493 nan 0.1000 0.0364 ## 6 0.6872 nan 0.1000 0.0280 ## 7 0.6286 nan 0.1000 0.0271 ## 8 0.5763 nan 0.1000 0.0238 ## 9 0.5324 nan 0.1000 0.0200 ## 10 0.4900 nan 0.1000 0.0182 ## 20 0.2624 nan 0.1000 0.0053 ## 40 0.1128 nan 0.1000 0.0000 ## 60 0.0578 nan 0.1000 -0.0006 ## 80 0.0331 nan 0.1000 -0.0006 ## 100 0.0202 nan 0.1000 -0.0005 ## 120 0.0143 nan 0.1000 -0.0001 ## 140 0.0100 nan 0.1000 -0.0001 ## 160 0.0062 nan 0.1000 0.0001 ## 180 0.0045 nan 0.1000 -0.0001 ## 200 0.0035 nan 0.1000 -0.0001 ## 220 0.0025 nan 0.1000 -0.0001 ## 240 0.0020 nan 0.1000 -0.0001 ## 260 0.0015 nan 0.1000 -0.0000 ## 280 0.0014 nan 0.1000 -0.0000 ## 300 0.0011 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1544 nan 0.1000 0.0682 ## 2 1.0388 nan 0.1000 0.0544 ## 3 0.9470 nan 0.1000 0.0440 ## 4 0.8735 nan 0.1000 0.0374 ## 5 0.8105 nan 0.1000 0.0315 ## 6 0.7587 nan 0.1000 0.0281 ## 7 0.7108 nan 0.1000 0.0228 ## 8 0.6644 nan 0.1000 0.0221 ## 9 0.6236 nan 0.1000 0.0181 ## 10 0.5893 nan 0.1000 0.0171 ## 20 0.3826 nan 0.1000 0.0061 ## 40 0.2487 nan 0.1000 0.0011 ## 60 0.2049 nan 0.1000 0.0010 ## 80 0.1810 nan 0.1000 -0.0012 ## 100 0.1627 nan 0.1000 0.0000 ## 120 0.1530 nan 0.1000 -0.0021 ## 140 0.1433 nan 0.1000 -0.0006 ## 160 0.1397 nan 0.1000 -0.0015 ## 180 0.1362 nan 0.1000 -0.0007 ## 200 0.1298 nan 0.1000 -0.0013 ## 220 0.1243 nan 0.1000 -0.0010 ## 240 0.1199 nan 0.1000 -0.0006 ## 260 0.1141 nan 0.1000 -0.0010 ## 280 0.1109 nan 0.1000 -0.0007 ## 300 0.1094 nan 0.1000 -0.0001 ## 320 0.1066 nan 0.1000 -0.0003 ## 340 0.1072 nan 0.1000 -0.0005 ## 360 0.1057 nan 0.1000 -0.0011 ## 380 0.1037 nan 0.1000 -0.0003 ## 400 0.1014 nan 0.1000 -0.0005 ## 420 0.1004 nan 0.1000 -0.0007 ## 440 0.0992 nan 0.1000 -0.0009 ## 460 0.0987 nan 0.1000 -0.0004 ## 480 0.0966 nan 0.1000 -0.0011 ## 500 0.0943 nan 0.1000 -0.0013 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1419 nan 0.1000 0.0710 ## 2 1.0233 nan 0.1000 0.0548 ## 3 0.9248 nan 0.1000 0.0468 ## 4 0.8439 nan 0.1000 0.0393 ## 5 0.7700 nan 0.1000 0.0353 ## 6 0.7088 nan 0.1000 0.0275 ## 7 0.6580 nan 0.1000 0.0266 ## 8 0.6117 nan 0.1000 0.0243 ## 9 0.5686 nan 0.1000 0.0199 ## 10 0.5331 nan 0.1000 0.0132 ## 20 0.3141 nan 0.1000 0.0064 ## 40 0.1937 nan 0.1000 -0.0005 ## 60 0.1530 nan 0.1000 -0.0006 ## 80 0.1272 nan 0.1000 -0.0001 ## 100 0.1072 nan 0.1000 -0.0019 ## 120 0.0929 nan 0.1000 -0.0010 ## 140 0.0799 nan 0.1000 -0.0008 ## 160 0.0705 nan 0.1000 -0.0004 ## 180 0.0632 nan 0.1000 -0.0003 ## 200 0.0557 nan 0.1000 -0.0004 ## 220 0.0515 nan 0.1000 -0.0002 ## 240 0.0455 nan 0.1000 -0.0006 ## 260 0.0409 nan 0.1000 -0.0001 ## 280 0.0368 nan 0.1000 -0.0001 ## 300 0.0343 nan 0.1000 -0.0002 ## 320 0.0313 nan 0.1000 -0.0003 ## 340 0.0285 nan 0.1000 -0.0001 ## 360 0.0258 nan 0.1000 -0.0001 ## 380 0.0242 nan 0.1000 -0.0002 ## 400 0.0222 nan 0.1000 -0.0003 ## 420 0.0200 nan 0.1000 -0.0000 ## 440 0.0190 nan 0.1000 -0.0002 ## 460 0.0178 nan 0.1000 -0.0002 ## 480 0.0167 nan 0.1000 -0.0002 ## 500 0.0152 nan 0.1000 -0.0002 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1342 nan 0.1000 0.0749 ## 2 1.0089 nan 0.1000 0.0582 ## 3 0.9088 nan 0.1000 0.0485 ## 4 0.8249 nan 0.1000 0.0406 ## 5 0.7519 nan 0.1000 0.0339 ## 6 0.6923 nan 0.1000 0.0285 ## 7 0.6389 nan 0.1000 0.0258 ## 8 0.5940 nan 0.1000 0.0212 ## 9 0.5503 nan 0.1000 0.0176 ## 10 0.5102 nan 0.1000 0.0185 ## 20 0.2955 nan 0.1000 0.0061 ## 40 0.1627 nan 0.1000 -0.0002 ## 60 0.1176 nan 0.1000 -0.0003 ## 80 0.0902 nan 0.1000 -0.0010 ## 100 0.0721 nan 0.1000 -0.0008 ## 120 0.0605 nan 0.1000 -0.0008 ## 140 0.0504 nan 0.1000 -0.0007 ## 160 0.0398 nan 0.1000 0.0000 ## 180 0.0349 nan 0.1000 -0.0006 ## 200 0.0288 nan 0.1000 -0.0003 ## 220 0.0233 nan 0.1000 -0.0001 ## 240 0.0198 nan 0.1000 -0.0003 ## 260 0.0168 nan 0.1000 -0.0002 ## 280 0.0146 nan 0.1000 -0.0001 ## 300 0.0126 nan 0.1000 -0.0002 ## 320 0.0107 nan 0.1000 -0.0002 ## 340 0.0092 nan 0.1000 -0.0001 ## 360 0.0078 nan 0.1000 -0.0000 ## 380 0.0068 nan 0.1000 -0.0000 ## 400 0.0060 nan 0.1000 -0.0001 ## 420 0.0053 nan 0.1000 -0.0001 ## 440 0.0045 nan 0.1000 -0.0000 ## 460 0.0039 nan 0.1000 -0.0000 ## 480 0.0034 nan 0.1000 -0.0001 ## 500 0.0030 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1329 nan 0.1000 0.0733 ## 2 1.0106 nan 0.1000 0.0586 ## 3 0.9059 nan 0.1000 0.0503 ## 4 0.8209 nan 0.1000 0.0407 ## 5 0.7498 nan 0.1000 0.0339 ## 6 0.6882 nan 0.1000 0.0279 ## 7 0.6348 nan 0.1000 0.0260 ## 8 0.5856 nan 0.1000 0.0221 ## 9 0.5414 nan 0.1000 0.0200 ## 10 0.5078 nan 0.1000 0.0152 ## 20 0.2789 nan 0.1000 0.0040 ## 40 0.1368 nan 0.1000 -0.0005 ## 60 0.0946 nan 0.1000 -0.0014 ## 80 0.0653 nan 0.1000 -0.0009 ## 100 0.0509 nan 0.1000 -0.0003 ## 120 0.0385 nan 0.1000 -0.0002 ## 140 0.0307 nan 0.1000 -0.0003 ## 160 0.0245 nan 0.1000 -0.0003 ## 180 0.0200 nan 0.1000 -0.0001 ## 200 0.0161 nan 0.1000 -0.0001 ## 220 0.0125 nan 0.1000 -0.0001 ## 240 0.0100 nan 0.1000 -0.0001 ## 260 0.0079 nan 0.1000 -0.0001 ## 280 0.0064 nan 0.1000 -0.0001 ## 300 0.0050 nan 0.1000 -0.0000 ## 320 0.0041 nan 0.1000 -0.0000 ## 340 0.0034 nan 0.1000 -0.0000 ## 360 0.0028 nan 0.1000 -0.0000 ## 380 0.0024 nan 0.1000 -0.0000 ## 400 0.0021 nan 0.1000 -0.0000 ## 420 0.0017 nan 0.1000 -0.0000 ## 440 0.0014 nan 0.1000 -0.0000 ## 460 0.0011 nan 0.1000 -0.0000 ## 480 0.0009 nan 0.1000 -0.0000 ## 500 0.0007 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1282 nan 0.1000 0.0785 ## 2 1.0040 nan 0.1000 0.0604 ## 3 0.9023 nan 0.1000 0.0488 ## 4 0.8131 nan 0.1000 0.0447 ## 5 0.7425 nan 0.1000 0.0350 ## 6 0.6794 nan 0.1000 0.0287 ## 7 0.6257 nan 0.1000 0.0246 ## 8 0.5811 nan 0.1000 0.0208 ## 9 0.5420 nan 0.1000 0.0169 ## 10 0.5046 nan 0.1000 0.0160 ## 20 0.2855 nan 0.1000 0.0052 ## 40 0.1359 nan 0.1000 -0.0013 ## 60 0.0868 nan 0.1000 -0.0000 ## 80 0.0590 nan 0.1000 -0.0010 ## 100 0.0423 nan 0.1000 -0.0006 ## 120 0.0281 nan 0.1000 -0.0001 ## 140 0.0219 nan 0.1000 -0.0003 ## 160 0.0165 nan 0.1000 -0.0001 ## 180 0.0120 nan 0.1000 0.0000 ## 200 0.0092 nan 0.1000 -0.0001 ## 220 0.0072 nan 0.1000 -0.0002 ## 240 0.0050 nan 0.1000 -0.0000 ## 260 0.0040 nan 0.1000 -0.0001 ## 280 0.0032 nan 0.1000 -0.0000 ## 300 0.0024 nan 0.1000 -0.0000 ## 320 0.0019 nan 0.1000 -0.0000 ## 340 0.0015 nan 0.1000 -0.0000 ## 360 0.0014 nan 0.1000 -0.0000 ## 380 0.0012 nan 0.1000 -0.0000 ## 400 0.0009 nan 0.1000 -0.0000 ## 420 0.0007 nan 0.1000 -0.0000 ## 440 0.0006 nan 0.1000 -0.0000 ## 460 0.0005 nan 0.1000 -0.0000 ## 480 0.0004 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1425 nan 0.1000 0.0755 ## 2 1.0146 nan 0.1000 0.0609 ## 3 0.9154 nan 0.1000 0.0461 ## 4 0.8268 nan 0.1000 0.0431 ## 5 0.7495 nan 0.1000 0.0374 ## 6 0.6868 nan 0.1000 0.0272 ## 7 0.6357 nan 0.1000 0.0221 ## 8 0.5841 nan 0.1000 0.0209 ## 9 0.5411 nan 0.1000 0.0195 ## 10 0.5054 nan 0.1000 0.0147 ## 20 0.2709 nan 0.1000 0.0052 ## 40 0.1237 nan 0.1000 -0.0012 ## 60 0.0691 nan 0.1000 -0.0006 ## 80 0.0462 nan 0.1000 -0.0001 ## 100 0.0317 nan 0.1000 -0.0003 ## 120 0.0236 nan 0.1000 -0.0001 ## 140 0.0165 nan 0.1000 -0.0003 ## 160 0.0120 nan 0.1000 -0.0003 ## 180 0.0092 nan 0.1000 -0.0002 ## 200 0.0068 nan 0.1000 -0.0000 ## 220 0.0051 nan 0.1000 -0.0000 ## 240 0.0037 nan 0.1000 -0.0001 ## 260 0.0030 nan 0.1000 -0.0001 ## 280 0.0024 nan 0.1000 -0.0000 ## 300 0.0017 nan 0.1000 -0.0000 ## 320 0.0013 nan 0.1000 -0.0000 ## 340 0.0010 nan 0.1000 0.0000 ## 360 0.0007 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1346 nan 0.1000 0.0747 ## 2 1.0183 nan 0.1000 0.0585 ## 3 0.9138 nan 0.1000 0.0477 ## 4 0.8298 nan 0.1000 0.0376 ## 5 0.7601 nan 0.1000 0.0335 ## 6 0.6965 nan 0.1000 0.0301 ## 7 0.6399 nan 0.1000 0.0243 ## 8 0.5890 nan 0.1000 0.0238 ## 9 0.5482 nan 0.1000 0.0182 ## 10 0.5074 nan 0.1000 0.0182 ## 20 0.2710 nan 0.1000 0.0057 ## 40 0.1190 nan 0.1000 0.0002 ## 60 0.0709 nan 0.1000 -0.0002 ## 80 0.0473 nan 0.1000 -0.0002 ## 100 0.0306 nan 0.1000 -0.0003 ## 120 0.0222 nan 0.1000 0.0001 ## 140 0.0152 nan 0.1000 -0.0002 ## 160 0.0117 nan 0.1000 -0.0002 ## 180 0.0083 nan 0.1000 0.0000 ## 200 0.0062 nan 0.1000 -0.0001 ## 220 0.0049 nan 0.1000 -0.0001 ## 240 0.0034 nan 0.1000 -0.0001 ## 260 0.0026 nan 0.1000 -0.0000 ## 280 0.0019 nan 0.1000 -0.0000 ## 300 0.0014 nan 0.1000 0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1354 nan 0.1000 0.0728 ## 2 1.0156 nan 0.1000 0.0574 ## 3 0.9111 nan 0.1000 0.0484 ## 4 0.8198 nan 0.1000 0.0425 ## 5 0.7457 nan 0.1000 0.0354 ## 6 0.6865 nan 0.1000 0.0255 ## 7 0.6311 nan 0.1000 0.0270 ## 8 0.5809 nan 0.1000 0.0221 ## 9 0.5401 nan 0.1000 0.0154 ## 10 0.4998 nan 0.1000 0.0193 ## 20 0.2713 nan 0.1000 0.0043 ## 40 0.1208 nan 0.1000 0.0003 ## 60 0.0650 nan 0.1000 -0.0009 ## 80 0.0417 nan 0.1000 -0.0002 ## 100 0.0268 nan 0.1000 -0.0004 ## 120 0.0211 nan 0.1000 -0.0004 ## 140 0.0145 nan 0.1000 -0.0003 ## 160 0.0095 nan 0.1000 -0.0002 ## 180 0.0071 nan 0.1000 -0.0001 ## 200 0.0055 nan 0.1000 -0.0001 ## 220 0.0041 nan 0.1000 -0.0001 ## 240 0.0027 nan 0.1000 -0.0000 ## 260 0.0020 nan 0.1000 -0.0000 ## 280 0.0015 nan 0.1000 -0.0000 ## 300 0.0011 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0006 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1347 nan 0.1000 0.0758 ## 2 1.0060 nan 0.1000 0.0639 ## 3 0.9023 nan 0.1000 0.0482 ## 4 0.8221 nan 0.1000 0.0367 ## 5 0.7454 nan 0.1000 0.0378 ## 6 0.6834 nan 0.1000 0.0272 ## 7 0.6238 nan 0.1000 0.0266 ## 8 0.5761 nan 0.1000 0.0214 ## 9 0.5363 nan 0.1000 0.0170 ## 10 0.5024 nan 0.1000 0.0129 ## 20 0.2659 nan 0.1000 0.0049 ## 40 0.1100 nan 0.1000 0.0003 ## 60 0.0624 nan 0.1000 -0.0010 ## 80 0.0347 nan 0.1000 -0.0005 ## 100 0.0236 nan 0.1000 -0.0005 ## 120 0.0167 nan 0.1000 0.0001 ## 140 0.0122 nan 0.1000 -0.0003 ## 160 0.0084 nan 0.1000 -0.0001 ## 180 0.0059 nan 0.1000 -0.0001 ## 200 0.0045 nan 0.1000 -0.0000 ## 220 0.0037 nan 0.1000 -0.0000 ## 240 0.0026 nan 0.1000 -0.0001 ## 260 0.0022 nan 0.1000 -0.0001 ## 280 0.0020 nan 0.1000 -0.0001 ## 300 0.0015 nan 0.1000 -0.0001 ## 320 0.0009 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1324 nan 0.1000 0.0723 ## 2 1.0078 nan 0.1000 0.0613 ## 3 0.9095 nan 0.1000 0.0440 ## 4 0.8226 nan 0.1000 0.0427 ## 5 0.7499 nan 0.1000 0.0324 ## 6 0.6845 nan 0.1000 0.0277 ## 7 0.6241 nan 0.1000 0.0298 ## 8 0.5777 nan 0.1000 0.0205 ## 9 0.5360 nan 0.1000 0.0189 ## 10 0.4970 nan 0.1000 0.0166 ## 20 0.2639 nan 0.1000 0.0036 ## 40 0.1150 nan 0.1000 -0.0013 ## 60 0.0633 nan 0.1000 -0.0010 ## 80 0.0371 nan 0.1000 -0.0003 ## 100 0.0236 nan 0.1000 -0.0001 ## 120 0.0170 nan 0.1000 -0.0003 ## 140 0.0116 nan 0.1000 0.0001 ## 160 0.0087 nan 0.1000 -0.0002 ## 180 0.0054 nan 0.1000 -0.0001 ## 200 0.0037 nan 0.1000 0.0000 ## 220 0.0030 nan 0.1000 -0.0001 ## 240 0.0022 nan 0.1000 -0.0001 ## 260 0.0014 nan 0.1000 -0.0000 ## 280 0.0015 nan 0.1000 -0.0001 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0003 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1528 nan 0.1000 0.0645 ## 2 1.0397 nan 0.1000 0.0563 ## 3 0.9505 nan 0.1000 0.0427 ## 4 0.8719 nan 0.1000 0.0396 ## 5 0.8112 nan 0.1000 0.0304 ## 6 0.7577 nan 0.1000 0.0246 ## 7 0.7050 nan 0.1000 0.0255 ## 8 0.6613 nan 0.1000 0.0203 ## 9 0.6254 nan 0.1000 0.0157 ## 10 0.5899 nan 0.1000 0.0153 ## 20 0.3914 nan 0.1000 0.0058 ## 40 0.2505 nan 0.1000 0.0012 ## 60 0.2096 nan 0.1000 -0.0007 ## 80 0.1829 nan 0.1000 -0.0005 ## 100 0.1648 nan 0.1000 -0.0010 ## 120 0.1545 nan 0.1000 -0.0011 ## 140 0.1443 nan 0.1000 -0.0002 ## 160 0.1367 nan 0.1000 -0.0004 ## 180 0.1315 nan 0.1000 -0.0009 ## 200 0.1264 nan 0.1000 -0.0011 ## 220 0.1225 nan 0.1000 -0.0014 ## 240 0.1187 nan 0.1000 -0.0018 ## 260 0.1158 nan 0.1000 -0.0011 ## 280 0.1133 nan 0.1000 -0.0009 ## 300 0.1102 nan 0.1000 -0.0008 ## 320 0.1087 nan 0.1000 -0.0003 ## 340 0.1061 nan 0.1000 -0.0012 ## 360 0.1044 nan 0.1000 -0.0004 ## 380 0.1020 nan 0.1000 -0.0013 ## 400 0.1004 nan 0.1000 -0.0006 ## 420 0.0992 nan 0.1000 -0.0008 ## 440 0.0977 nan 0.1000 -0.0014 ## 460 0.0975 nan 0.1000 -0.0009 ## 480 0.0962 nan 0.1000 -0.0006 ## 500 0.0943 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1424 nan 0.1000 0.0727 ## 2 1.0433 nan 0.1000 0.0449 ## 3 0.9408 nan 0.1000 0.0497 ## 4 0.8590 nan 0.1000 0.0416 ## 5 0.7901 nan 0.1000 0.0344 ## 6 0.7248 nan 0.1000 0.0323 ## 7 0.6707 nan 0.1000 0.0249 ## 8 0.6248 nan 0.1000 0.0221 ## 9 0.5798 nan 0.1000 0.0215 ## 10 0.5412 nan 0.1000 0.0191 ## 20 0.3270 nan 0.1000 0.0062 ## 40 0.1909 nan 0.1000 0.0001 ## 60 0.1503 nan 0.1000 -0.0004 ## 80 0.1235 nan 0.1000 -0.0012 ## 100 0.1058 nan 0.1000 -0.0002 ## 120 0.0891 nan 0.1000 -0.0008 ## 140 0.0774 nan 0.1000 -0.0005 ## 160 0.0681 nan 0.1000 -0.0002 ## 180 0.0597 nan 0.1000 -0.0002 ## 200 0.0541 nan 0.1000 -0.0005 ## 220 0.0502 nan 0.1000 -0.0007 ## 240 0.0450 nan 0.1000 -0.0002 ## 260 0.0420 nan 0.1000 -0.0001 ## 280 0.0378 nan 0.1000 -0.0005 ## 300 0.0352 nan 0.1000 -0.0002 ## 320 0.0309 nan 0.1000 -0.0000 ## 340 0.0286 nan 0.1000 -0.0002 ## 360 0.0253 nan 0.1000 0.0000 ## 380 0.0237 nan 0.1000 -0.0002 ## 400 0.0213 nan 0.1000 0.0000 ## 420 0.0197 nan 0.1000 -0.0002 ## 440 0.0178 nan 0.1000 -0.0002 ## 460 0.0160 nan 0.1000 -0.0001 ## 480 0.0151 nan 0.1000 0.0000 ## 500 0.0140 nan 0.1000 -0.0001 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1362 nan 0.1000 0.0738 ## 2 1.0147 nan 0.1000 0.0574 ## 3 0.9190 nan 0.1000 0.0476 ## 4 0.8340 nan 0.1000 0.0405 ## 5 0.7610 nan 0.1000 0.0344 ## 6 0.7002 nan 0.1000 0.0272 ## 7 0.6420 nan 0.1000 0.0269 ## 8 0.5938 nan 0.1000 0.0223 ## 9 0.5528 nan 0.1000 0.0177 ## 10 0.5133 nan 0.1000 0.0171 ## 20 0.2956 nan 0.1000 0.0037 ## 40 0.1720 nan 0.1000 -0.0010 ## 60 0.1188 nan 0.1000 -0.0007 ## 80 0.0933 nan 0.1000 -0.0016 ## 100 0.0736 nan 0.1000 -0.0008 ## 120 0.0582 nan 0.1000 -0.0004 ## 140 0.0482 nan 0.1000 -0.0007 ## 160 0.0394 nan 0.1000 -0.0000 ## 180 0.0329 nan 0.1000 -0.0003 ## 200 0.0266 nan 0.1000 -0.0001 ## 220 0.0226 nan 0.1000 -0.0002 ## 240 0.0197 nan 0.1000 -0.0001 ## 260 0.0169 nan 0.1000 -0.0002 ## 280 0.0147 nan 0.1000 -0.0002 ## 300 0.0132 nan 0.1000 -0.0001 ## 320 0.0111 nan 0.1000 -0.0000 ## 340 0.0099 nan 0.1000 -0.0001 ## 360 0.0083 nan 0.1000 -0.0001 ## 380 0.0071 nan 0.1000 -0.0000 ## 400 0.0062 nan 0.1000 -0.0001 ## 420 0.0054 nan 0.1000 -0.0001 ## 440 0.0045 nan 0.1000 -0.0000 ## 460 0.0038 nan 0.1000 -0.0000 ## 480 0.0032 nan 0.1000 -0.0001 ## 500 0.0028 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1345 nan 0.1000 0.0697 ## 2 1.0138 nan 0.1000 0.0605 ## 3 0.9126 nan 0.1000 0.0455 ## 4 0.8249 nan 0.1000 0.0412 ## 5 0.7514 nan 0.1000 0.0338 ## 6 0.6883 nan 0.1000 0.0240 ## 7 0.6337 nan 0.1000 0.0259 ## 8 0.5856 nan 0.1000 0.0212 ## 9 0.5438 nan 0.1000 0.0184 ## 10 0.5100 nan 0.1000 0.0132 ## 20 0.2926 nan 0.1000 0.0050 ## 40 0.1482 nan 0.1000 -0.0013 ## 60 0.1000 nan 0.1000 -0.0007 ## 80 0.0728 nan 0.1000 -0.0007 ## 100 0.0568 nan 0.1000 -0.0007 ## 120 0.0432 nan 0.1000 -0.0004 ## 140 0.0365 nan 0.1000 -0.0006 ## 160 0.0272 nan 0.1000 -0.0000 ## 180 0.0222 nan 0.1000 -0.0003 ## 200 0.0177 nan 0.1000 -0.0001 ## 220 0.0141 nan 0.1000 -0.0000 ## 240 0.0116 nan 0.1000 -0.0000 ## 260 0.0094 nan 0.1000 -0.0000 ## 280 0.0076 nan 0.1000 -0.0001 ## 300 0.0061 nan 0.1000 -0.0001 ## 320 0.0049 nan 0.1000 -0.0001 ## 340 0.0042 nan 0.1000 -0.0000 ## 360 0.0034 nan 0.1000 -0.0001 ## 380 0.0028 nan 0.1000 -0.0000 ## 400 0.0023 nan 0.1000 -0.0000 ## 420 0.0019 nan 0.1000 -0.0000 ## 440 0.0016 nan 0.1000 -0.0000 ## 460 0.0013 nan 0.1000 -0.0000 ## 480 0.0011 nan 0.1000 -0.0000 ## 500 0.0009 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1398 nan 0.1000 0.0696 ## 2 1.0158 nan 0.1000 0.0613 ## 3 0.9136 nan 0.1000 0.0465 ## 4 0.8248 nan 0.1000 0.0426 ## 5 0.7481 nan 0.1000 0.0343 ## 6 0.6840 nan 0.1000 0.0285 ## 7 0.6295 nan 0.1000 0.0231 ## 8 0.5773 nan 0.1000 0.0235 ## 9 0.5304 nan 0.1000 0.0226 ## 10 0.4939 nan 0.1000 0.0151 ## 20 0.2782 nan 0.1000 0.0044 ## 40 0.1285 nan 0.1000 -0.0005 ## 60 0.0774 nan 0.1000 -0.0003 ## 80 0.0538 nan 0.1000 -0.0013 ## 100 0.0422 nan 0.1000 -0.0006 ## 120 0.0301 nan 0.1000 -0.0004 ## 140 0.0229 nan 0.1000 0.0001 ## 160 0.0173 nan 0.1000 -0.0001 ## 180 0.0126 nan 0.1000 -0.0001 ## 200 0.0096 nan 0.1000 -0.0000 ## 220 0.0074 nan 0.1000 -0.0000 ## 240 0.0059 nan 0.1000 0.0000 ## 260 0.0045 nan 0.1000 -0.0001 ## 280 0.0036 nan 0.1000 -0.0000 ## 300 0.0030 nan 0.1000 -0.0001 ## 320 0.0023 nan 0.1000 -0.0000 ## 340 0.0018 nan 0.1000 0.0000 ## 360 0.0015 nan 0.1000 0.0000 ## 380 0.0012 nan 0.1000 -0.0000 ## 400 0.0009 nan 0.1000 -0.0000 ## 420 0.0008 nan 0.1000 -0.0000 ## 440 0.0006 nan 0.1000 -0.0000 ## 460 0.0005 nan 0.1000 -0.0000 ## 480 0.0004 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1339 nan 0.1000 0.0721 ## 2 1.0110 nan 0.1000 0.0580 ## 3 0.9158 nan 0.1000 0.0418 ## 4 0.8311 nan 0.1000 0.0349 ## 5 0.7590 nan 0.1000 0.0326 ## 6 0.6977 nan 0.1000 0.0267 ## 7 0.6444 nan 0.1000 0.0264 ## 8 0.5978 nan 0.1000 0.0196 ## 9 0.5541 nan 0.1000 0.0183 ## 10 0.5189 nan 0.1000 0.0144 ## 20 0.2903 nan 0.1000 0.0036 ## 40 0.1335 nan 0.1000 -0.0002 ## 60 0.0805 nan 0.1000 -0.0010 ## 80 0.0526 nan 0.1000 -0.0007 ## 100 0.0356 nan 0.1000 -0.0005 ## 120 0.0244 nan 0.1000 -0.0002 ## 140 0.0175 nan 0.1000 -0.0001 ## 160 0.0129 nan 0.1000 -0.0003 ## 180 0.0096 nan 0.1000 -0.0002 ## 200 0.0074 nan 0.1000 -0.0002 ## 220 0.0056 nan 0.1000 -0.0000 ## 240 0.0041 nan 0.1000 -0.0000 ## 260 0.0031 nan 0.1000 -0.0000 ## 280 0.0024 nan 0.1000 -0.0000 ## 300 0.0017 nan 0.1000 -0.0000 ## 320 0.0013 nan 0.1000 -0.0000 ## 340 0.0010 nan 0.1000 -0.0000 ## 360 0.0007 nan 0.1000 -0.0000 ## 380 0.0006 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1377 nan 0.1000 0.0754 ## 2 1.0152 nan 0.1000 0.0602 ## 3 0.9181 nan 0.1000 0.0465 ## 4 0.8325 nan 0.1000 0.0389 ## 5 0.7566 nan 0.1000 0.0346 ## 6 0.6902 nan 0.1000 0.0305 ## 7 0.6386 nan 0.1000 0.0237 ## 8 0.5919 nan 0.1000 0.0204 ## 9 0.5451 nan 0.1000 0.0199 ## 10 0.5085 nan 0.1000 0.0164 ## 20 0.2726 nan 0.1000 0.0055 ## 40 0.1179 nan 0.1000 -0.0011 ## 60 0.0651 nan 0.1000 -0.0005 ## 80 0.0406 nan 0.1000 -0.0003 ## 100 0.0275 nan 0.1000 -0.0005 ## 120 0.0194 nan 0.1000 -0.0002 ## 140 0.0131 nan 0.1000 -0.0003 ## 160 0.0099 nan 0.1000 -0.0002 ## 180 0.0071 nan 0.1000 -0.0001 ## 200 0.0053 nan 0.1000 -0.0001 ## 220 0.0042 nan 0.1000 0.0000 ## 240 0.0032 nan 0.1000 -0.0001 ## 260 0.0025 nan 0.1000 -0.0001 ## 280 0.0018 nan 0.1000 -0.0000 ## 300 0.0015 nan 0.1000 -0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0007 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1401 nan 0.1000 0.0692 ## 2 1.0161 nan 0.1000 0.0609 ## 3 0.9169 nan 0.1000 0.0460 ## 4 0.8297 nan 0.1000 0.0396 ## 5 0.7569 nan 0.1000 0.0352 ## 6 0.6929 nan 0.1000 0.0286 ## 7 0.6395 nan 0.1000 0.0237 ## 8 0.5921 nan 0.1000 0.0214 ## 9 0.5477 nan 0.1000 0.0204 ## 10 0.5064 nan 0.1000 0.0186 ## 20 0.2742 nan 0.1000 0.0049 ## 40 0.1151 nan 0.1000 -0.0000 ## 60 0.0641 nan 0.1000 -0.0011 ## 80 0.0370 nan 0.1000 -0.0005 ## 100 0.0250 nan 0.1000 -0.0001 ## 120 0.0165 nan 0.1000 -0.0003 ## 140 0.0108 nan 0.1000 -0.0000 ## 160 0.0082 nan 0.1000 -0.0001 ## 180 0.0062 nan 0.1000 -0.0001 ## 200 0.0039 nan 0.1000 -0.0000 ## 220 0.0029 nan 0.1000 -0.0001 ## 240 0.0020 nan 0.1000 -0.0000 ## 260 0.0017 nan 0.1000 0.0000 ## 280 0.0013 nan 0.1000 -0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0006 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1392 nan 0.1000 0.0726 ## 2 1.0200 nan 0.1000 0.0547 ## 3 0.9143 nan 0.1000 0.0520 ## 4 0.8277 nan 0.1000 0.0405 ## 5 0.7552 nan 0.1000 0.0329 ## 6 0.6930 nan 0.1000 0.0286 ## 7 0.6350 nan 0.1000 0.0276 ## 8 0.5902 nan 0.1000 0.0201 ## 9 0.5456 nan 0.1000 0.0177 ## 10 0.5091 nan 0.1000 0.0157 ## 20 0.2824 nan 0.1000 0.0042 ## 40 0.1189 nan 0.1000 -0.0005 ## 60 0.0606 nan 0.1000 -0.0001 ## 80 0.0384 nan 0.1000 -0.0000 ## 100 0.0264 nan 0.1000 -0.0005 ## 120 0.0177 nan 0.1000 -0.0002 ## 140 0.0122 nan 0.1000 -0.0002 ## 160 0.0081 nan 0.1000 -0.0000 ## 180 0.0061 nan 0.1000 -0.0001 ## 200 0.0038 nan 0.1000 -0.0001 ## 220 0.0028 nan 0.1000 -0.0001 ## 240 0.0021 nan 0.1000 -0.0001 ## 260 0.0017 nan 0.1000 -0.0000 ## 280 0.0012 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1318 nan 0.1000 0.0715 ## 2 1.0075 nan 0.1000 0.0554 ## 3 0.9060 nan 0.1000 0.0482 ## 4 0.8234 nan 0.1000 0.0360 ## 5 0.7562 nan 0.1000 0.0275 ## 6 0.6914 nan 0.1000 0.0302 ## 7 0.6367 nan 0.1000 0.0232 ## 8 0.5860 nan 0.1000 0.0209 ## 9 0.5426 nan 0.1000 0.0198 ## 10 0.5054 nan 0.1000 0.0156 ## 20 0.2762 nan 0.1000 0.0061 ## 40 0.1178 nan 0.1000 -0.0002 ## 60 0.0710 nan 0.1000 -0.0010 ## 80 0.0451 nan 0.1000 0.0003 ## 100 0.0287 nan 0.1000 -0.0005 ## 120 0.0180 nan 0.1000 -0.0003 ## 140 0.0118 nan 0.1000 -0.0003 ## 160 0.0080 nan 0.1000 0.0000 ## 180 0.0048 nan 0.1000 -0.0002 ## 200 0.0035 nan 0.1000 -0.0001 ## 220 0.0025 nan 0.1000 -0.0000 ## 240 0.0018 nan 0.1000 -0.0000 ## 260 0.0014 nan 0.1000 -0.0000 ## 280 0.0011 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 -0.0000 ## 340 0.0004 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 0.0000 ## 480 0.0000 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1475 nan 0.1000 0.0706 ## 2 1.0392 nan 0.1000 0.0534 ## 3 0.9468 nan 0.1000 0.0420 ## 4 0.8658 nan 0.1000 0.0372 ## 5 0.8018 nan 0.1000 0.0291 ## 6 0.7504 nan 0.1000 0.0262 ## 7 0.6970 nan 0.1000 0.0249 ## 8 0.6489 nan 0.1000 0.0220 ## 9 0.6042 nan 0.1000 0.0201 ## 10 0.5682 nan 0.1000 0.0152 ## 20 0.3703 nan 0.1000 0.0052 ## 40 0.2381 nan 0.1000 -0.0001 ## 60 0.1871 nan 0.1000 -0.0010 ## 80 0.1644 nan 0.1000 -0.0002 ## 100 0.1491 nan 0.1000 0.0001 ## 120 0.1434 nan 0.1000 -0.0013 ## 140 0.1311 nan 0.1000 -0.0021 ## 160 0.1252 nan 0.1000 -0.0009 ## 180 0.1202 nan 0.1000 -0.0006 ## 200 0.1145 nan 0.1000 -0.0006 ## 220 0.1091 nan 0.1000 -0.0007 ## 240 0.1070 nan 0.1000 -0.0008 ## 260 0.1058 nan 0.1000 0.0001 ## 280 0.1026 nan 0.1000 -0.0004 ## 300 0.1008 nan 0.1000 -0.0006 ## 320 0.1003 nan 0.1000 -0.0006 ## 340 0.0965 nan 0.1000 -0.0008 ## 360 0.0942 nan 0.1000 -0.0011 ## 380 0.0943 nan 0.1000 -0.0007 ## 400 0.0911 nan 0.1000 -0.0008 ## 420 0.0897 nan 0.1000 -0.0005 ## 440 0.0891 nan 0.1000 -0.0008 ## 460 0.0880 nan 0.1000 -0.0004 ## 480 0.0853 nan 0.1000 -0.0000 ## 500 0.0841 nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1376 nan 0.1000 0.0744 ## 2 1.0190 nan 0.1000 0.0594 ## 3 0.9172 nan 0.1000 0.0507 ## 4 0.8337 nan 0.1000 0.0410 ## 5 0.7591 nan 0.1000 0.0344 ## 6 0.6982 nan 0.1000 0.0287 ## 7 0.6435 nan 0.1000 0.0264 ## 8 0.5949 nan 0.1000 0.0236 ## 9 0.5542 nan 0.1000 0.0179 ## 10 0.5145 nan 0.1000 0.0194 ## 20 0.2957 nan 0.1000 0.0053 ## 40 0.1843 nan 0.1000 -0.0002 ## 60 0.1345 nan 0.1000 -0.0012 ## 80 0.1073 nan 0.1000 -0.0006 ## 100 0.0906 nan 0.1000 -0.0004 ## 120 0.0811 nan 0.1000 -0.0011 ## 140 0.0674 nan 0.1000 -0.0007 ## 160 0.0598 nan 0.1000 -0.0004 ## 180 0.0531 nan 0.1000 -0.0005 ## 200 0.0471 nan 0.1000 -0.0003 ## 220 0.0427 nan 0.1000 -0.0007 ## 240 0.0383 nan 0.1000 -0.0003 ## 260 0.0335 nan 0.1000 -0.0001 ## 280 0.0307 nan 0.1000 -0.0003 ## 300 0.0285 nan 0.1000 -0.0002 ## 320 0.0247 nan 0.1000 -0.0004 ## 340 0.0220 nan 0.1000 -0.0001 ## 360 0.0202 nan 0.1000 -0.0002 ## 380 0.0184 nan 0.1000 -0.0002 ## 400 0.0166 nan 0.1000 -0.0002 ## 420 0.0155 nan 0.1000 -0.0001 ## 440 0.0144 nan 0.1000 -0.0002 ## 460 0.0132 nan 0.1000 -0.0000 ## 480 0.0118 nan 0.1000 -0.0000 ## 500 0.0109 nan 0.1000 -0.0001 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1354 nan 0.1000 0.0780 ## 2 1.0171 nan 0.1000 0.0538 ## 3 0.9099 nan 0.1000 0.0489 ## 4 0.8314 nan 0.1000 0.0375 ## 5 0.7601 nan 0.1000 0.0303 ## 6 0.6969 nan 0.1000 0.0310 ## 7 0.6421 nan 0.1000 0.0252 ## 8 0.5919 nan 0.1000 0.0234 ## 9 0.5460 nan 0.1000 0.0217 ## 10 0.5045 nan 0.1000 0.0191 ## 20 0.2867 nan 0.1000 0.0043 ## 40 0.1520 nan 0.1000 -0.0010 ## 60 0.0999 nan 0.1000 0.0003 ## 80 0.0753 nan 0.1000 -0.0007 ## 100 0.0595 nan 0.1000 -0.0009 ## 120 0.0462 nan 0.1000 -0.0000 ## 140 0.0354 nan 0.1000 -0.0003 ## 160 0.0279 nan 0.1000 0.0000 ## 180 0.0232 nan 0.1000 -0.0002 ## 200 0.0196 nan 0.1000 -0.0001 ## 220 0.0162 nan 0.1000 -0.0001 ## 240 0.0136 nan 0.1000 -0.0000 ## 260 0.0116 nan 0.1000 -0.0002 ## 280 0.0095 nan 0.1000 -0.0001 ## 300 0.0079 nan 0.1000 -0.0000 ## 320 0.0067 nan 0.1000 -0.0000 ## 340 0.0054 nan 0.1000 -0.0000 ## 360 0.0047 nan 0.1000 -0.0000 ## 380 0.0039 nan 0.1000 -0.0000 ## 400 0.0032 nan 0.1000 -0.0000 ## 420 0.0027 nan 0.1000 -0.0000 ## 440 0.0022 nan 0.1000 -0.0000 ## 460 0.0019 nan 0.1000 -0.0000 ## 480 0.0017 nan 0.1000 -0.0000 ## 500 0.0014 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1363 nan 0.1000 0.0749 ## 2 1.0129 nan 0.1000 0.0612 ## 3 0.9164 nan 0.1000 0.0471 ## 4 0.8267 nan 0.1000 0.0448 ## 5 0.7478 nan 0.1000 0.0372 ## 6 0.6799 nan 0.1000 0.0314 ## 7 0.6238 nan 0.1000 0.0272 ## 8 0.5708 nan 0.1000 0.0243 ## 9 0.5309 nan 0.1000 0.0193 ## 10 0.4911 nan 0.1000 0.0180 ## 20 0.2644 nan 0.1000 0.0036 ## 40 0.1236 nan 0.1000 -0.0001 ## 60 0.0782 nan 0.1000 -0.0007 ## 80 0.0538 nan 0.1000 -0.0002 ## 100 0.0390 nan 0.1000 -0.0003 ## 120 0.0286 nan 0.1000 -0.0004 ## 140 0.0202 nan 0.1000 0.0000 ## 160 0.0153 nan 0.1000 -0.0000 ## 180 0.0120 nan 0.1000 -0.0002 ## 200 0.0097 nan 0.1000 -0.0001 ## 220 0.0073 nan 0.1000 -0.0001 ## 240 0.0055 nan 0.1000 -0.0000 ## 260 0.0045 nan 0.1000 -0.0000 ## 280 0.0034 nan 0.1000 -0.0000 ## 300 0.0026 nan 0.1000 0.0000 ## 320 0.0021 nan 0.1000 -0.0000 ## 340 0.0016 nan 0.1000 -0.0000 ## 360 0.0013 nan 0.1000 -0.0000 ## 380 0.0010 nan 0.1000 -0.0000 ## 400 0.0008 nan 0.1000 -0.0000 ## 420 0.0006 nan 0.1000 -0.0000 ## 440 0.0005 nan 0.1000 -0.0000 ## 460 0.0004 nan 0.1000 -0.0000 ## 480 0.0003 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1356 nan 0.1000 0.0767 ## 2 1.0090 nan 0.1000 0.0600 ## 3 0.9026 nan 0.1000 0.0484 ## 4 0.8203 nan 0.1000 0.0371 ## 5 0.7483 nan 0.1000 0.0346 ## 6 0.6843 nan 0.1000 0.0289 ## 7 0.6226 nan 0.1000 0.0277 ## 8 0.5706 nan 0.1000 0.0251 ## 9 0.5269 nan 0.1000 0.0201 ## 10 0.4938 nan 0.1000 0.0122 ## 20 0.2635 nan 0.1000 0.0057 ## 40 0.1224 nan 0.1000 -0.0011 ## 60 0.0744 nan 0.1000 -0.0004 ## 80 0.0497 nan 0.1000 -0.0000 ## 100 0.0347 nan 0.1000 -0.0006 ## 120 0.0248 nan 0.1000 -0.0006 ## 140 0.0179 nan 0.1000 -0.0002 ## 160 0.0124 nan 0.1000 0.0000 ## 180 0.0091 nan 0.1000 -0.0001 ## 200 0.0066 nan 0.1000 -0.0001 ## 220 0.0046 nan 0.1000 -0.0001 ## 240 0.0038 nan 0.1000 -0.0001 ## 260 0.0027 nan 0.1000 -0.0000 ## 280 0.0022 nan 0.1000 -0.0001 ## 300 0.0017 nan 0.1000 -0.0000 ## 320 0.0012 nan 0.1000 -0.0000 ## 340 0.0010 nan 0.1000 -0.0000 ## 360 0.0008 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1349 nan 0.1000 0.0758 ## 2 1.0082 nan 0.1000 0.0599 ## 3 0.9050 nan 0.1000 0.0468 ## 4 0.8195 nan 0.1000 0.0388 ## 5 0.7440 nan 0.1000 0.0358 ## 6 0.6813 nan 0.1000 0.0279 ## 7 0.6251 nan 0.1000 0.0252 ## 8 0.5765 nan 0.1000 0.0201 ## 9 0.5335 nan 0.1000 0.0185 ## 10 0.4957 nan 0.1000 0.0155 ## 20 0.2561 nan 0.1000 0.0068 ## 40 0.1068 nan 0.1000 0.0002 ## 60 0.0596 nan 0.1000 -0.0002 ## 80 0.0375 nan 0.1000 -0.0002 ## 100 0.0246 nan 0.1000 -0.0004 ## 120 0.0160 nan 0.1000 -0.0001 ## 140 0.0117 nan 0.1000 -0.0002 ## 160 0.0081 nan 0.1000 -0.0002 ## 180 0.0060 nan 0.1000 -0.0000 ## 200 0.0044 nan 0.1000 -0.0000 ## 220 0.0035 nan 0.1000 -0.0001 ## 240 0.0028 nan 0.1000 -0.0000 ## 260 0.0020 nan 0.1000 -0.0001 ## 280 0.0017 nan 0.1000 -0.0001 ## 300 0.0014 nan 0.1000 -0.0000 ## 320 0.0012 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0007 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1296 nan 0.1000 0.0712 ## 2 1.0069 nan 0.1000 0.0608 ## 3 0.9031 nan 0.1000 0.0502 ## 4 0.8138 nan 0.1000 0.0391 ## 5 0.7399 nan 0.1000 0.0351 ## 6 0.6726 nan 0.1000 0.0327 ## 7 0.6172 nan 0.1000 0.0254 ## 8 0.5704 nan 0.1000 0.0194 ## 9 0.5253 nan 0.1000 0.0206 ## 10 0.4847 nan 0.1000 0.0164 ## 20 0.2611 nan 0.1000 0.0046 ## 40 0.1067 nan 0.1000 0.0001 ## 60 0.0597 nan 0.1000 -0.0012 ## 80 0.0323 nan 0.1000 -0.0001 ## 100 0.0224 nan 0.1000 -0.0007 ## 120 0.0137 nan 0.1000 -0.0002 ## 140 0.0100 nan 0.1000 -0.0004 ## 160 0.0066 nan 0.1000 0.0000 ## 180 0.0048 nan 0.1000 -0.0001 ## 200 0.0036 nan 0.1000 -0.0000 ## 220 0.0027 nan 0.1000 -0.0000 ## 240 0.0022 nan 0.1000 -0.0000 ## 260 0.0018 nan 0.1000 -0.0000 ## 280 0.0013 nan 0.1000 -0.0000 ## 300 0.0012 nan 0.1000 -0.0001 ## 320 0.0012 nan 0.1000 -0.0000 ## 340 0.0009 nan 0.1000 -0.0001 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1392 nan 0.1000 0.0741 ## 2 1.0146 nan 0.1000 0.0564 ## 3 0.9167 nan 0.1000 0.0434 ## 4 0.8264 nan 0.1000 0.0443 ## 5 0.7587 nan 0.1000 0.0306 ## 6 0.6915 nan 0.1000 0.0317 ## 7 0.6360 nan 0.1000 0.0256 ## 8 0.5836 nan 0.1000 0.0247 ## 9 0.5370 nan 0.1000 0.0228 ## 10 0.4964 nan 0.1000 0.0190 ## 20 0.2599 nan 0.1000 0.0049 ## 40 0.1050 nan 0.1000 -0.0005 ## 60 0.0562 nan 0.1000 -0.0008 ## 80 0.0324 nan 0.1000 -0.0006 ## 100 0.0207 nan 0.1000 0.0000 ## 120 0.0146 nan 0.1000 -0.0002 ## 140 0.0112 nan 0.1000 -0.0004 ## 160 0.0077 nan 0.1000 -0.0003 ## 180 0.0061 nan 0.1000 -0.0002 ## 200 0.0041 nan 0.1000 -0.0000 ## 220 0.0033 nan 0.1000 -0.0001 ## 240 0.0024 nan 0.1000 -0.0000 ## 260 0.0021 nan 0.1000 -0.0001 ## 280 0.0015 nan 0.1000 -0.0000 ## 300 0.0014 nan 0.1000 -0.0000 ## 320 0.0013 nan 0.1000 -0.0001 ## 340 0.0009 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0006 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1225 nan 0.1000 0.0810 ## 2 0.9911 nan 0.1000 0.0652 ## 3 0.8884 nan 0.1000 0.0475 ## 4 0.8045 nan 0.1000 0.0395 ## 5 0.7317 nan 0.1000 0.0328 ## 6 0.6671 nan 0.1000 0.0288 ## 7 0.6106 nan 0.1000 0.0256 ## 8 0.5619 nan 0.1000 0.0218 ## 9 0.5189 nan 0.1000 0.0197 ## 10 0.4792 nan 0.1000 0.0184 ## 20 0.2472 nan 0.1000 0.0068 ## 40 0.0999 nan 0.1000 -0.0002 ## 60 0.0558 nan 0.1000 -0.0004 ## 80 0.0412 nan 0.1000 -0.0009 ## 100 0.0275 nan 0.1000 -0.0007 ## 120 0.0175 nan 0.1000 -0.0004 ## 140 0.0128 nan 0.1000 -0.0004 ## 160 0.0100 nan 0.1000 -0.0002 ## 180 0.0071 nan 0.1000 -0.0003 ## 200 0.0046 nan 0.1000 -0.0001 ## 220 0.0030 nan 0.1000 -0.0001 ## 240 0.0021 nan 0.1000 -0.0000 ## 260 0.0014 nan 0.1000 -0.0000 ## 280 0.0011 nan 0.1000 -0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0004 nan 0.1000 -0.0000 ## 460 0.0003 nan 0.1000 -0.0000 ## 480 0.0003 nan 0.1000 -0.0000 ## 500 0.0002 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1338 nan 0.1000 0.0711 ## 2 1.0086 nan 0.1000 0.0608 ## 3 0.9022 nan 0.1000 0.0488 ## 4 0.8210 nan 0.1000 0.0366 ## 5 0.7431 nan 0.1000 0.0361 ## 6 0.6752 nan 0.1000 0.0334 ## 7 0.6174 nan 0.1000 0.0262 ## 8 0.5661 nan 0.1000 0.0247 ## 9 0.5200 nan 0.1000 0.0198 ## 10 0.4807 nan 0.1000 0.0176 ## 20 0.2570 nan 0.1000 0.0050 ## 40 0.1002 nan 0.1000 0.0021 ## 60 0.0547 nan 0.1000 -0.0011 ## 80 0.0324 nan 0.1000 0.0000 ## 100 0.0213 nan 0.1000 0.0003 ## 120 0.0147 nan 0.1000 0.0000 ## 140 0.0099 nan 0.1000 -0.0003 ## 160 0.0070 nan 0.1000 -0.0001 ## 180 0.0047 nan 0.1000 -0.0001 ## 200 0.0038 nan 0.1000 -0.0002 ## 220 0.0028 nan 0.1000 -0.0001 ## 240 0.0021 nan 0.1000 -0.0000 ## 260 0.0015 nan 0.1000 -0.0000 ## 280 0.0014 nan 0.1000 -0.0001 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0006 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1491 nan 0.1000 0.0664 ## 2 1.0313 nan 0.1000 0.0568 ## 3 0.9371 nan 0.1000 0.0447 ## 4 0.8564 nan 0.1000 0.0363 ## 5 0.7965 nan 0.1000 0.0289 ## 6 0.7345 nan 0.1000 0.0285 ## 7 0.6827 nan 0.1000 0.0231 ## 8 0.6438 nan 0.1000 0.0162 ## 9 0.6098 nan 0.1000 0.0132 ## 10 0.5737 nan 0.1000 0.0177 ## 20 0.3615 nan 0.1000 0.0062 ## 40 0.2176 nan 0.1000 0.0004 ## 60 0.1620 nan 0.1000 -0.0005 ## 80 0.1335 nan 0.1000 -0.0002 ## 100 0.1178 nan 0.1000 -0.0006 ## 120 0.1079 nan 0.1000 -0.0013 ## 140 0.1002 nan 0.1000 -0.0006 ## 160 0.0927 nan 0.1000 -0.0009 ## 180 0.0860 nan 0.1000 -0.0005 ## 200 0.0837 nan 0.1000 -0.0002 ## 220 0.0761 nan 0.1000 -0.0006 ## 240 0.0741 nan 0.1000 -0.0006 ## 260 0.0700 nan 0.1000 -0.0004 ## 280 0.0690 nan 0.1000 -0.0009 ## 300 0.0661 nan 0.1000 -0.0004 ## 320 0.0647 nan 0.1000 -0.0010 ## 340 0.0636 nan 0.1000 -0.0006 ## 360 0.0611 nan 0.1000 -0.0004 ## 380 0.0604 nan 0.1000 -0.0008 ## 400 0.0593 nan 0.1000 -0.0006 ## 420 0.0582 nan 0.1000 -0.0002 ## 440 0.0573 nan 0.1000 -0.0008 ## 460 0.0580 nan 0.1000 -0.0008 ## 480 0.0563 nan 0.1000 -0.0005 ## 500 0.0562 nan 0.1000 -0.0005 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1387 nan 0.1000 0.0695 ## 2 1.0214 nan 0.1000 0.0589 ## 3 0.9154 nan 0.1000 0.0522 ## 4 0.8278 nan 0.1000 0.0422 ## 5 0.7554 nan 0.1000 0.0367 ## 6 0.6943 nan 0.1000 0.0287 ## 7 0.6317 nan 0.1000 0.0272 ## 8 0.5856 nan 0.1000 0.0222 ## 9 0.5431 nan 0.1000 0.0196 ## 10 0.5087 nan 0.1000 0.0168 ## 20 0.2833 nan 0.1000 0.0052 ## 40 0.1593 nan 0.1000 0.0003 ## 60 0.1132 nan 0.1000 -0.0005 ## 80 0.0895 nan 0.1000 0.0002 ## 100 0.0734 nan 0.1000 -0.0005 ## 120 0.0598 nan 0.1000 -0.0006 ## 140 0.0519 nan 0.1000 -0.0003 ## 160 0.0448 nan 0.1000 -0.0004 ## 180 0.0399 nan 0.1000 -0.0005 ## 200 0.0361 nan 0.1000 -0.0002 ## 220 0.0321 nan 0.1000 -0.0001 ## 240 0.0297 nan 0.1000 -0.0003 ## 260 0.0264 nan 0.1000 -0.0002 ## 280 0.0227 nan 0.1000 -0.0001 ## 300 0.0206 nan 0.1000 -0.0002 ## 320 0.0190 nan 0.1000 -0.0002 ## 340 0.0167 nan 0.1000 -0.0001 ## 360 0.0143 nan 0.1000 -0.0001 ## 380 0.0128 nan 0.1000 -0.0002 ## 400 0.0112 nan 0.1000 -0.0000 ## 420 0.0102 nan 0.1000 -0.0000 ## 440 0.0091 nan 0.1000 -0.0001 ## 460 0.0086 nan 0.1000 -0.0001 ## 480 0.0079 nan 0.1000 -0.0000 ## 500 0.0070 nan 0.1000 -0.0001 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1324 nan 0.1000 0.0750 ## 2 1.0107 nan 0.1000 0.0597 ## 3 0.9140 nan 0.1000 0.0449 ## 4 0.8229 nan 0.1000 0.0446 ## 5 0.7546 nan 0.1000 0.0296 ## 6 0.6960 nan 0.1000 0.0255 ## 7 0.6398 nan 0.1000 0.0275 ## 8 0.5914 nan 0.1000 0.0239 ## 9 0.5463 nan 0.1000 0.0194 ## 10 0.5062 nan 0.1000 0.0189 ## 20 0.2759 nan 0.1000 0.0026 ## 40 0.1280 nan 0.1000 0.0009 ## 60 0.0883 nan 0.1000 -0.0009 ## 80 0.0666 nan 0.1000 -0.0005 ## 100 0.0504 nan 0.1000 -0.0003 ## 120 0.0402 nan 0.1000 -0.0003 ## 140 0.0321 nan 0.1000 -0.0003 ## 160 0.0271 nan 0.1000 0.0000 ## 180 0.0218 nan 0.1000 -0.0004 ## 200 0.0176 nan 0.1000 -0.0001 ## 220 0.0143 nan 0.1000 0.0000 ## 240 0.0122 nan 0.1000 -0.0002 ## 260 0.0099 nan 0.1000 -0.0001 ## 280 0.0088 nan 0.1000 -0.0001 ## 300 0.0074 nan 0.1000 -0.0000 ## 320 0.0063 nan 0.1000 -0.0000 ## 340 0.0050 nan 0.1000 -0.0000 ## 360 0.0042 nan 0.1000 -0.0000 ## 380 0.0033 nan 0.1000 -0.0000 ## 400 0.0027 nan 0.1000 -0.0000 ## 420 0.0022 nan 0.1000 -0.0000 ## 440 0.0019 nan 0.1000 -0.0000 ## 460 0.0016 nan 0.1000 -0.0000 ## 480 0.0014 nan 0.1000 -0.0000 ## 500 0.0012 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1365 nan 0.1000 0.0709 ## 2 1.0144 nan 0.1000 0.0599 ## 3 0.9107 nan 0.1000 0.0509 ## 4 0.8248 nan 0.1000 0.0405 ## 5 0.7534 nan 0.1000 0.0329 ## 6 0.6857 nan 0.1000 0.0303 ## 7 0.6307 nan 0.1000 0.0258 ## 8 0.5759 nan 0.1000 0.0264 ## 9 0.5315 nan 0.1000 0.0199 ## 10 0.4908 nan 0.1000 0.0178 ## 20 0.2617 nan 0.1000 0.0050 ## 40 0.1223 nan 0.1000 0.0000 ## 60 0.0736 nan 0.1000 -0.0011 ## 80 0.0518 nan 0.1000 -0.0006 ## 100 0.0383 nan 0.1000 -0.0002 ## 120 0.0312 nan 0.1000 -0.0000 ## 140 0.0218 nan 0.1000 -0.0001 ## 160 0.0179 nan 0.1000 -0.0000 ## 180 0.0147 nan 0.1000 -0.0001 ## 200 0.0113 nan 0.1000 -0.0002 ## 220 0.0088 nan 0.1000 -0.0001 ## 240 0.0068 nan 0.1000 -0.0001 ## 260 0.0053 nan 0.1000 -0.0001 ## 280 0.0043 nan 0.1000 -0.0001 ## 300 0.0036 nan 0.1000 -0.0000 ## 320 0.0031 nan 0.1000 -0.0001 ## 340 0.0022 nan 0.1000 -0.0000 ## 360 0.0018 nan 0.1000 -0.0000 ## 380 0.0015 nan 0.1000 -0.0000 ## 400 0.0013 nan 0.1000 -0.0000 ## 420 0.0010 nan 0.1000 -0.0000 ## 440 0.0008 nan 0.1000 -0.0000 ## 460 0.0007 nan 0.1000 -0.0000 ## 480 0.0006 nan 0.1000 -0.0000 ## 500 0.0005 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1274 nan 0.1000 0.0824 ## 2 1.0048 nan 0.1000 0.0603 ## 3 0.9093 nan 0.1000 0.0487 ## 4 0.8202 nan 0.1000 0.0408 ## 5 0.7465 nan 0.1000 0.0352 ## 6 0.6835 nan 0.1000 0.0300 ## 7 0.6276 nan 0.1000 0.0248 ## 8 0.5766 nan 0.1000 0.0235 ## 9 0.5329 nan 0.1000 0.0173 ## 10 0.4927 nan 0.1000 0.0171 ## 20 0.2589 nan 0.1000 0.0061 ## 40 0.1114 nan 0.1000 0.0002 ## 60 0.0634 nan 0.1000 0.0000 ## 80 0.0413 nan 0.1000 -0.0004 ## 100 0.0277 nan 0.1000 -0.0002 ## 120 0.0197 nan 0.1000 -0.0001 ## 140 0.0139 nan 0.1000 -0.0002 ## 160 0.0107 nan 0.1000 -0.0002 ## 180 0.0077 nan 0.1000 -0.0002 ## 200 0.0061 nan 0.1000 -0.0002 ## 220 0.0045 nan 0.1000 -0.0001 ## 240 0.0036 nan 0.1000 -0.0000 ## 260 0.0026 nan 0.1000 -0.0000 ## 280 0.0019 nan 0.1000 -0.0000 ## 300 0.0016 nan 0.1000 -0.0000 ## 320 0.0012 nan 0.1000 -0.0000 ## 340 0.0009 nan 0.1000 -0.0000 ## 360 0.0007 nan 0.1000 0.0000 ## 380 0.0006 nan 0.1000 -0.0000 ## 400 0.0005 nan 0.1000 -0.0000 ## 420 0.0004 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1371 nan 0.1000 0.0687 ## 2 1.0153 nan 0.1000 0.0552 ## 3 0.9113 nan 0.1000 0.0498 ## 4 0.8274 nan 0.1000 0.0391 ## 5 0.7517 nan 0.1000 0.0341 ## 6 0.6833 nan 0.1000 0.0336 ## 7 0.6269 nan 0.1000 0.0270 ## 8 0.5778 nan 0.1000 0.0209 ## 9 0.5322 nan 0.1000 0.0206 ## 10 0.4922 nan 0.1000 0.0189 ## 20 0.2678 nan 0.1000 0.0042 ## 40 0.1037 nan 0.1000 -0.0006 ## 60 0.0565 nan 0.1000 -0.0004 ## 80 0.0353 nan 0.1000 -0.0003 ## 100 0.0237 nan 0.1000 -0.0002 ## 120 0.0183 nan 0.1000 -0.0002 ## 140 0.0120 nan 0.1000 -0.0002 ## 160 0.0090 nan 0.1000 -0.0001 ## 180 0.0065 nan 0.1000 0.0001 ## 200 0.0050 nan 0.1000 -0.0001 ## 220 0.0037 nan 0.1000 -0.0001 ## 240 0.0028 nan 0.1000 -0.0000 ## 260 0.0020 nan 0.1000 -0.0001 ## 280 0.0015 nan 0.1000 0.0000 ## 300 0.0013 nan 0.1000 -0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1314 nan 0.1000 0.0756 ## 2 1.0051 nan 0.1000 0.0611 ## 3 0.9026 nan 0.1000 0.0461 ## 4 0.8142 nan 0.1000 0.0401 ## 5 0.7389 nan 0.1000 0.0339 ## 6 0.6725 nan 0.1000 0.0320 ## 7 0.6198 nan 0.1000 0.0226 ## 8 0.5711 nan 0.1000 0.0236 ## 9 0.5246 nan 0.1000 0.0216 ## 10 0.4829 nan 0.1000 0.0181 ## 20 0.2550 nan 0.1000 0.0072 ## 40 0.0953 nan 0.1000 0.0007 ## 60 0.0522 nan 0.1000 -0.0008 ## 80 0.0301 nan 0.1000 -0.0002 ## 100 0.0195 nan 0.1000 -0.0001 ## 120 0.0133 nan 0.1000 -0.0001 ## 140 0.0090 nan 0.1000 -0.0002 ## 160 0.0066 nan 0.1000 -0.0001 ## 180 0.0048 nan 0.1000 -0.0000 ## 200 0.0035 nan 0.1000 -0.0001 ## 220 0.0027 nan 0.1000 -0.0000 ## 240 0.0021 nan 0.1000 -0.0000 ## 260 0.0014 nan 0.1000 0.0000 ## 280 0.0010 nan 0.1000 -0.0000 ## 300 0.0007 nan 0.1000 -0.0000 ## 320 0.0005 nan 0.1000 -0.0000 ## 340 0.0004 nan 0.1000 -0.0000 ## 360 0.0003 nan 0.1000 -0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1328 nan 0.1000 0.0737 ## 2 1.0077 nan 0.1000 0.0619 ## 3 0.9030 nan 0.1000 0.0490 ## 4 0.8121 nan 0.1000 0.0439 ## 5 0.7381 nan 0.1000 0.0332 ## 6 0.6694 nan 0.1000 0.0339 ## 7 0.6131 nan 0.1000 0.0260 ## 8 0.5641 nan 0.1000 0.0221 ## 9 0.5202 nan 0.1000 0.0205 ## 10 0.4843 nan 0.1000 0.0148 ## 20 0.2529 nan 0.1000 0.0045 ## 40 0.0938 nan 0.1000 -0.0005 ## 60 0.0456 nan 0.1000 0.0000 ## 80 0.0264 nan 0.1000 -0.0005 ## 100 0.0182 nan 0.1000 -0.0002 ## 120 0.0118 nan 0.1000 -0.0002 ## 140 0.0079 nan 0.1000 -0.0001 ## 160 0.0062 nan 0.1000 -0.0001 ## 180 0.0045 nan 0.1000 -0.0002 ## 200 0.0032 nan 0.1000 -0.0000 ## 220 0.0023 nan 0.1000 -0.0000 ## 240 0.0020 nan 0.1000 -0.0001 ## 260 0.0015 nan 0.1000 -0.0001 ## 280 0.0012 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1297 nan 0.1000 0.0765 ## 2 1.0052 nan 0.1000 0.0569 ## 3 0.8979 nan 0.1000 0.0503 ## 4 0.8061 nan 0.1000 0.0430 ## 5 0.7345 nan 0.1000 0.0319 ## 6 0.6675 nan 0.1000 0.0295 ## 7 0.6085 nan 0.1000 0.0256 ## 8 0.5604 nan 0.1000 0.0218 ## 9 0.5150 nan 0.1000 0.0221 ## 10 0.4743 nan 0.1000 0.0171 ## 20 0.2487 nan 0.1000 0.0041 ## 40 0.0954 nan 0.1000 0.0001 ## 60 0.0521 nan 0.1000 -0.0005 ## 80 0.0306 nan 0.1000 -0.0003 ## 100 0.0214 nan 0.1000 -0.0002 ## 120 0.0154 nan 0.1000 -0.0002 ## 140 0.0113 nan 0.1000 -0.0001 ## 160 0.0078 nan 0.1000 -0.0000 ## 180 0.0059 nan 0.1000 -0.0001 ## 200 0.0044 nan 0.1000 -0.0001 ## 220 0.0030 nan 0.1000 -0.0001 ## 240 0.0023 nan 0.1000 -0.0001 ## 260 0.0016 nan 0.1000 -0.0000 ## 280 0.0014 nan 0.1000 -0.0001 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1239 nan 0.1000 0.0769 ## 2 0.9954 nan 0.1000 0.0626 ## 3 0.8904 nan 0.1000 0.0487 ## 4 0.8044 nan 0.1000 0.0383 ## 5 0.7340 nan 0.1000 0.0317 ## 6 0.6702 nan 0.1000 0.0305 ## 7 0.6130 nan 0.1000 0.0262 ## 8 0.5640 nan 0.1000 0.0218 ## 9 0.5202 nan 0.1000 0.0192 ## 10 0.4794 nan 0.1000 0.0187 ## 20 0.2513 nan 0.1000 0.0045 ## 40 0.0953 nan 0.1000 0.0005 ## 60 0.0519 nan 0.1000 -0.0003 ## 80 0.0350 nan 0.1000 -0.0003 ## 100 0.0242 nan 0.1000 -0.0005 ## 120 0.0141 nan 0.1000 -0.0001 ## 140 0.0088 nan 0.1000 -0.0001 ## 160 0.0056 nan 0.1000 -0.0001 ## 180 0.0039 nan 0.1000 -0.0000 ## 200 0.0029 nan 0.1000 -0.0001 ## 220 0.0022 nan 0.1000 -0.0000 ## 240 0.0019 nan 0.1000 -0.0000 ## 260 0.0015 nan 0.1000 -0.0000 ## 280 0.0011 nan 0.1000 -0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1466 nan 0.1000 0.0661 ## 2 1.0433 nan 0.1000 0.0500 ## 3 0.9458 nan 0.1000 0.0481 ## 4 0.8639 nan 0.1000 0.0389 ## 5 0.7918 nan 0.1000 0.0348 ## 6 0.7312 nan 0.1000 0.0300 ## 7 0.6776 nan 0.1000 0.0243 ## 8 0.6327 nan 0.1000 0.0224 ## 9 0.5925 nan 0.1000 0.0185 ## 10 0.5557 nan 0.1000 0.0182 ## 20 0.3583 nan 0.1000 0.0043 ## 40 0.2201 nan 0.1000 -0.0006 ## 60 0.1759 nan 0.1000 -0.0009 ## 80 0.1527 nan 0.1000 -0.0008 ## 100 0.1372 nan 0.1000 -0.0013 ## 120 0.1269 nan 0.1000 0.0000 ## 140 0.1154 nan 0.1000 -0.0005 ## 160 0.1066 nan 0.1000 -0.0007 ## 180 0.1007 nan 0.1000 -0.0003 ## 200 0.0936 nan 0.1000 -0.0005 ## 220 0.0914 nan 0.1000 -0.0008 ## 240 0.0879 nan 0.1000 -0.0001 ## 260 0.0846 nan 0.1000 -0.0003 ## 280 0.0815 nan 0.1000 -0.0008 ## 300 0.0797 nan 0.1000 -0.0009 ## 320 0.0788 nan 0.1000 -0.0006 ## 340 0.0752 nan 0.1000 -0.0011 ## 360 0.0738 nan 0.1000 -0.0003 ## 380 0.0721 nan 0.1000 -0.0007 ## 400 0.0714 nan 0.1000 -0.0005 ## 420 0.0700 nan 0.1000 -0.0008 ## 440 0.0683 nan 0.1000 -0.0003 ## 460 0.0676 nan 0.1000 -0.0005 ## 480 0.0662 nan 0.1000 -0.0004 ## 500 0.0650 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1372 nan 0.1000 0.0756 ## 2 1.0143 nan 0.1000 0.0598 ## 3 0.9145 nan 0.1000 0.0476 ## 4 0.8274 nan 0.1000 0.0421 ## 5 0.7507 nan 0.1000 0.0382 ## 6 0.6939 nan 0.1000 0.0250 ## 7 0.6390 nan 0.1000 0.0258 ## 8 0.5913 nan 0.1000 0.0232 ## 9 0.5479 nan 0.1000 0.0206 ## 10 0.5102 nan 0.1000 0.0164 ## 20 0.2949 nan 0.1000 0.0031 ## 40 0.1682 nan 0.1000 -0.0003 ## 60 0.1245 nan 0.1000 -0.0008 ## 80 0.0967 nan 0.1000 -0.0011 ## 100 0.0835 nan 0.1000 -0.0010 ## 120 0.0699 nan 0.1000 -0.0007 ## 140 0.0602 nan 0.1000 -0.0002 ## 160 0.0549 nan 0.1000 -0.0003 ## 180 0.0487 nan 0.1000 -0.0003 ## 200 0.0434 nan 0.1000 -0.0008 ## 220 0.0396 nan 0.1000 -0.0005 ## 240 0.0356 nan 0.1000 -0.0002 ## 260 0.0310 nan 0.1000 -0.0003 ## 280 0.0288 nan 0.1000 -0.0003 ## 300 0.0264 nan 0.1000 -0.0003 ## 320 0.0243 nan 0.1000 -0.0004 ## 340 0.0231 nan 0.1000 -0.0002 ## 360 0.0206 nan 0.1000 -0.0003 ## 380 0.0188 nan 0.1000 -0.0002 ## 400 0.0170 nan 0.1000 -0.0003 ## 420 0.0154 nan 0.1000 -0.0002 ## 440 0.0139 nan 0.1000 0.0000 ## 460 0.0128 nan 0.1000 -0.0000 ## 480 0.0114 nan 0.1000 -0.0001 ## 500 0.0106 nan 0.1000 -0.0001 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1309 nan 0.1000 0.0721 ## 2 1.0076 nan 0.1000 0.0626 ## 3 0.9050 nan 0.1000 0.0465 ## 4 0.8241 nan 0.1000 0.0384 ## 5 0.7524 nan 0.1000 0.0351 ## 6 0.6838 nan 0.1000 0.0289 ## 7 0.6296 nan 0.1000 0.0249 ## 8 0.5843 nan 0.1000 0.0214 ## 9 0.5383 nan 0.1000 0.0204 ## 10 0.5013 nan 0.1000 0.0159 ## 20 0.2780 nan 0.1000 0.0047 ## 40 0.1375 nan 0.1000 -0.0007 ## 60 0.0941 nan 0.1000 -0.0009 ## 80 0.0699 nan 0.1000 -0.0010 ## 100 0.0528 nan 0.1000 0.0001 ## 120 0.0413 nan 0.1000 -0.0003 ## 140 0.0360 nan 0.1000 0.0000 ## 160 0.0308 nan 0.1000 -0.0004 ## 180 0.0256 nan 0.1000 -0.0004 ## 200 0.0206 nan 0.1000 -0.0001 ## 220 0.0167 nan 0.1000 -0.0001 ## 240 0.0141 nan 0.1000 -0.0001 ## 260 0.0117 nan 0.1000 -0.0001 ## 280 0.0099 nan 0.1000 -0.0002 ## 300 0.0083 nan 0.1000 -0.0000 ## 320 0.0073 nan 0.1000 -0.0001 ## 340 0.0063 nan 0.1000 -0.0001 ## 360 0.0054 nan 0.1000 -0.0000 ## 380 0.0045 nan 0.1000 -0.0000 ## 400 0.0041 nan 0.1000 -0.0001 ## 420 0.0035 nan 0.1000 -0.0000 ## 440 0.0029 nan 0.1000 -0.0000 ## 460 0.0025 nan 0.1000 -0.0000 ## 480 0.0021 nan 0.1000 -0.0000 ## 500 0.0018 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1381 nan 0.1000 0.0744 ## 2 1.0124 nan 0.1000 0.0623 ## 3 0.9079 nan 0.1000 0.0496 ## 4 0.8168 nan 0.1000 0.0461 ## 5 0.7448 nan 0.1000 0.0336 ## 6 0.6799 nan 0.1000 0.0294 ## 7 0.6193 nan 0.1000 0.0275 ## 8 0.5729 nan 0.1000 0.0211 ## 9 0.5257 nan 0.1000 0.0213 ## 10 0.4862 nan 0.1000 0.0186 ## 20 0.2655 nan 0.1000 0.0048 ## 40 0.1255 nan 0.1000 -0.0000 ## 60 0.0828 nan 0.1000 -0.0014 ## 80 0.0608 nan 0.1000 -0.0008 ## 100 0.0437 nan 0.1000 -0.0005 ## 120 0.0317 nan 0.1000 -0.0004 ## 140 0.0242 nan 0.1000 -0.0002 ## 160 0.0176 nan 0.1000 -0.0001 ## 180 0.0138 nan 0.1000 -0.0000 ## 200 0.0106 nan 0.1000 -0.0001 ## 220 0.0086 nan 0.1000 -0.0000 ## 240 0.0065 nan 0.1000 -0.0000 ## 260 0.0053 nan 0.1000 -0.0001 ## 280 0.0042 nan 0.1000 -0.0000 ## 300 0.0035 nan 0.1000 -0.0000 ## 320 0.0029 nan 0.1000 -0.0000 ## 340 0.0022 nan 0.1000 -0.0000 ## 360 0.0018 nan 0.1000 -0.0000 ## 380 0.0013 nan 0.1000 -0.0000 ## 400 0.0010 nan 0.1000 -0.0000 ## 420 0.0008 nan 0.1000 -0.0000 ## 440 0.0006 nan 0.1000 -0.0000 ## 460 0.0005 nan 0.1000 -0.0000 ## 480 0.0004 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1331 nan 0.1000 0.0741 ## 2 1.0097 nan 0.1000 0.0596 ## 3 0.9025 nan 0.1000 0.0470 ## 4 0.8165 nan 0.1000 0.0401 ## 5 0.7428 nan 0.1000 0.0357 ## 6 0.6783 nan 0.1000 0.0293 ## 7 0.6228 nan 0.1000 0.0244 ## 8 0.5700 nan 0.1000 0.0250 ## 9 0.5274 nan 0.1000 0.0184 ## 10 0.4874 nan 0.1000 0.0171 ## 20 0.2637 nan 0.1000 0.0039 ## 40 0.1162 nan 0.1000 0.0004 ## 60 0.0662 nan 0.1000 -0.0005 ## 80 0.0448 nan 0.1000 -0.0007 ## 100 0.0313 nan 0.1000 -0.0002 ## 120 0.0247 nan 0.1000 0.0001 ## 140 0.0184 nan 0.1000 -0.0002 ## 160 0.0133 nan 0.1000 -0.0002 ## 180 0.0096 nan 0.1000 -0.0001 ## 200 0.0069 nan 0.1000 -0.0002 ## 220 0.0048 nan 0.1000 -0.0001 ## 240 0.0037 nan 0.1000 -0.0001 ## 260 0.0027 nan 0.1000 0.0000 ## 280 0.0021 nan 0.1000 -0.0000 ## 300 0.0015 nan 0.1000 -0.0000 ## 320 0.0011 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1323 nan 0.1000 0.0753 ## 2 1.0085 nan 0.1000 0.0596 ## 3 0.9053 nan 0.1000 0.0505 ## 4 0.8160 nan 0.1000 0.0443 ## 5 0.7379 nan 0.1000 0.0352 ## 6 0.6789 nan 0.1000 0.0264 ## 7 0.6243 nan 0.1000 0.0225 ## 8 0.5747 nan 0.1000 0.0215 ## 9 0.5288 nan 0.1000 0.0217 ## 10 0.4891 nan 0.1000 0.0186 ## 20 0.2585 nan 0.1000 0.0057 ## 40 0.1027 nan 0.1000 0.0008 ## 60 0.0581 nan 0.1000 -0.0008 ## 80 0.0389 nan 0.1000 -0.0005 ## 100 0.0256 nan 0.1000 -0.0003 ## 120 0.0175 nan 0.1000 -0.0001 ## 140 0.0129 nan 0.1000 -0.0003 ## 160 0.0092 nan 0.1000 -0.0000 ## 180 0.0067 nan 0.1000 -0.0001 ## 200 0.0046 nan 0.1000 -0.0000 ## 220 0.0033 nan 0.1000 0.0000 ## 240 0.0024 nan 0.1000 -0.0000 ## 260 0.0016 nan 0.1000 -0.0000 ## 280 0.0011 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1299 nan 0.1000 0.0744 ## 2 1.0018 nan 0.1000 0.0626 ## 3 0.8963 nan 0.1000 0.0482 ## 4 0.8076 nan 0.1000 0.0419 ## 5 0.7316 nan 0.1000 0.0394 ## 6 0.6672 nan 0.1000 0.0296 ## 7 0.6078 nan 0.1000 0.0274 ## 8 0.5607 nan 0.1000 0.0190 ## 9 0.5144 nan 0.1000 0.0192 ## 10 0.4789 nan 0.1000 0.0150 ## 20 0.2546 nan 0.1000 0.0030 ## 40 0.1082 nan 0.1000 -0.0003 ## 60 0.0587 nan 0.1000 -0.0007 ## 80 0.0407 nan 0.1000 -0.0016 ## 100 0.0254 nan 0.1000 -0.0000 ## 120 0.0171 nan 0.1000 -0.0003 ## 140 0.0125 nan 0.1000 -0.0005 ## 160 0.0093 nan 0.1000 -0.0003 ## 180 0.0070 nan 0.1000 -0.0001 ## 200 0.0053 nan 0.1000 -0.0002 ## 220 0.0041 nan 0.1000 -0.0002 ## 240 0.0029 nan 0.1000 -0.0000 ## 260 0.0020 nan 0.1000 -0.0000 ## 280 0.0014 nan 0.1000 -0.0000 ## 300 0.0010 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1347 nan 0.1000 0.0719 ## 2 1.0126 nan 0.1000 0.0564 ## 3 0.9078 nan 0.1000 0.0518 ## 4 0.8233 nan 0.1000 0.0396 ## 5 0.7449 nan 0.1000 0.0365 ## 6 0.6781 nan 0.1000 0.0340 ## 7 0.6232 nan 0.1000 0.0226 ## 8 0.5720 nan 0.1000 0.0248 ## 9 0.5300 nan 0.1000 0.0174 ## 10 0.4925 nan 0.1000 0.0175 ## 20 0.2562 nan 0.1000 0.0051 ## 40 0.1031 nan 0.1000 0.0003 ## 60 0.0482 nan 0.1000 -0.0004 ## 80 0.0294 nan 0.1000 -0.0004 ## 100 0.0191 nan 0.1000 -0.0003 ## 120 0.0145 nan 0.1000 -0.0005 ## 140 0.0109 nan 0.1000 -0.0000 ## 160 0.0067 nan 0.1000 -0.0001 ## 180 0.0045 nan 0.1000 -0.0001 ## 200 0.0036 nan 0.1000 -0.0001 ## 220 0.0026 nan 0.1000 -0.0000 ## 240 0.0017 nan 0.1000 -0.0000 ## 260 0.0013 nan 0.1000 -0.0000 ## 280 0.0009 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0003 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1303 nan 0.1000 0.0725 ## 2 1.0023 nan 0.1000 0.0618 ## 3 0.8951 nan 0.1000 0.0531 ## 4 0.8087 nan 0.1000 0.0401 ## 5 0.7378 nan 0.1000 0.0312 ## 6 0.6752 nan 0.1000 0.0295 ## 7 0.6190 nan 0.1000 0.0249 ## 8 0.5697 nan 0.1000 0.0232 ## 9 0.5274 nan 0.1000 0.0208 ## 10 0.4886 nan 0.1000 0.0171 ## 20 0.2496 nan 0.1000 0.0050 ## 40 0.1027 nan 0.1000 0.0002 ## 60 0.0485 nan 0.1000 -0.0004 ## 80 0.0296 nan 0.1000 -0.0002 ## 100 0.0194 nan 0.1000 -0.0001 ## 120 0.0136 nan 0.1000 -0.0002 ## 140 0.0107 nan 0.1000 -0.0003 ## 160 0.0082 nan 0.1000 -0.0003 ## 180 0.0060 nan 0.1000 0.0000 ## 200 0.0040 nan 0.1000 -0.0001 ## 220 0.0028 nan 0.1000 -0.0001 ## 240 0.0021 nan 0.1000 -0.0000 ## 260 0.0016 nan 0.1000 -0.0000 ## 280 0.0012 nan 0.1000 0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0006 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1345 nan 0.1000 0.0759 ## 2 1.0035 nan 0.1000 0.0667 ## 3 0.8998 nan 0.1000 0.0497 ## 4 0.8138 nan 0.1000 0.0401 ## 5 0.7339 nan 0.1000 0.0358 ## 6 0.6653 nan 0.1000 0.0300 ## 7 0.6077 nan 0.1000 0.0244 ## 8 0.5631 nan 0.1000 0.0177 ## 9 0.5199 nan 0.1000 0.0198 ## 10 0.4820 nan 0.1000 0.0165 ## 20 0.2487 nan 0.1000 0.0050 ## 40 0.0998 nan 0.1000 -0.0003 ## 60 0.0496 nan 0.1000 0.0001 ## 80 0.0320 nan 0.1000 -0.0005 ## 100 0.0201 nan 0.1000 -0.0002 ## 120 0.0123 nan 0.1000 -0.0002 ## 140 0.0090 nan 0.1000 -0.0001 ## 160 0.0061 nan 0.1000 -0.0001 ## 180 0.0046 nan 0.1000 -0.0001 ## 200 0.0036 nan 0.1000 -0.0001 ## 220 0.0025 nan 0.1000 -0.0001 ## 240 0.0021 nan 0.1000 -0.0000 ## 260 0.0015 nan 0.1000 -0.0000 ## 280 0.0010 nan 0.1000 -0.0000 ## 300 0.0007 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 -0.0000 ## 340 0.0004 nan 0.1000 -0.0000 ## 360 0.0003 nan 0.1000 0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1463 nan 0.1000 0.0652 ## 2 1.0342 nan 0.1000 0.0550 ## 3 0.9444 nan 0.1000 0.0453 ## 4 0.8684 nan 0.1000 0.0381 ## 5 0.8040 nan 0.1000 0.0321 ## 6 0.7465 nan 0.1000 0.0260 ## 7 0.6950 nan 0.1000 0.0241 ## 8 0.6533 nan 0.1000 0.0207 ## 9 0.6164 nan 0.1000 0.0183 ## 10 0.5806 nan 0.1000 0.0173 ## 20 0.3862 nan 0.1000 0.0055 ## 40 0.2442 nan 0.1000 -0.0012 ## 60 0.1975 nan 0.1000 -0.0001 ## 80 0.1744 nan 0.1000 -0.0005 ## 100 0.1568 nan 0.1000 -0.0007 ## 120 0.1465 nan 0.1000 -0.0011 ## 140 0.1346 nan 0.1000 -0.0013 ## 160 0.1249 nan 0.1000 -0.0020 ## 180 0.1208 nan 0.1000 -0.0011 ## 200 0.1147 nan 0.1000 -0.0003 ## 220 0.1088 nan 0.1000 -0.0007 ## 240 0.1043 nan 0.1000 -0.0006 ## 260 0.1005 nan 0.1000 -0.0006 ## 280 0.0960 nan 0.1000 -0.0012 ## 300 0.0923 nan 0.1000 -0.0007 ## 320 0.0919 nan 0.1000 -0.0009 ## 340 0.0888 nan 0.1000 -0.0004 ## 360 0.0870 nan 0.1000 -0.0005 ## 380 0.0856 nan 0.1000 -0.0004 ## 400 0.0836 nan 0.1000 -0.0013 ## 420 0.0814 nan 0.1000 -0.0004 ## 440 0.0804 nan 0.1000 -0.0010 ## 460 0.0796 nan 0.1000 -0.0007 ## 480 0.0785 nan 0.1000 -0.0005 ## 500 0.0761 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1367 nan 0.1000 0.0690 ## 2 1.0159 nan 0.1000 0.0581 ## 3 0.9179 nan 0.1000 0.0474 ## 4 0.8367 nan 0.1000 0.0380 ## 5 0.7651 nan 0.1000 0.0339 ## 6 0.7007 nan 0.1000 0.0311 ## 7 0.6490 nan 0.1000 0.0229 ## 8 0.6037 nan 0.1000 0.0213 ## 9 0.5680 nan 0.1000 0.0180 ## 10 0.5275 nan 0.1000 0.0175 ## 20 0.3221 nan 0.1000 0.0042 ## 40 0.1933 nan 0.1000 -0.0019 ## 60 0.1488 nan 0.1000 -0.0002 ## 80 0.1190 nan 0.1000 -0.0003 ## 100 0.1011 nan 0.1000 -0.0008 ## 120 0.0830 nan 0.1000 0.0000 ## 140 0.0722 nan 0.1000 -0.0002 ## 160 0.0639 nan 0.1000 -0.0011 ## 180 0.0576 nan 0.1000 -0.0006 ## 200 0.0506 nan 0.1000 -0.0006 ## 220 0.0457 nan 0.1000 -0.0005 ## 240 0.0421 nan 0.1000 -0.0005 ## 260 0.0377 nan 0.1000 -0.0005 ## 280 0.0341 nan 0.1000 -0.0005 ## 300 0.0306 nan 0.1000 -0.0006 ## 320 0.0274 nan 0.1000 -0.0002 ## 340 0.0257 nan 0.1000 -0.0001 ## 360 0.0232 nan 0.1000 -0.0002 ## 380 0.0213 nan 0.1000 -0.0002 ## 400 0.0193 nan 0.1000 -0.0002 ## 420 0.0177 nan 0.1000 -0.0001 ## 440 0.0162 nan 0.1000 -0.0001 ## 460 0.0147 nan 0.1000 -0.0001 ## 480 0.0130 nan 0.1000 -0.0001 ## 500 0.0116 nan 0.1000 -0.0001 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1393 nan 0.1000 0.0704 ## 2 1.0126 nan 0.1000 0.0623 ## 3 0.9160 nan 0.1000 0.0480 ## 4 0.8293 nan 0.1000 0.0411 ## 5 0.7581 nan 0.1000 0.0336 ## 6 0.7001 nan 0.1000 0.0254 ## 7 0.6461 nan 0.1000 0.0257 ## 8 0.5928 nan 0.1000 0.0251 ## 9 0.5498 nan 0.1000 0.0197 ## 10 0.5110 nan 0.1000 0.0183 ## 20 0.2908 nan 0.1000 0.0038 ## 40 0.1528 nan 0.1000 -0.0002 ## 60 0.1092 nan 0.1000 -0.0011 ## 80 0.0837 nan 0.1000 -0.0004 ## 100 0.0675 nan 0.1000 -0.0015 ## 120 0.0551 nan 0.1000 -0.0005 ## 140 0.0458 nan 0.1000 -0.0003 ## 160 0.0369 nan 0.1000 -0.0004 ## 180 0.0298 nan 0.1000 -0.0005 ## 200 0.0244 nan 0.1000 -0.0002 ## 220 0.0207 nan 0.1000 -0.0001 ## 240 0.0173 nan 0.1000 -0.0002 ## 260 0.0147 nan 0.1000 -0.0001 ## 280 0.0126 nan 0.1000 -0.0001 ## 300 0.0108 nan 0.1000 -0.0001 ## 320 0.0097 nan 0.1000 -0.0001 ## 340 0.0080 nan 0.1000 -0.0000 ## 360 0.0068 nan 0.1000 -0.0001 ## 380 0.0058 nan 0.1000 -0.0001 ## 400 0.0049 nan 0.1000 0.0000 ## 420 0.0041 nan 0.1000 -0.0000 ## 440 0.0035 nan 0.1000 -0.0000 ## 460 0.0030 nan 0.1000 -0.0000 ## 480 0.0026 nan 0.1000 -0.0000 ## 500 0.0022 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1349 nan 0.1000 0.0692 ## 2 1.0107 nan 0.1000 0.0604 ## 3 0.9070 nan 0.1000 0.0479 ## 4 0.8205 nan 0.1000 0.0379 ## 5 0.7520 nan 0.1000 0.0313 ## 6 0.6897 nan 0.1000 0.0292 ## 7 0.6326 nan 0.1000 0.0264 ## 8 0.5820 nan 0.1000 0.0223 ## 9 0.5413 nan 0.1000 0.0183 ## 10 0.5059 nan 0.1000 0.0143 ## 20 0.2819 nan 0.1000 0.0051 ## 40 0.1432 nan 0.1000 0.0002 ## 60 0.0929 nan 0.1000 -0.0008 ## 80 0.0662 nan 0.1000 -0.0010 ## 100 0.0470 nan 0.1000 -0.0003 ## 120 0.0355 nan 0.1000 -0.0000 ## 140 0.0266 nan 0.1000 -0.0004 ## 160 0.0202 nan 0.1000 -0.0000 ## 180 0.0161 nan 0.1000 -0.0001 ## 200 0.0125 nan 0.1000 -0.0002 ## 220 0.0100 nan 0.1000 -0.0002 ## 240 0.0079 nan 0.1000 -0.0000 ## 260 0.0065 nan 0.1000 -0.0000 ## 280 0.0051 nan 0.1000 -0.0000 ## 300 0.0041 nan 0.1000 -0.0001 ## 320 0.0033 nan 0.1000 -0.0000 ## 340 0.0026 nan 0.1000 -0.0000 ## 360 0.0021 nan 0.1000 -0.0000 ## 380 0.0018 nan 0.1000 -0.0000 ## 400 0.0014 nan 0.1000 -0.0000 ## 420 0.0011 nan 0.1000 -0.0000 ## 440 0.0009 nan 0.1000 -0.0000 ## 460 0.0008 nan 0.1000 -0.0000 ## 480 0.0006 nan 0.1000 -0.0000 ## 500 0.0006 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1244 nan 0.1000 0.0809 ## 2 1.0035 nan 0.1000 0.0611 ## 3 0.9046 nan 0.1000 0.0493 ## 4 0.8213 nan 0.1000 0.0406 ## 5 0.7470 nan 0.1000 0.0335 ## 6 0.6871 nan 0.1000 0.0255 ## 7 0.6310 nan 0.1000 0.0283 ## 8 0.5837 nan 0.1000 0.0198 ## 9 0.5376 nan 0.1000 0.0211 ## 10 0.5012 nan 0.1000 0.0132 ## 20 0.2792 nan 0.1000 0.0051 ## 40 0.1339 nan 0.1000 -0.0007 ## 60 0.0862 nan 0.1000 -0.0009 ## 80 0.0562 nan 0.1000 -0.0005 ## 100 0.0398 nan 0.1000 -0.0001 ## 120 0.0278 nan 0.1000 -0.0000 ## 140 0.0207 nan 0.1000 -0.0004 ## 160 0.0153 nan 0.1000 -0.0002 ## 180 0.0125 nan 0.1000 -0.0001 ## 200 0.0089 nan 0.1000 -0.0001 ## 220 0.0069 nan 0.1000 0.0000 ## 240 0.0055 nan 0.1000 -0.0001 ## 260 0.0040 nan 0.1000 -0.0000 ## 280 0.0032 nan 0.1000 -0.0001 ## 300 0.0026 nan 0.1000 -0.0000 ## 320 0.0021 nan 0.1000 -0.0000 ## 340 0.0016 nan 0.1000 -0.0000 ## 360 0.0013 nan 0.1000 -0.0000 ## 380 0.0010 nan 0.1000 -0.0000 ## 400 0.0008 nan 0.1000 -0.0000 ## 420 0.0006 nan 0.1000 -0.0000 ## 440 0.0005 nan 0.1000 -0.0000 ## 460 0.0004 nan 0.1000 0.0000 ## 480 0.0003 nan 0.1000 -0.0000 ## 500 0.0002 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1337 nan 0.1000 0.0750 ## 2 1.0115 nan 0.1000 0.0596 ## 3 0.9078 nan 0.1000 0.0458 ## 4 0.8246 nan 0.1000 0.0393 ## 5 0.7494 nan 0.1000 0.0343 ## 6 0.6828 nan 0.1000 0.0343 ## 7 0.6270 nan 0.1000 0.0241 ## 8 0.5763 nan 0.1000 0.0226 ## 9 0.5320 nan 0.1000 0.0184 ## 10 0.4936 nan 0.1000 0.0164 ## 20 0.2718 nan 0.1000 0.0026 ## 40 0.1226 nan 0.1000 -0.0000 ## 60 0.0705 nan 0.1000 -0.0003 ## 80 0.0464 nan 0.1000 -0.0007 ## 100 0.0303 nan 0.1000 -0.0001 ## 120 0.0216 nan 0.1000 -0.0003 ## 140 0.0150 nan 0.1000 0.0000 ## 160 0.0108 nan 0.1000 -0.0001 ## 180 0.0077 nan 0.1000 -0.0001 ## 200 0.0065 nan 0.1000 -0.0001 ## 220 0.0043 nan 0.1000 -0.0000 ## 240 0.0034 nan 0.1000 -0.0001 ## 260 0.0024 nan 0.1000 -0.0000 ## 280 0.0018 nan 0.1000 -0.0000 ## 300 0.0013 nan 0.1000 -0.0000 ## 320 0.0011 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1348 nan 0.1000 0.0735 ## 2 1.0100 nan 0.1000 0.0588 ## 3 0.9099 nan 0.1000 0.0496 ## 4 0.8261 nan 0.1000 0.0387 ## 5 0.7515 nan 0.1000 0.0358 ## 6 0.6889 nan 0.1000 0.0292 ## 7 0.6359 nan 0.1000 0.0250 ## 8 0.5875 nan 0.1000 0.0213 ## 9 0.5424 nan 0.1000 0.0186 ## 10 0.5021 nan 0.1000 0.0167 ## 20 0.2723 nan 0.1000 0.0039 ## 40 0.1227 nan 0.1000 -0.0002 ## 60 0.0722 nan 0.1000 -0.0002 ## 80 0.0462 nan 0.1000 -0.0004 ## 100 0.0300 nan 0.1000 -0.0006 ## 120 0.0195 nan 0.1000 -0.0003 ## 140 0.0131 nan 0.1000 -0.0001 ## 160 0.0093 nan 0.1000 -0.0001 ## 180 0.0066 nan 0.1000 -0.0002 ## 200 0.0050 nan 0.1000 -0.0001 ## 220 0.0040 nan 0.1000 -0.0001 ## 240 0.0023 nan 0.1000 -0.0000 ## 260 0.0016 nan 0.1000 -0.0000 ## 280 0.0012 nan 0.1000 -0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1304 nan 0.1000 0.0791 ## 2 1.0064 nan 0.1000 0.0616 ## 3 0.9016 nan 0.1000 0.0474 ## 4 0.8138 nan 0.1000 0.0432 ## 5 0.7401 nan 0.1000 0.0356 ## 6 0.6764 nan 0.1000 0.0305 ## 7 0.6219 nan 0.1000 0.0246 ## 8 0.5741 nan 0.1000 0.0222 ## 9 0.5332 nan 0.1000 0.0175 ## 10 0.4967 nan 0.1000 0.0156 ## 20 0.2779 nan 0.1000 0.0029 ## 40 0.1226 nan 0.1000 0.0011 ## 60 0.0673 nan 0.1000 -0.0007 ## 80 0.0416 nan 0.1000 -0.0005 ## 100 0.0285 nan 0.1000 -0.0005 ## 120 0.0192 nan 0.1000 -0.0004 ## 140 0.0138 nan 0.1000 -0.0002 ## 160 0.0111 nan 0.1000 -0.0004 ## 180 0.0074 nan 0.1000 -0.0000 ## 200 0.0053 nan 0.1000 -0.0001 ## 220 0.0044 nan 0.1000 -0.0002 ## 240 0.0037 nan 0.1000 -0.0000 ## 260 0.0028 nan 0.1000 -0.0001 ## 280 0.0020 nan 0.1000 -0.0000 ## 300 0.0013 nan 0.1000 -0.0000 ## 320 0.0009 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0006 nan 0.1000 -0.0000 ## 400 0.0005 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0003 nan 0.1000 0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0002 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1304 nan 0.1000 0.0727 ## 2 1.0121 nan 0.1000 0.0582 ## 3 0.9117 nan 0.1000 0.0507 ## 4 0.8243 nan 0.1000 0.0427 ## 5 0.7510 nan 0.1000 0.0332 ## 6 0.6856 nan 0.1000 0.0305 ## 7 0.6327 nan 0.1000 0.0237 ## 8 0.5842 nan 0.1000 0.0210 ## 9 0.5379 nan 0.1000 0.0211 ## 10 0.5016 nan 0.1000 0.0171 ## 20 0.2693 nan 0.1000 0.0052 ## 40 0.1153 nan 0.1000 0.0004 ## 60 0.0604 nan 0.1000 -0.0011 ## 80 0.0375 nan 0.1000 -0.0002 ## 100 0.0245 nan 0.1000 -0.0003 ## 120 0.0164 nan 0.1000 -0.0006 ## 140 0.0108 nan 0.1000 -0.0001 ## 160 0.0081 nan 0.1000 -0.0002 ## 180 0.0056 nan 0.1000 -0.0001 ## 200 0.0038 nan 0.1000 -0.0001 ## 220 0.0026 nan 0.1000 -0.0001 ## 240 0.0018 nan 0.1000 -0.0000 ## 260 0.0014 nan 0.1000 -0.0000 ## 280 0.0009 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1335 nan 0.1000 0.0747 ## 2 1.0122 nan 0.1000 0.0595 ## 3 0.9150 nan 0.1000 0.0479 ## 4 0.8230 nan 0.1000 0.0435 ## 5 0.7477 nan 0.1000 0.0336 ## 6 0.6883 nan 0.1000 0.0286 ## 7 0.6328 nan 0.1000 0.0233 ## 8 0.5826 nan 0.1000 0.0239 ## 9 0.5380 nan 0.1000 0.0212 ## 10 0.4963 nan 0.1000 0.0202 ## 20 0.2634 nan 0.1000 0.0058 ## 40 0.1070 nan 0.1000 -0.0011 ## 60 0.0553 nan 0.1000 -0.0001 ## 80 0.0353 nan 0.1000 -0.0003 ## 100 0.0256 nan 0.1000 -0.0006 ## 120 0.0167 nan 0.1000 -0.0002 ## 140 0.0114 nan 0.1000 -0.0004 ## 160 0.0083 nan 0.1000 -0.0000 ## 180 0.0056 nan 0.1000 -0.0000 ## 200 0.0038 nan 0.1000 0.0000 ## 220 0.0029 nan 0.1000 -0.0000 ## 240 0.0025 nan 0.1000 -0.0000 ## 260 0.0016 nan 0.1000 -0.0000 ## 280 0.0012 nan 0.1000 -0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 0.0000 ## 340 0.0005 nan 0.1000 0.0000 ## 360 0.0003 nan 0.1000 -0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0001 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1672 nan 0.1000 0.0564 ## 2 1.0603 nan 0.1000 0.0543 ## 3 0.9636 nan 0.1000 0.0452 ## 4 0.8785 nan 0.1000 0.0405 ## 5 0.8108 nan 0.1000 0.0341 ## 6 0.7523 nan 0.1000 0.0273 ## 7 0.7144 nan 0.1000 0.0156 ## 8 0.6715 nan 0.1000 0.0199 ## 9 0.6278 nan 0.1000 0.0205 ## 10 0.5939 nan 0.1000 0.0161 ## 20 0.3797 nan 0.1000 0.0051 ## 40 0.2421 nan 0.1000 0.0007 ## 60 0.1969 nan 0.1000 -0.0014 ## 80 0.1797 nan 0.1000 -0.0004 ## 100 0.1635 nan 0.1000 -0.0017 ## 120 0.1491 nan 0.1000 -0.0014 ## 140 0.1417 nan 0.1000 -0.0015 ## 160 0.1361 nan 0.1000 -0.0005 ## 180 0.1312 nan 0.1000 -0.0022 ## 200 0.1242 nan 0.1000 -0.0005 ## 220 0.1216 nan 0.1000 -0.0011 ## 240 0.1192 nan 0.1000 -0.0012 ## 260 0.1166 nan 0.1000 -0.0004 ## 280 0.1128 nan 0.1000 -0.0005 ## 300 0.1107 nan 0.1000 -0.0007 ## 320 0.1084 nan 0.1000 -0.0006 ## 340 0.1085 nan 0.1000 -0.0015 ## 360 0.1039 nan 0.1000 -0.0006 ## 380 0.1031 nan 0.1000 -0.0013 ## 400 0.1028 nan 0.1000 -0.0018 ## 420 0.0991 nan 0.1000 -0.0016 ## 440 0.0997 nan 0.1000 -0.0011 ## 460 0.0971 nan 0.1000 -0.0002 ## 480 0.0960 nan 0.1000 -0.0005 ## 500 0.0955 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1491 nan 0.1000 0.0640 ## 2 1.0313 nan 0.1000 0.0579 ## 3 0.9290 nan 0.1000 0.0491 ## 4 0.8475 nan 0.1000 0.0415 ## 5 0.7828 nan 0.1000 0.0330 ## 6 0.7193 nan 0.1000 0.0292 ## 7 0.6685 nan 0.1000 0.0250 ## 8 0.6244 nan 0.1000 0.0199 ## 9 0.5826 nan 0.1000 0.0192 ## 10 0.5412 nan 0.1000 0.0196 ## 20 0.3160 nan 0.1000 0.0050 ## 40 0.1956 nan 0.1000 -0.0024 ## 60 0.1517 nan 0.1000 -0.0015 ## 80 0.1246 nan 0.1000 -0.0009 ## 100 0.1079 nan 0.1000 -0.0010 ## 120 0.0911 nan 0.1000 -0.0010 ## 140 0.0796 nan 0.1000 -0.0008 ## 160 0.0713 nan 0.1000 -0.0011 ## 180 0.0642 nan 0.1000 -0.0001 ## 200 0.0555 nan 0.1000 -0.0000 ## 220 0.0509 nan 0.1000 -0.0009 ## 240 0.0448 nan 0.1000 -0.0003 ## 260 0.0418 nan 0.1000 -0.0005 ## 280 0.0372 nan 0.1000 -0.0003 ## 300 0.0339 nan 0.1000 -0.0002 ## 320 0.0312 nan 0.1000 -0.0002 ## 340 0.0284 nan 0.1000 -0.0002 ## 360 0.0260 nan 0.1000 -0.0002 ## 380 0.0240 nan 0.1000 -0.0002 ## 400 0.0223 nan 0.1000 -0.0001 ## 420 0.0208 nan 0.1000 -0.0001 ## 440 0.0194 nan 0.1000 -0.0002 ## 460 0.0182 nan 0.1000 -0.0001 ## 480 0.0170 nan 0.1000 -0.0001 ## 500 0.0154 nan 0.1000 -0.0002 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1390 nan 0.1000 0.0718 ## 2 1.0173 nan 0.1000 0.0584 ## 3 0.9175 nan 0.1000 0.0492 ## 4 0.8303 nan 0.1000 0.0419 ## 5 0.7591 nan 0.1000 0.0341 ## 6 0.6968 nan 0.1000 0.0300 ## 7 0.6442 nan 0.1000 0.0249 ## 8 0.5967 nan 0.1000 0.0214 ## 9 0.5527 nan 0.1000 0.0201 ## 10 0.5113 nan 0.1000 0.0183 ## 20 0.2895 nan 0.1000 0.0030 ## 40 0.1563 nan 0.1000 0.0004 ## 60 0.1143 nan 0.1000 -0.0011 ## 80 0.0909 nan 0.1000 -0.0011 ## 100 0.0727 nan 0.1000 0.0002 ## 120 0.0593 nan 0.1000 -0.0004 ## 140 0.0459 nan 0.1000 -0.0006 ## 160 0.0385 nan 0.1000 -0.0002 ## 180 0.0317 nan 0.1000 -0.0001 ## 200 0.0265 nan 0.1000 -0.0001 ## 220 0.0223 nan 0.1000 -0.0002 ## 240 0.0185 nan 0.1000 -0.0002 ## 260 0.0160 nan 0.1000 -0.0001 ## 280 0.0135 nan 0.1000 -0.0002 ## 300 0.0116 nan 0.1000 -0.0001 ## 320 0.0106 nan 0.1000 -0.0001 ## 340 0.0090 nan 0.1000 -0.0001 ## 360 0.0079 nan 0.1000 -0.0001 ## 380 0.0069 nan 0.1000 -0.0001 ## 400 0.0060 nan 0.1000 -0.0000 ## 420 0.0051 nan 0.1000 -0.0000 ## 440 0.0043 nan 0.1000 -0.0000 ## 460 0.0037 nan 0.1000 -0.0000 ## 480 0.0032 nan 0.1000 -0.0001 ## 500 0.0028 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1380 nan 0.1000 0.0739 ## 2 1.0122 nan 0.1000 0.0624 ## 3 0.9070 nan 0.1000 0.0495 ## 4 0.8218 nan 0.1000 0.0407 ## 5 0.7438 nan 0.1000 0.0360 ## 6 0.6787 nan 0.1000 0.0311 ## 7 0.6260 nan 0.1000 0.0253 ## 8 0.5797 nan 0.1000 0.0214 ## 9 0.5373 nan 0.1000 0.0184 ## 10 0.5047 nan 0.1000 0.0143 ## 20 0.2770 nan 0.1000 0.0052 ## 40 0.1391 nan 0.1000 0.0005 ## 60 0.0904 nan 0.1000 -0.0004 ## 80 0.0656 nan 0.1000 -0.0010 ## 100 0.0477 nan 0.1000 -0.0004 ## 120 0.0366 nan 0.1000 -0.0004 ## 140 0.0273 nan 0.1000 -0.0000 ## 160 0.0218 nan 0.1000 -0.0006 ## 180 0.0176 nan 0.1000 -0.0001 ## 200 0.0137 nan 0.1000 -0.0001 ## 220 0.0114 nan 0.1000 -0.0001 ## 240 0.0095 nan 0.1000 -0.0001 ## 260 0.0078 nan 0.1000 -0.0001 ## 280 0.0063 nan 0.1000 -0.0001 ## 300 0.0052 nan 0.1000 -0.0001 ## 320 0.0042 nan 0.1000 -0.0000 ## 340 0.0033 nan 0.1000 -0.0000 ## 360 0.0027 nan 0.1000 -0.0000 ## 380 0.0022 nan 0.1000 -0.0000 ## 400 0.0018 nan 0.1000 -0.0000 ## 420 0.0015 nan 0.1000 -0.0000 ## 440 0.0012 nan 0.1000 -0.0000 ## 460 0.0010 nan 0.1000 -0.0000 ## 480 0.0008 nan 0.1000 -0.0000 ## 500 0.0007 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1361 nan 0.1000 0.0735 ## 2 1.0105 nan 0.1000 0.0628 ## 3 0.9093 nan 0.1000 0.0493 ## 4 0.8260 nan 0.1000 0.0402 ## 5 0.7538 nan 0.1000 0.0338 ## 6 0.6921 nan 0.1000 0.0296 ## 7 0.6380 nan 0.1000 0.0244 ## 8 0.5886 nan 0.1000 0.0229 ## 9 0.5461 nan 0.1000 0.0178 ## 10 0.5031 nan 0.1000 0.0182 ## 20 0.2787 nan 0.1000 0.0024 ## 40 0.1288 nan 0.1000 -0.0009 ## 60 0.0781 nan 0.1000 -0.0003 ## 80 0.0530 nan 0.1000 -0.0003 ## 100 0.0359 nan 0.1000 -0.0004 ## 120 0.0261 nan 0.1000 -0.0005 ## 140 0.0208 nan 0.1000 -0.0001 ## 160 0.0162 nan 0.1000 -0.0003 ## 180 0.0119 nan 0.1000 -0.0000 ## 200 0.0091 nan 0.1000 -0.0000 ## 220 0.0068 nan 0.1000 -0.0001 ## 240 0.0056 nan 0.1000 -0.0001 ## 260 0.0043 nan 0.1000 -0.0001 ## 280 0.0032 nan 0.1000 -0.0000 ## 300 0.0025 nan 0.1000 -0.0000 ## 320 0.0021 nan 0.1000 -0.0000 ## 340 0.0018 nan 0.1000 -0.0000 ## 360 0.0015 nan 0.1000 -0.0000 ## 380 0.0011 nan 0.1000 -0.0000 ## 400 0.0009 nan 0.1000 -0.0000 ## 420 0.0007 nan 0.1000 -0.0000 ## 440 0.0005 nan 0.1000 -0.0000 ## 460 0.0004 nan 0.1000 -0.0000 ## 480 0.0003 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1312 nan 0.1000 0.0757 ## 2 1.0038 nan 0.1000 0.0626 ## 3 0.9045 nan 0.1000 0.0462 ## 4 0.8277 nan 0.1000 0.0321 ## 5 0.7513 nan 0.1000 0.0355 ## 6 0.6904 nan 0.1000 0.0273 ## 7 0.6364 nan 0.1000 0.0233 ## 8 0.5904 nan 0.1000 0.0195 ## 9 0.5493 nan 0.1000 0.0177 ## 10 0.5145 nan 0.1000 0.0136 ## 20 0.2801 nan 0.1000 0.0065 ## 40 0.1282 nan 0.1000 -0.0007 ## 60 0.0780 nan 0.1000 -0.0011 ## 80 0.0543 nan 0.1000 -0.0005 ## 100 0.0413 nan 0.1000 -0.0008 ## 120 0.0270 nan 0.1000 -0.0002 ## 140 0.0183 nan 0.1000 -0.0002 ## 160 0.0142 nan 0.1000 -0.0004 ## 180 0.0110 nan 0.1000 -0.0001 ## 200 0.0078 nan 0.1000 -0.0002 ## 220 0.0061 nan 0.1000 -0.0001 ## 240 0.0046 nan 0.1000 -0.0001 ## 260 0.0035 nan 0.1000 -0.0000 ## 280 0.0028 nan 0.1000 -0.0000 ## 300 0.0022 nan 0.1000 0.0000 ## 320 0.0017 nan 0.1000 -0.0000 ## 340 0.0012 nan 0.1000 -0.0000 ## 360 0.0009 nan 0.1000 -0.0000 ## 380 0.0007 nan 0.1000 -0.0000 ## 400 0.0006 nan 0.1000 -0.0000 ## 420 0.0004 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0003 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0002 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1331 nan 0.1000 0.0763 ## 2 1.0114 nan 0.1000 0.0600 ## 3 0.9073 nan 0.1000 0.0494 ## 4 0.8249 nan 0.1000 0.0370 ## 5 0.7560 nan 0.1000 0.0323 ## 6 0.6937 nan 0.1000 0.0306 ## 7 0.6357 nan 0.1000 0.0247 ## 8 0.5864 nan 0.1000 0.0221 ## 9 0.5455 nan 0.1000 0.0174 ## 10 0.5062 nan 0.1000 0.0179 ## 20 0.2692 nan 0.1000 0.0048 ## 40 0.1150 nan 0.1000 -0.0012 ## 60 0.0692 nan 0.1000 -0.0002 ## 80 0.0454 nan 0.1000 -0.0011 ## 100 0.0309 nan 0.1000 -0.0006 ## 120 0.0233 nan 0.1000 -0.0004 ## 140 0.0173 nan 0.1000 -0.0004 ## 160 0.0139 nan 0.1000 -0.0004 ## 180 0.0085 nan 0.1000 0.0000 ## 200 0.0066 nan 0.1000 -0.0002 ## 220 0.0054 nan 0.1000 -0.0002 ## 240 0.0040 nan 0.1000 -0.0002 ## 260 0.0030 nan 0.1000 -0.0001 ## 280 0.0020 nan 0.1000 -0.0000 ## 300 0.0015 nan 0.1000 -0.0001 ## 320 0.0011 nan 0.1000 -0.0000 ## 340 0.0009 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1371 nan 0.1000 0.0716 ## 2 1.0154 nan 0.1000 0.0542 ## 3 0.9140 nan 0.1000 0.0434 ## 4 0.8200 nan 0.1000 0.0431 ## 5 0.7398 nan 0.1000 0.0370 ## 6 0.6788 nan 0.1000 0.0273 ## 7 0.6220 nan 0.1000 0.0266 ## 8 0.5774 nan 0.1000 0.0162 ## 9 0.5341 nan 0.1000 0.0190 ## 10 0.4956 nan 0.1000 0.0169 ## 20 0.2704 nan 0.1000 0.0045 ## 40 0.1160 nan 0.1000 -0.0005 ## 60 0.0646 nan 0.1000 -0.0001 ## 80 0.0383 nan 0.1000 0.0000 ## 100 0.0238 nan 0.1000 -0.0001 ## 120 0.0160 nan 0.1000 -0.0003 ## 140 0.0103 nan 0.1000 -0.0002 ## 160 0.0068 nan 0.1000 -0.0001 ## 180 0.0045 nan 0.1000 -0.0001 ## 200 0.0034 nan 0.1000 -0.0001 ## 220 0.0027 nan 0.1000 -0.0000 ## 240 0.0020 nan 0.1000 -0.0000 ## 260 0.0015 nan 0.1000 -0.0000 ## 280 0.0011 nan 0.1000 0.0000 ## 300 0.0008 nan 0.1000 -0.0000 ## 320 0.0006 nan 0.1000 -0.0000 ## 340 0.0005 nan 0.1000 -0.0000 ## 360 0.0004 nan 0.1000 -0.0000 ## 380 0.0003 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1364 nan 0.1000 0.0763 ## 2 1.0169 nan 0.1000 0.0565 ## 3 0.9170 nan 0.1000 0.0505 ## 4 0.8279 nan 0.1000 0.0433 ## 5 0.7562 nan 0.1000 0.0324 ## 6 0.6882 nan 0.1000 0.0316 ## 7 0.6320 nan 0.1000 0.0272 ## 8 0.5828 nan 0.1000 0.0220 ## 9 0.5389 nan 0.1000 0.0183 ## 10 0.5006 nan 0.1000 0.0153 ## 20 0.2701 nan 0.1000 0.0055 ## 40 0.1142 nan 0.1000 -0.0006 ## 60 0.0661 nan 0.1000 0.0007 ## 80 0.0419 nan 0.1000 -0.0005 ## 100 0.0300 nan 0.1000 -0.0001 ## 120 0.0207 nan 0.1000 -0.0003 ## 140 0.0153 nan 0.1000 -0.0003 ## 160 0.0099 nan 0.1000 -0.0002 ## 180 0.0071 nan 0.1000 -0.0002 ## 200 0.0053 nan 0.1000 -0.0000 ## 220 0.0039 nan 0.1000 -0.0002 ## 240 0.0032 nan 0.1000 -0.0001 ## 260 0.0028 nan 0.1000 -0.0001 ## 280 0.0024 nan 0.1000 -0.0001 ## 300 0.0016 nan 0.1000 -0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1327 nan 0.1000 0.0713 ## 2 1.0079 nan 0.1000 0.0563 ## 3 0.9086 nan 0.1000 0.0468 ## 4 0.8241 nan 0.1000 0.0393 ## 5 0.7548 nan 0.1000 0.0333 ## 6 0.6908 nan 0.1000 0.0296 ## 7 0.6416 nan 0.1000 0.0207 ## 8 0.5931 nan 0.1000 0.0209 ## 9 0.5493 nan 0.1000 0.0196 ## 10 0.5120 nan 0.1000 0.0165 ## 20 0.2683 nan 0.1000 0.0055 ## 40 0.1133 nan 0.1000 -0.0001 ## 60 0.0649 nan 0.1000 -0.0005 ## 80 0.0443 nan 0.1000 -0.0004 ## 100 0.0268 nan 0.1000 -0.0003 ## 120 0.0192 nan 0.1000 -0.0003 ## 140 0.0131 nan 0.1000 -0.0002 ## 160 0.0091 nan 0.1000 -0.0000 ## 180 0.0067 nan 0.1000 -0.0001 ## 200 0.0051 nan 0.1000 -0.0001 ## 220 0.0035 nan 0.1000 -0.0001 ## 240 0.0027 nan 0.1000 -0.0001 ## 260 0.0026 nan 0.1000 -0.0001 ## 280 0.0020 nan 0.1000 -0.0000 ## 300 0.0015 nan 0.1000 -0.0000 ## 320 0.0012 nan 0.1000 -0.0000 ## 340 0.0009 nan 0.1000 -0.0000 ## 360 0.0007 nan 0.1000 -0.0000 ## 380 0.0005 nan 0.1000 -0.0000 ## 400 0.0004 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1532 nan 0.1000 0.0666 ## 2 1.0461 nan 0.1000 0.0559 ## 3 0.9626 nan 0.1000 0.0400 ## 4 0.8793 nan 0.1000 0.0399 ## 5 0.8079 nan 0.1000 0.0329 ## 6 0.7485 nan 0.1000 0.0288 ## 7 0.6987 nan 0.1000 0.0245 ## 8 0.6576 nan 0.1000 0.0200 ## 9 0.6185 nan 0.1000 0.0191 ## 10 0.5861 nan 0.1000 0.0146 ## 20 0.3824 nan 0.1000 0.0039 ## 40 0.2417 nan 0.1000 -0.0002 ## 60 0.1922 nan 0.1000 -0.0006 ## 80 0.1700 nan 0.1000 0.0006 ## 100 0.1535 nan 0.1000 -0.0011 ## 120 0.1392 nan 0.1000 -0.0003 ## 140 0.1317 nan 0.1000 -0.0018 ## 160 0.1239 nan 0.1000 -0.0003 ## 180 0.1146 nan 0.1000 -0.0006 ## 200 0.1065 nan 0.1000 -0.0010 ## 220 0.1021 nan 0.1000 -0.0008 ## 240 0.0974 nan 0.1000 -0.0006 ## 260 0.0950 nan 0.1000 -0.0008 ## 280 0.0900 nan 0.1000 -0.0010 ## 300 0.0869 nan 0.1000 -0.0007 ## 320 0.0843 nan 0.1000 -0.0002 ## 340 0.0836 nan 0.1000 -0.0006 ## 360 0.0831 nan 0.1000 -0.0012 ## 380 0.0811 nan 0.1000 -0.0006 ## 400 0.0780 nan 0.1000 -0.0006 ## 420 0.0779 nan 0.1000 -0.0007 ## 440 0.0767 nan 0.1000 -0.0006 ## 460 0.0748 nan 0.1000 -0.0008 ## 480 0.0740 nan 0.1000 -0.0006 ## 500 0.0723 nan 0.1000 -0.0007 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1418 nan 0.1000 0.0704 ## 2 1.0277 nan 0.1000 0.0553 ## 3 0.9270 nan 0.1000 0.0516 ## 4 0.8436 nan 0.1000 0.0405 ## 5 0.7738 nan 0.1000 0.0338 ## 6 0.7104 nan 0.1000 0.0301 ## 7 0.6571 nan 0.1000 0.0242 ## 8 0.6098 nan 0.1000 0.0247 ## 9 0.5725 nan 0.1000 0.0170 ## 10 0.5419 nan 0.1000 0.0152 ## 20 0.3151 nan 0.1000 0.0038 ## 40 0.1829 nan 0.1000 0.0002 ## 60 0.1382 nan 0.1000 -0.0012 ## 80 0.1132 nan 0.1000 -0.0006 ## 100 0.0917 nan 0.1000 -0.0006 ## 120 0.0783 nan 0.1000 -0.0001 ## 140 0.0684 nan 0.1000 -0.0004 ## 160 0.0601 nan 0.1000 -0.0003 ## 180 0.0538 nan 0.1000 -0.0008 ## 200 0.0472 nan 0.1000 -0.0002 ## 220 0.0426 nan 0.1000 -0.0001 ## 240 0.0386 nan 0.1000 -0.0006 ## 260 0.0358 nan 0.1000 -0.0003 ## 280 0.0320 nan 0.1000 -0.0001 ## 300 0.0289 nan 0.1000 -0.0003 ## 320 0.0258 nan 0.1000 -0.0002 ## 340 0.0234 nan 0.1000 -0.0001 ## 360 0.0207 nan 0.1000 -0.0003 ## 380 0.0191 nan 0.1000 -0.0002 ## 400 0.0170 nan 0.1000 -0.0001 ## 420 0.0157 nan 0.1000 -0.0001 ## 440 0.0142 nan 0.1000 -0.0001 ## 460 0.0130 nan 0.1000 0.0000 ## 480 0.0125 nan 0.1000 -0.0001 ## 500 0.0113 nan 0.1000 -0.0001 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1356 nan 0.1000 0.0740 ## 2 1.0178 nan 0.1000 0.0555 ## 3 0.9133 nan 0.1000 0.0475 ## 4 0.8312 nan 0.1000 0.0384 ## 5 0.7560 nan 0.1000 0.0350 ## 6 0.6933 nan 0.1000 0.0306 ## 7 0.6368 nan 0.1000 0.0238 ## 8 0.5925 nan 0.1000 0.0198 ## 9 0.5453 nan 0.1000 0.0214 ## 10 0.5085 nan 0.1000 0.0165 ## 20 0.2886 nan 0.1000 0.0054 ## 40 0.1546 nan 0.1000 -0.0012 ## 60 0.1063 nan 0.1000 -0.0013 ## 80 0.0793 nan 0.1000 -0.0008 ## 100 0.0587 nan 0.1000 -0.0004 ## 120 0.0489 nan 0.1000 -0.0001 ## 140 0.0388 nan 0.1000 -0.0004 ## 160 0.0327 nan 0.1000 0.0000 ## 180 0.0249 nan 0.1000 -0.0001 ## 200 0.0205 nan 0.1000 -0.0002 ## 220 0.0166 nan 0.1000 -0.0002 ## 240 0.0135 nan 0.1000 -0.0001 ## 260 0.0113 nan 0.1000 -0.0000 ## 280 0.0095 nan 0.1000 -0.0001 ## 300 0.0080 nan 0.1000 -0.0001 ## 320 0.0068 nan 0.1000 -0.0001 ## 340 0.0058 nan 0.1000 -0.0001 ## 360 0.0050 nan 0.1000 -0.0000 ## 380 0.0044 nan 0.1000 -0.0001 ## 400 0.0034 nan 0.1000 -0.0000 ## 420 0.0029 nan 0.1000 -0.0000 ## 440 0.0025 nan 0.1000 -0.0000 ## 460 0.0021 nan 0.1000 -0.0000 ## 480 0.0018 nan 0.1000 -0.0000 ## 500 0.0015 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1377 nan 0.1000 0.0685 ## 2 1.0159 nan 0.1000 0.0602 ## 3 0.9185 nan 0.1000 0.0473 ## 4 0.8325 nan 0.1000 0.0417 ## 5 0.7618 nan 0.1000 0.0330 ## 6 0.7000 nan 0.1000 0.0279 ## 7 0.6417 nan 0.1000 0.0265 ## 8 0.5892 nan 0.1000 0.0232 ## 9 0.5439 nan 0.1000 0.0208 ## 10 0.5078 nan 0.1000 0.0163 ## 20 0.2918 nan 0.1000 0.0043 ## 40 0.1386 nan 0.1000 -0.0000 ## 60 0.0874 nan 0.1000 0.0003 ## 80 0.0583 nan 0.1000 0.0002 ## 100 0.0417 nan 0.1000 -0.0006 ## 120 0.0309 nan 0.1000 -0.0003 ## 140 0.0233 nan 0.1000 -0.0002 ## 160 0.0183 nan 0.1000 -0.0002 ## 180 0.0138 nan 0.1000 -0.0002 ## 200 0.0105 nan 0.1000 -0.0001 ## 220 0.0082 nan 0.1000 -0.0001 ## 240 0.0062 nan 0.1000 -0.0000 ## 260 0.0050 nan 0.1000 -0.0000 ## 280 0.0040 nan 0.1000 -0.0000 ## 300 0.0031 nan 0.1000 -0.0000 ## 320 0.0026 nan 0.1000 -0.0000 ## 340 0.0021 nan 0.1000 -0.0000 ## 360 0.0017 nan 0.1000 -0.0000 ## 380 0.0012 nan 0.1000 -0.0000 ## 400 0.0010 nan 0.1000 -0.0000 ## 420 0.0008 nan 0.1000 -0.0000 ## 440 0.0006 nan 0.1000 -0.0000 ## 460 0.0005 nan 0.1000 -0.0000 ## 480 0.0004 nan 0.1000 -0.0000 ## 500 0.0003 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1360 nan 0.1000 0.0705 ## 2 1.0126 nan 0.1000 0.0615 ## 3 0.9108 nan 0.1000 0.0513 ## 4 0.8227 nan 0.1000 0.0427 ## 5 0.7506 nan 0.1000 0.0348 ## 6 0.6847 nan 0.1000 0.0313 ## 7 0.6257 nan 0.1000 0.0280 ## 8 0.5799 nan 0.1000 0.0195 ## 9 0.5393 nan 0.1000 0.0174 ## 10 0.5008 nan 0.1000 0.0159 ## 20 0.2728 nan 0.1000 0.0057 ## 40 0.1240 nan 0.1000 0.0005 ## 60 0.0737 nan 0.1000 0.0007 ## 80 0.0497 nan 0.1000 -0.0001 ## 100 0.0324 nan 0.1000 0.0000 ## 120 0.0228 nan 0.1000 -0.0003 ## 140 0.0163 nan 0.1000 -0.0003 ## 160 0.0123 nan 0.1000 -0.0001 ## 180 0.0086 nan 0.1000 -0.0002 ## 200 0.0060 nan 0.1000 -0.0000 ## 220 0.0044 nan 0.1000 0.0000 ## 240 0.0034 nan 0.1000 -0.0000 ## 260 0.0027 nan 0.1000 -0.0001 ## 280 0.0020 nan 0.1000 -0.0000 ## 300 0.0017 nan 0.1000 -0.0000 ## 320 0.0013 nan 0.1000 -0.0000 ## 340 0.0010 nan 0.1000 -0.0000 ## 360 0.0008 nan 0.1000 -0.0000 ## 380 0.0007 nan 0.1000 -0.0000 ## 400 0.0006 nan 0.1000 -0.0000 ## 420 0.0004 nan 0.1000 -0.0000 ## 440 0.0003 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 -0.0000 ## 480 0.0002 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1290 nan 0.1000 0.0749 ## 2 1.0009 nan 0.1000 0.0613 ## 3 0.8983 nan 0.1000 0.0455 ## 4 0.8156 nan 0.1000 0.0380 ## 5 0.7440 nan 0.1000 0.0305 ## 6 0.6813 nan 0.1000 0.0302 ## 7 0.6213 nan 0.1000 0.0267 ## 8 0.5744 nan 0.1000 0.0218 ## 9 0.5279 nan 0.1000 0.0213 ## 10 0.4874 nan 0.1000 0.0166 ## 20 0.2618 nan 0.1000 0.0064 ## 40 0.1119 nan 0.1000 -0.0002 ## 60 0.0637 nan 0.1000 -0.0005 ## 80 0.0386 nan 0.1000 -0.0005 ## 100 0.0262 nan 0.1000 -0.0004 ## 120 0.0194 nan 0.1000 -0.0003 ## 140 0.0157 nan 0.1000 -0.0003 ## 160 0.0113 nan 0.1000 -0.0003 ## 180 0.0091 nan 0.1000 -0.0002 ## 200 0.0062 nan 0.1000 -0.0001 ## 220 0.0039 nan 0.1000 -0.0000 ## 240 0.0031 nan 0.1000 -0.0001 ## 260 0.0025 nan 0.1000 -0.0000 ## 280 0.0019 nan 0.1000 -0.0000 ## 300 0.0014 nan 0.1000 -0.0000 ## 320 0.0010 nan 0.1000 -0.0000 ## 340 0.0008 nan 0.1000 -0.0000 ## 360 0.0006 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0003 nan 0.1000 -0.0000 ## 440 0.0002 nan 0.1000 -0.0000 ## 460 0.0002 nan 0.1000 0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1286 nan 0.1000 0.0796 ## 2 1.0082 nan 0.1000 0.0585 ## 3 0.9066 nan 0.1000 0.0484 ## 4 0.8176 nan 0.1000 0.0411 ## 5 0.7396 nan 0.1000 0.0370 ## 6 0.6809 nan 0.1000 0.0242 ## 7 0.6235 nan 0.1000 0.0265 ## 8 0.5730 nan 0.1000 0.0241 ## 9 0.5302 nan 0.1000 0.0177 ## 10 0.4908 nan 0.1000 0.0188 ## 20 0.2664 nan 0.1000 0.0062 ## 40 0.1146 nan 0.1000 0.0010 ## 60 0.0663 nan 0.1000 -0.0012 ## 80 0.0379 nan 0.1000 -0.0001 ## 100 0.0217 nan 0.1000 -0.0001 ## 120 0.0150 nan 0.1000 -0.0001 ## 140 0.0109 nan 0.1000 -0.0003 ## 160 0.0078 nan 0.1000 -0.0001 ## 180 0.0056 nan 0.1000 -0.0001 ## 200 0.0038 nan 0.1000 -0.0000 ## 220 0.0031 nan 0.1000 -0.0001 ## 240 0.0025 nan 0.1000 -0.0000 ## 260 0.0016 nan 0.1000 -0.0000 ## 280 0.0013 nan 0.1000 -0.0000 ## 300 0.0009 nan 0.1000 -0.0000 ## 320 0.0007 nan 0.1000 -0.0000 ## 340 0.0004 nan 0.1000 -0.0000 ## 360 0.0003 nan 0.1000 -0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0002 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1342 nan 0.1000 0.0766 ## 2 1.0123 nan 0.1000 0.0564 ## 3 0.9102 nan 0.1000 0.0491 ## 4 0.8220 nan 0.1000 0.0423 ## 5 0.7459 nan 0.1000 0.0338 ## 6 0.6778 nan 0.1000 0.0321 ## 7 0.6270 nan 0.1000 0.0237 ## 8 0.5788 nan 0.1000 0.0229 ## 9 0.5365 nan 0.1000 0.0203 ## 10 0.4973 nan 0.1000 0.0188 ## 20 0.2602 nan 0.1000 0.0066 ## 40 0.1053 nan 0.1000 0.0011 ## 60 0.0597 nan 0.1000 -0.0011 ## 80 0.0330 nan 0.1000 -0.0004 ## 100 0.0208 nan 0.1000 -0.0000 ## 120 0.0139 nan 0.1000 -0.0002 ## 140 0.0088 nan 0.1000 -0.0002 ## 160 0.0067 nan 0.1000 -0.0001 ## 180 0.0049 nan 0.1000 -0.0001 ## 200 0.0034 nan 0.1000 0.0000 ## 220 0.0024 nan 0.1000 -0.0001 ## 240 0.0017 nan 0.1000 -0.0000 ## 260 0.0012 nan 0.1000 -0.0000 ## 280 0.0009 nan 0.1000 -0.0000 ## 300 0.0007 nan 0.1000 -0.0000 ## 320 0.0004 nan 0.1000 -0.0000 ## 340 0.0003 nan 0.1000 -0.0000 ## 360 0.0002 nan 0.1000 -0.0000 ## 380 0.0002 nan 0.1000 -0.0000 ## 400 0.0001 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1308 nan 0.1000 0.0723 ## 2 1.0046 nan 0.1000 0.0637 ## 3 0.9038 nan 0.1000 0.0460 ## 4 0.8165 nan 0.1000 0.0394 ## 5 0.7431 nan 0.1000 0.0351 ## 6 0.6783 nan 0.1000 0.0284 ## 7 0.6240 nan 0.1000 0.0265 ## 8 0.5774 nan 0.1000 0.0210 ## 9 0.5381 nan 0.1000 0.0171 ## 10 0.5010 nan 0.1000 0.0159 ## 20 0.2603 nan 0.1000 0.0049 ## 40 0.1087 nan 0.1000 0.0004 ## 60 0.0591 nan 0.1000 -0.0011 ## 80 0.0346 nan 0.1000 -0.0006 ## 100 0.0227 nan 0.1000 0.0002 ## 120 0.0158 nan 0.1000 -0.0004 ## 140 0.0104 nan 0.1000 -0.0004 ## 160 0.0080 nan 0.1000 -0.0002 ## 180 0.0061 nan 0.1000 -0.0002 ## 200 0.0044 nan 0.1000 -0.0000 ## 220 0.0038 nan 0.1000 -0.0001 ## 240 0.0026 nan 0.1000 -0.0001 ## 260 0.0020 nan 0.1000 -0.0000 ## 280 0.0017 nan 0.1000 -0.0000 ## 300 0.0010 nan 0.1000 -0.0000 ## 320 0.0008 nan 0.1000 -0.0000 ## 340 0.0007 nan 0.1000 -0.0000 ## 360 0.0005 nan 0.1000 -0.0000 ## 380 0.0004 nan 0.1000 -0.0000 ## 400 0.0003 nan 0.1000 -0.0000 ## 420 0.0002 nan 0.1000 -0.0000 ## 440 0.0001 nan 0.1000 -0.0000 ## 460 0.0001 nan 0.1000 -0.0000 ## 480 0.0001 nan 0.1000 -0.0000 ## 500 0.0001 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1360 nan 0.1000 0.0763 ## 2 1.0152 nan 0.1000 0.0600 ## 3 0.9149 nan 0.1000 0.0476 ## 4 0.8323 nan 0.1000 0.0362 ## 5 0.7554 nan 0.1000 0.0367 ## 6 0.6915 nan 0.1000 0.0304 ## 7 0.6326 nan 0.1000 0.0267 ## 8 0.5850 nan 0.1000 0.0215 ## 9 0.5390 nan 0.1000 0.0193 ## 10 0.5022 nan 0.1000 0.0139 ## 20 0.2669 nan 0.1000 0.0041 ## 40 0.1084 nan 0.1000 0.0004 ## 60 0.0539 nan 0.1000 -0.0002 ## 80 0.0292 nan 0.1000 0.0002 ## 100 0.0186 nan 0.1000 -0.0002 ## 120 0.0125 nan 0.1000 -0.0005 ## 140 0.0074 nan 0.1000 -0.0001 ## 160 0.0054 nan 0.1000 -0.0001 ## 180 0.0036 nan 0.1000 -0.0001 ## 200 0.0024 nan 0.1000 -0.0000 ## 220 0.0017 nan 0.1000 -0.0000 ## 240 0.0014 nan 0.1000 -0.0000 ## 260 0.0010 nan 0.1000 -0.0000 ## 280 0.0006 nan 0.1000 -0.0000 ## 300 0.0004 nan 0.1000 -0.0000 ## 320 0.0003 nan 0.1000 -0.0000 ## 340 0.0002 nan 0.1000 -0.0000 ## 360 0.0002 nan 0.1000 -0.0000 ## 380 0.0001 nan 0.1000 0.0000 ## 400 0.0001 nan 0.1000 -0.0000 ## 420 0.0001 nan 0.1000 -0.0000 ## 440 0.0000 nan 0.1000 -0.0000 ## 460 0.0000 nan 0.1000 -0.0000 ## 480 0.0000 nan 0.1000 -0.0000 ## 500 0.0000 nan 0.1000 -0.0000 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 1.1323 nan 0.1000 0.0771 ## 2 1.0072 nan 0.1000 0.0590 ## 3 0.9077 nan 0.1000 0.0485 ## 4 0.8202 nan 0.1000 0.0413 ## 5 0.7432 nan 0.1000 0.0377 ## 6 0.6793 nan 0.1000 0.0295 ## 7 0.6198 nan 0.1000 0.0259 ## 8 0.5696 nan 0.1000 0.0238 ## 9 0.5237 nan 0.1000 0.0198 ## 10 0.4853 nan 0.1000 0.0171 ## 20 0.2601 nan 0.1000 0.0038 ## 40 0.1132 nan 0.1000 0.0007 ## 60 0.0659 nan 0.1000 -0.0008 ## 80 0.0412 nan 0.1000 -0.0004 ## 100 0.0291 nan 0.1000 -0.0001 ## 120 0.0211 nan 0.1000 -0.0003 ## 140 0.0163 nan 0.1000 -0.0002 ## 150 0.0133 nan 0.1000 -0.0002 summary(gbm) ## var rel.inf ## Cell.shape.L Cell.shape.L 34.019899160 ## Cell.size.L Cell.size.L 33.945736557 ## Cl.thickness.L Cl.thickness.L 5.148781596 ## Epith.c.size.L Epith.c.size.L 3.721787231 ## Marg.adhesion.L Marg.adhesion.L 2.284713140 ## Cell.size^5 Cell.size^5 1.612481713 ## Epith.c.size.Q Epith.c.size.Q 1.526622845 ## Marg.adhesion^9 Marg.adhesion^9 1.112704849 ## Cl.thickness^4 Cl.thickness^4 1.047868769 ## Epith.c.size^5 Epith.c.size^5 1.018452214 ## Cell.shape.Q Cell.shape.Q 0.958963895 ## Marg.adhesion.Q Marg.adhesion.Q 0.927327940 ## Cl.thickness^9 Cl.thickness^9 0.917482427 ## Cell.shape^7 Cell.shape^7 0.737573067 ## Cell.size^8 Cell.size^8 0.725473198 ## Cell.size.Q Cell.size.Q 0.684507459 ## Marg.adhesion^5 Marg.adhesion^5 0.684042933 ## Cell.size^4 Cell.size^4 0.667807695 ## Cell.size^7 Cell.size^7 0.627808870 ## Cell.shape^8 Cell.shape^8 0.610750035 ## Bl.cromatin4 Bl.cromatin4 0.605100397 ## Bl.cromatin5 Bl.cromatin5 0.542376201 ## Epith.c.size^9 Epith.c.size^9 0.489746326 ## Cell.size^9 Cell.size^9 0.479544369 ## Marg.adhesion^7 Marg.adhesion^7 0.464082103 ## Cell.shape^5 Cell.shape^5 0.454885656 ## Cl.thickness^8 Cl.thickness^8 0.319727239 ## Cl.thickness.Q Cl.thickness.Q 0.285998944 ## Cell.size^6 Cell.size^6 0.250220030 ## Normal.nucleoli10 Normal.nucleoli10 0.235789813 ## Epith.c.size^8 Epith.c.size^8 0.233298921 ## Epith.c.size^6 Epith.c.size^6 0.195192427 ## Marg.adhesion.C Marg.adhesion.C 0.195019938 ## Cell.size.C Cell.size.C 0.187025587 ## Cell.shape^6 Cell.shape^6 0.186992637 ## Marg.adhesion^8 Marg.adhesion^8 0.180777704 ## Cell.shape.C Cell.shape.C 0.179319599 ## Bl.cromatin7 Bl.cromatin7 0.171228044 ## Epith.c.size^7 Epith.c.size^7 0.170200293 ## Marg.adhesion^4 Marg.adhesion^4 0.168205952 ## Cl.thickness^6 Cl.thickness^6 0.131316900 ## Bl.cromatin3 Bl.cromatin3 0.119258478 ## Cell.shape^9 Cell.shape^9 0.118742215 ## Normal.nucleoli8 Normal.nucleoli8 0.117109214 ## Cl.thickness^5 Cl.thickness^5 0.101190293 ## Marg.adhesion^6 Marg.adhesion^6 0.091375810 ## Cell.shape^4 Cell.shape^4 0.079103318 ## Cl.thickness^7 Cl.thickness^7 0.066088160 ## Epith.c.size^4 Epith.c.size^4 0.055727339 ## Normal.nucleoli3 Normal.nucleoli3 0.049411397 ## Epith.c.size.C Epith.c.size.C 0.032180840 ## Mitoses2 Mitoses2 0.020866411 ## Cl.thickness.C Cl.thickness.C 0.019820125 ## Normal.nucleoli4 Normal.nucleoli4 0.017283676 ## Normal.nucleoli2 Normal.nucleoli2 0.003974461 ## Bl.cromatin8 Bl.cromatin8 0.001033587 ## Bl.cromatin2 Bl.cromatin2 0.000000000 ## Bl.cromatin6 Bl.cromatin6 0.000000000 ## Bl.cromatin9 Bl.cromatin9 0.000000000 ## Bl.cromatin10 Bl.cromatin10 0.000000000 ## Normal.nucleoli5 Normal.nucleoli5 0.000000000 ## Normal.nucleoli6 Normal.nucleoli6 0.000000000 ## Normal.nucleoli7 Normal.nucleoli7 0.000000000 ## Normal.nucleoli9 Normal.nucleoli9 0.000000000 ## Mitoses3 Mitoses3 0.000000000 ## Mitoses4 Mitoses4 0.000000000 ## Mitoses5 Mitoses5 0.000000000 ## Mitoses6 Mitoses6 0.000000000 ## Mitoses7 Mitoses7 0.000000000 ## Mitoses8 Mitoses8 0.000000000 ## Mitoses10 Mitoses10 0.000000000 pred_gbm&lt;-predict(gbm,BreastCancer) confusionMatrix(pred_gbm, BreastCancer$Class) ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 466 roc_gbm&lt;-pROC::roc(BreastCancer$Class, as.numeric(pred_gbm)) ## Setting levels: control = benign, case = malignant ## Setting direction: controls &lt; cases roc_gbm ## ## Call: ## roc.default(response = BreastCancer$Class, predictor = as.numeric(pred_gbm)) ## ## Data: as.numeric(pred_gbm) in 458 controls (BreastCancer$Class benign) &lt; 241 cases (BreastCancer$Class malignant). ## Area under the curve: 0.9771 6.2.2.2 Extreme gradient boost machine In the examples above, the outcome variable is treated as a factor. Extreme gradient boost machine xgboost requires conversion to numeric variable. library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:rattle&#39;: ## ## xgboost ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## slice ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following object is masked from &#39;package:tidygraph&#39;: ## ## slice ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice ## The following object is masked from &#39;package:oro.nifti&#39;: ## ## slice library(caret) data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) #predict breast cancer BreastCancer$Class&lt;-as.character(BreastCancer$Class) BreastCancer$Class[BreastCancer$Class==&quot;benign&quot;]&lt;-0 BreastCancer$Class[BreastCancer$Class==&quot;malignant&quot;]&lt;-1 BreastCancer$Class&lt;-as.numeric(BreastCancer$Class) #remove ID column #remove column a=with NA #remaining 9 columns #convert multiple columns to numeric #lapply output a list BreastCancer2&lt;-lapply(BreastCancer[,-c(1,7)], as.numeric) BreastCancer2&lt;-as.data.frame(BreastCancer2) set.seed(1234) parts = createDataPartition(BreastCancer2$Class, p = 0.75, list=F) train = BreastCancer2[parts, ] test = BreastCancer2[-parts, ] X_train = data.matrix(train[,-9]) # independent variables for train y_train = train[,9] # dependent variables for train X_test = data.matrix(test[,-9]) # independent variables for test y_test = test[,9] # dependent variables for test # convert the train and test data into xgboost matrix type. xgboost_train = xgb.DMatrix(data=X_train, label=as.matrix(y_train)) xgboost_test = xgb.DMatrix(data=X_test, label=as.matrix(y_test)) # train a model using our training data # nthread is the number of CPU threads we use # nrounds is the number of passes on the data #the function xgboost exist in xgboost and rattle model &lt;- xgboost::xgboost(data = xgboost_train, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;, verbose = 2) ## [1] train-logloss:0.221760 ## [2] train-logloss:0.129150 summary(model) ## Length Class Mode ## handle 1 xgb.Booster.handle externalptr ## raw 5890 -none- raw ## niter 1 -none- numeric ## evaluation_log 2 data.table list ## call 17 -none- call ## params 5 -none- list ## callbacks 2 -none- list ## feature_names 8 -none- character ## nfeatures 1 -none- numeric #use model to make predictions on test data pred_test = predict(model, xgboost_test) pred_test ## [1] 0.04247886 0.04247886 0.93661994 0.04247886 0.04247886 0.93661994 0.04247886 ## [8] 0.04247886 0.50793731 0.84730983 0.04247886 0.04247886 0.04247886 0.93661994 ## [15] 0.13179043 0.61858213 0.04247886 0.93661994 0.04247886 0.90757442 0.04247886 ## [22] 0.93661994 0.50793731 0.10175808 0.04247886 0.13179043 0.04247886 0.04247886 ## [29] 0.04247886 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994 0.90757442 ## [36] 0.90757442 0.93661994 0.90757442 0.04247886 0.61858213 0.04247886 0.04247886 ## [43] 0.04247886 0.04247886 0.04247886 0.93661994 0.04247886 0.61858213 0.04247886 ## [50] 0.04247886 0.93661994 0.61858213 0.93661994 0.04247886 0.93661994 0.04247886 ## [57] 0.04247886 0.04247886 0.04247886 0.93661994 0.93661994 0.04247886 0.93661994 ## [64] 0.93661994 0.04247886 0.93661994 0.13179043 0.04247886 0.93661994 0.61858213 ## [71] 0.04247886 0.93661994 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994 ## [78] 0.93661994 0.04247886 0.28787071 0.84532809 0.04247886 0.04247886 0.04247886 ## [85] 0.04247886 0.93661994 0.93661994 0.93661994 0.04247886 0.84730983 0.04247886 ## [92] 0.61858213 0.84730983 0.04247886 0.93661994 0.93661994 0.93661994 0.61858213 ## [99] 0.04247886 0.04247886 0.04247886 0.04247886 0.84730983 0.93661994 0.04247886 ## [106] 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994 0.10175808 0.04247886 ## [113] 0.10175808 0.61858213 0.90757442 0.04247886 0.04247886 0.04247886 0.04247886 ## [120] 0.93661994 0.04247886 0.04247886 0.04247886 0.04247886 0.04247886 0.93661994 ## [127] 0.04247886 0.93661994 0.04247886 0.04247886 0.04247886 0.04247886 0.04247886 ## [134] 0.04247886 0.93661994 0.04247886 0.04247886 0.93661994 0.04247886 0.04247886 ## [141] 0.04247886 0.93661994 0.93661994 0.13179043 0.04247886 0.04247886 0.04247886 ## [148] 0.04247886 0.93661994 0.04247886 0.04247886 0.04247886 0.93661994 0.04247886 ## [155] 0.93661994 0.93661994 0.04247886 0.93661994 0.93661994 0.10175808 0.04247886 ## [162] 0.04247886 0.04247886 0.04247886 0.04247886 0.61858213 0.04247886 0.93661994 ## [169] 0.04247886 0.10175808 0.04247886 0.93661994 0.04247886 0.93661994 #classify 1 if prediction &gt;.5 prediction &lt;- as.numeric(pred_test &gt; 0.5) print(head(prediction)) ## [1] 0 0 1 0 0 1 err &lt;- mean(as.numeric(pred_test &gt; 0.5) != y_test) print(paste(&quot;test-error=&quot;, err)) ## [1] &quot;test-error= 0.0632183908045977&quot; #plot of the first 2 trees xgb.plot.tree(model = model, trees = 1:2) 6.2.3 Bayesian trees method 6.2.3.1 BART BART or Bayesian additive regression trees is a non-parametric method that uses a sum of Bayesian trees to estimate an unknown function. Every tree acts as a weak learner in this ensemle method. It can also be used in causal inference. It uses tuning parameters derived from Bayesian priors. Each predicted value has a posterior distribution. BART uses a regularization prior that forces each tree to be able to explain only a limited subset of the relationships between the covariates and the predictor variable. In some instances, BART outperform xgboost. library(BART) data(Melanoma, package = &quot;MASS&quot;) N &lt;- length(Melanoma$status) #table(Melanoma$ph.karno, cancer$pat.karno) ## if physician&#39;s KPS unavailable, then use the patient&#39;s #h &lt;- which(is.na(cancer$ph.karno)) #cancer$ph.karno[h] &lt;- cancer$pat.karno[h] times &lt;- Melanoma$time times &lt;- ceiling(times/7) ## weeks #1 died from melanoma, 2 alive, 3 dead from other causes. ##delta: 0=censored, 1=dead delta=ifelse(Melanoma$status==2,0,1) ## matrix of observed covariates x.train &lt;- cbind(Melanoma$sex, Melanoma$age, Melanoma$thickness) #provide column names dimnames(x.train)[[2]] &lt;- c(&#39;M(1):F(0)&#39;,&#39;age&#39;, &#39;thickness&#39;) table(x.train[ , 1]) ## ## 0 1 ## 126 79 summary(x.train[ , 2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.00 42.00 54.00 52.46 65.00 95.00 table(x.train[ , 3]) ## ## 0.1 0.16 0.24 0.32 0.48 0.58 0.64 0.65 0.81 0.97 1.03 1.13 1.29 1.34 ## 1 7 1 6 4 1 4 10 11 11 1 4 16 2 ## 1.37 1.45 1.53 1.62 1.76 1.78 1.94 2.1 2.24 2.26 2.34 2.42 2.58 2.74 ## 1 3 1 12 1 2 10 3 1 5 1 1 9 1 ## 2.9 3.06 3.22 3.54 3.56 3.87 4.04 4.09 4.19 4.51 4.82 4.83 4.84 5.16 ## 3 2 10 8 1 6 1 1 2 1 1 2 5 3 ## 5.48 5.64 5.8 6.12 6.44 6.76 7.06 7.09 7.41 7.73 7.89 8.06 8.38 8.54 ## 2 1 2 2 1 1 2 2 1 2 1 1 1 1 ## 9.66 12.08 12.24 12.56 12.88 13.85 14.66 17.42 ## 1 1 1 1 2 1 1 1 ##test BART with token run to ensure installation works set.seed(99) post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, nskip=1, ndpost=1, keepevery=1) ## *****Calling gbart: type=2 ## *****Data: ## data:n,p,np: 17042, 4, 0 ## y1,yn: 1.000000, 0.000000 ## x1,x[n*p]: 2.000000, 2.900000 ## *****Number of Trees: 50 ## *****Number of Cut Points: 100 ... 63 ## *****burn,nd,thin: 1,1,1 ## *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.212132,3,1,-2.6383 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,3,0 ## *****printevery: 100 ## ## MCMC ## done 0 (out of 2) ## time: 0s ## trcnt,tecnt: 1,0 post &lt;- surv.bart(x.train=x.train, times=times, delta=delta, seed=99) ## *****Calling gbart: type=2 ## *****Data: ## data:n,p,np: 17042, 4, 0 ## y1,yn: 1.000000, 0.000000 ## x1,x[n*p]: 2.000000, 2.900000 ## *****Number of Trees: 50 ## *****Number of Cut Points: 100 ... 63 ## *****burn,nd,thin: 250,10000,10 ## *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.212132,3,1,-2.6383 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,3,0 ## *****printevery: 100 ## ## MCMC ## done 0 (out of 10250) ## done 100 (out of 10250) ## done 200 (out of 10250) ## done 300 (out of 10250) ## done 400 (out of 10250) ## done 500 (out of 10250) ## done 600 (out of 10250) ## done 700 (out of 10250) ## done 800 (out of 10250) ## done 900 (out of 10250) ## done 1000 (out of 10250) ## done 1100 (out of 10250) ## done 1200 (out of 10250) ## done 1300 (out of 10250) ## done 1400 (out of 10250) ## done 1500 (out of 10250) ## done 1600 (out of 10250) ## done 1700 (out of 10250) ## done 1800 (out of 10250) ## done 1900 (out of 10250) ## done 2000 (out of 10250) ## done 2100 (out of 10250) ## done 2200 (out of 10250) ## done 2300 (out of 10250) ## done 2400 (out of 10250) ## done 2500 (out of 10250) ## done 2600 (out of 10250) ## done 2700 (out of 10250) ## done 2800 (out of 10250) ## done 2900 (out of 10250) ## done 3000 (out of 10250) ## done 3100 (out of 10250) ## done 3200 (out of 10250) ## done 3300 (out of 10250) ## done 3400 (out of 10250) ## done 3500 (out of 10250) ## done 3600 (out of 10250) ## done 3700 (out of 10250) ## done 3800 (out of 10250) ## done 3900 (out of 10250) ## done 4000 (out of 10250) ## done 4100 (out of 10250) ## done 4200 (out of 10250) ## done 4300 (out of 10250) ## done 4400 (out of 10250) ## done 4500 (out of 10250) ## done 4600 (out of 10250) ## done 4700 (out of 10250) ## done 4800 (out of 10250) ## done 4900 (out of 10250) ## done 5000 (out of 10250) ## done 5100 (out of 10250) ## done 5200 (out of 10250) ## done 5300 (out of 10250) ## done 5400 (out of 10250) ## done 5500 (out of 10250) ## done 5600 (out of 10250) ## done 5700 (out of 10250) ## done 5800 (out of 10250) ## done 5900 (out of 10250) ## done 6000 (out of 10250) ## done 6100 (out of 10250) ## done 6200 (out of 10250) ## done 6300 (out of 10250) ## done 6400 (out of 10250) ## done 6500 (out of 10250) ## done 6600 (out of 10250) ## done 6700 (out of 10250) ## done 6800 (out of 10250) ## done 6900 (out of 10250) ## done 7000 (out of 10250) ## done 7100 (out of 10250) ## done 7200 (out of 10250) ## done 7300 (out of 10250) ## done 7400 (out of 10250) ## done 7500 (out of 10250) ## done 7600 (out of 10250) ## done 7700 (out of 10250) ## done 7800 (out of 10250) ## done 7900 (out of 10250) ## done 8000 (out of 10250) ## done 8100 (out of 10250) ## done 8200 (out of 10250) ## done 8300 (out of 10250) ## done 8400 (out of 10250) ## done 8500 (out of 10250) ## done 8600 (out of 10250) ## done 8700 (out of 10250) ## done 8800 (out of 10250) ## done 8900 (out of 10250) ## done 9000 (out of 10250) ## done 9100 (out of 10250) ## done 9200 (out of 10250) ## done 9300 (out of 10250) ## done 9400 (out of 10250) ## done 9500 (out of 10250) ## done 9600 (out of 10250) ## done 9700 (out of 10250) ## done 9800 (out of 10250) ## done 9900 (out of 10250) ## done 10000 (out of 10250) ## done 10100 (out of 10250) ## done 10200 (out of 10250) ## time: 425s ## trcnt,tecnt: 1000,0 pre &lt;- surv.pre.bart(times=times, delta=delta, x.train=x.train, x.test=x.train) K &lt;- pre$K M &lt;- nrow(post$yhat.train) pre$tx.test &lt;- rbind(pre$tx.test, pre$tx.test) pre$tx.test[ , 2] &lt;- c(rep(1, N*K), rep(2, N*K)) ## sex pushed to col 2, since time is always in col 1 pred &lt;- predict(post, newdata=pre$tx.test) ## *****In main of C++ for bart prediction ## tc (threadcount): 1 ## number of bart draws: 1000 ## number of trees in bart sum: 50 ## number of x columns: 4 ## from x,np,p: 4, 67240 ## ***using serial code pd &lt;- matrix(nrow=M, ncol=2*K) for(j in 1:K) { h &lt;- seq(j, N*K, by=K) pd[ , j] &lt;- apply(pred$surv.test[ , h], 1, mean) pd[ , j+K] &lt;- apply(pred$surv.test[ , h+N*K], 1, mean) } pd.mu &lt;- apply(pd, 2, mean) pd.025 &lt;- apply(pd, 2, quantile, probs=0.025) pd.975 &lt;- apply(pd, 2, quantile, probs=0.975) males &lt;- 1:K females &lt;- males+K plot(c(0, pre$times), c(1, pd.mu[males]), type=&#39;s&#39;, col=&#39;blue&#39;, ylim=0:1, ylab=&#39;S(t, x)&#39;, xlab=&#39;t (weeks)&#39;, main=paste(&#39;Melanoma ex. (MASS:: Melanoma)&#39;, &quot;Friedman&#39;s partial dependence function&quot;, &#39;Male (blue) vs. Female (red)&#39;, sep=&#39;\\n&#39;)) lines(c(0, pre$times), c(1, pd.025[males]), col=&#39;blue&#39;, type=&#39;s&#39;, lty=2) lines(c(0, pre$times), c(1, pd.975[males]), col=&#39;blue&#39;, type=&#39;s&#39;, lty=2) lines(c(0, pre$times), c(1, pd.mu[females]), col=&#39;red&#39;, type=&#39;s&#39;) lines(c(0, pre$times), c(1, pd.025[females]), col=&#39;red&#39;, type=&#39;s&#39;, lty=2) lines(c(0, pre$times), c(1, pd.975[females]), col=&#39;red&#39;, type=&#39;s&#39;, lty=2) 6.3 KNN K nearest neighbour (KNN) uses ’feature similarity based on measure of distance between data points to make prediction. The K in KNN refers to the number of neighbours to define the case for similarity. K nearest neighbour is available from the caret library. library(caret) data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; &quot;Cell.shape&quot; ## [5] &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; ## [9] &quot;Normal.nucleoli&quot; &quot;Mitoses&quot; &quot;Class&quot; #note Class is benign or malignant of class factor #column Bare.nuclei removed due to NA BreastCancer&lt;-BreastCancer[,-c(1,7)] #split data set.seed(123) split = caTools::sample.split(BreastCancer$Class, SplitRatio = 0.75) Train = subset(BreastCancer, split == TRUE) Test = subset(BreastCancer, split == FALSE) #grid of values to test in cross-validation. knn_Grid &lt;- expand.grid(k = c(1:15)) knn_Control &lt;- trainControl(method = &quot;cv&quot;, number = 10, # repeats = 10, # uncomment for repeatedcv ## Estimate class probabilities classProbs = TRUE, ## Evaluate performance using ## the following function summaryFunction = twoClassSummary) #scaling data is performed here under preProcess knn &lt;- caret::train(Class ~ ., data = Train, method = &quot;knn&quot;, trControl=knn_Control, tuneGrid=knn_Grid, #optimise with roc metric metric=&quot;ROC&quot;) summary(knn) ## Length Class Mode ## learn 2 -none- list ## k 1 -none- numeric ## theDots 0 -none- list ## xNames 71 -none- character ## problemType 1 -none- character ## tuneValue 1 data.frame list ## obsLevels 2 -none- character ## param 0 -none- list pred_knn&lt;-predict(knn,Test) confusionMatrix(pred_knn, Test$Class) ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 139 roc_knn&lt;-pROC::roc(Test$Class, as.numeric(pred_knn)) ## Setting levels: control = benign, case = malignant ## Setting direction: controls &lt; cases roc_knn ## ## Call: ## roc.default(response = Test$Class, predictor = as.numeric(pred_knn)) ## ## Data: as.numeric(pred_knn) in 114 controls (Test$Class benign) &lt; 60 cases (Test$Class malignant). ## Area under the curve: 0.7917 #https://plotly.com/r/knn-classification/ pdb &lt;- cbind(Test[,-9], Test[,9]) pdb &lt;- cbind(pdb, pred_knn) fig &lt;- plotly::plot_ly(data = pdb, x = ~as.numeric(Test$Cl.thickness), y = ~as.numeric(Test$Epith.c.size), type = &#39;scatter&#39;, mode = &#39;markers&#39;,color = ~pred_knn, colors = &#39;RdBu&#39;, symbol = ~Test$Class, split = ~Test$Class, symbols = c(&#39;square-dot&#39;,&#39;circle-dot&#39;), marker = list(size = 12, line = list(color = &#39;black&#39;, width = 1))) fig 6.4 Support vector machine In brief, support vector machine regression (SVR) can be seen as a way to enhance data which may not be easily separated in its native space. It manipulates data from low dimension to higher dimension in feature space and which can reveal relationship not discernible in low dimensional space. It does this around the hyperparameter controlling the margin of the data from a fitted line in a way not dissimilar from fitting a regression line based on minimising least squares. The default setting is radial basis function. library(e1071) library(caret) # The Breast cancer data is used again from knn trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 3) #scaling data is performed here under preProcess svm_Linear &lt;- caret::train(Class ~ ., data = Train, method = &quot;svmLinear&quot;, trControl=trctrl, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10) summary(svm_Linear) ## Length Class Mode ## 1 ksvm S4 pred&lt;-predict(svm_Linear,BreastCancer) confusionMatrix(pred, BreastCancer$Class) ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 468 roc_svm&lt;-pROC::roc(BreastCancer$Class, as.numeric(pred)) ## Setting levels: control = benign, case = malignant ## Setting direction: controls &lt; cases roc_svm ## ## Call: ## roc.default(response = BreastCancer$Class, predictor = as.numeric(pred)) ## ## Data: as.numeric(pred) in 458 controls (BreastCancer$Class benign) &lt; 241 cases (BreastCancer$Class malignant). ## Area under the curve: 0.9698 6.5 Non-negative matrix factorisation Non-negative matrix factorisation is an unsupervised machine learning method, which seeks to explain the observed clinical features using smaller number of basis components (hidden variables). A matrix V of dimension m x n is factorise to 2 matrices W and H. W has dimensions m x k an H has dimensions n x k. For topic modeling in the chapter of text mining, V matrix is the document term matrix. Each row of H is the word embedding and the columns pf W represent the weight. The interpretation of NMF components is similar to, but often more natural than, related methods such as factor analysis and principal component analysis. The non-negativity constraint in NMF leads to a simple “parts-based” interpretation and has been successfully used in facial recognition, metagene pattern discovery, and market research. For a clinical example, the matrix for NMF decomposition consists of rows of hospitals and their service availability. The example below used the recommended procedure to estimate the factorization rank, based on stability of the cophenetic correlation coefficient and the residual error, prior to performing the NMF analysis. The data were permuted and the factorization rank computed. These data were used as reference for selecting factorization rank to minimize the chance of overfitting. #BiocManager::install(&quot;Biobase&quot;) library(NMF,quietly = TRUE) library(tidyverse) edge&lt;- read.csv(&quot;./Data-Use/Hosp_Network_geocoded.csv&quot;) df&lt;-edge[,c(2:dim(edge)[2])] row.names(df)&lt;-edge[,1] #bipartite matrix #select columns#remove distance data df_se&lt;-edge[,c(2:16)] row.names(df_se)&lt;-edge[,1] #bipartite matrix #south eastern hospitals #select rows df_se&lt;-df_se[c(1,6,7,11,12,13,14,17,19,20,24,31,33,34,35),] #estimate factorisation rank-prevent overfitting estim.r &lt;- nmf(df_se, 2:6, nrun = 10, seed = 123456) plot(estim.r) consensusmap(estim.r) ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; The optimal number of rank for this data is likely to be 4. #Using the data above we can use which argument to find the order #since the starting point is 2 we just need to add 1 Rank=which(estim.r$measures$cophenetic==max(estim.r$measures$cophenetic))+1 model&lt;-nmf(df_se, Rank,nrun=100) pmodel&lt;-predict(model,prob=TRUE) coefmap(model) basismap(model) consensusmap(model) ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; 6.6 Formal concept analysis This is an unsupervised machine learning method which takes an input matrix of objects and attributes (binary values) and seeks to find the hierarchy of relations. Each concept shares a set of attributes with other objects and each sub-concept shares a smaller set of attributes with a subset of the objects. A Hasse diagram is used to display the hierarchy of relations. First we will illustrate with a simple relationship among fruit. Note in this example there is no close set for apple and pear, as both share the attribute of green color. There is a close set for the tropical fruit mango and and banana. There are several libraries for FCA. Here we will use multiplex. The fcaR library can also handle fuzzy data. #BiocManager::install(&quot;Rgraphviz&quot;) library(multiplex) #Algebraic Tools for the Analysis of Multiple Social Networks ## ## Attaching package: &#39;multiplex&#39; ## The following object is masked from &#39;package:meta&#39;: ## ## transf ## The following object is masked from &#39;package:car&#39;: ## ## mmp ## The following object is masked from &#39;package:mgcv&#39;: ## ## ti ## The following object is masked from &#39;package:pracma&#39;: ## ## fact ## The following object is masked from &#39;package:rms&#39;: ## ## cph ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## diagram library(Rgraphviz) #plot hasse diagram ## Loading required package: graph ## ## Attaching package: &#39;graph&#39; ## The following object is masked from &#39;package:munsell&#39;: ## ## complement ## The following objects are masked from &#39;package:igraph&#39;: ## ## degree, edges, intersection ## The following object is masked from &#39;package:XML&#39;: ## ## addNode ## The following object is masked from &#39;package:party&#39;: ## ## nodes ## The following object is masked from &#39;package:strucchange&#39;: ## ## boundary ## The following object is masked from &#39;package:plyr&#39;: ## ## join ## The following object is masked from &#39;package:stringr&#39;: ## ## boundary ## ## Attaching package: &#39;Rgraphviz&#39; ## The following object is masked from &#39;package:car&#39;: ## ## sp ## The following object is masked from &#39;package:crayon&#39;: ## ## style ## The following object is masked from &#39;package:plotly&#39;: ## ## style ## The following object is masked from &#39;package:NMF&#39;: ## ## name fr&lt;-data.frame(Fruit=c(&quot;Apple&quot;, &quot;Banana&quot;,&quot;Pear&quot;, &quot;Mango&quot;), round=c(1,0,0,0), cylindrical=c(0,1,0,0), yellow=c(0,1,0,1), red=c(1,0,1,1), green=c(1,0,1,0), #color when ripe tropical=c(0,1,0,1), large_seed=c(0,0,0,1) ) df&lt;-fr[,c(2:dim(fr)[2])] row.names(df)&lt;-fr[,1] #bipartite matrix #perform Galois derivations between partially ordered subsets #galois(df_se&#39;,labeling = &quot;full&quot;) gf &lt;- galois(df, labeling = &quot;reduced&quot;) #partial ordering of concept po&lt;-partial.order(gf,type=&quot;galois&quot;) diagram(po, main=&quot;Hasse diagram of partial order - Fruit&quot;) #lattice diagram with reduced context diagram.levels(po) ## $`4` ## [1] &quot;{round} {Apple}&quot; &quot;{cylindrical} {Banana}&quot; &quot;{large_seed} {Mango}&quot; ## ## $`3` ## [1] &quot;{tropical, yellow} {}&quot; &quot;{green} {Pear}&quot; ## ## $`2` ## [1] &quot;{red} {}&quot; ## ## $`5` ## [1] &quot;7&quot; ## ## $`1` ## [1] &quot;8&quot; Next we illustrate FCA in network of hospitals in South-Eastern Melbourne. The objects are the hospitals and the attributes are the services available in those hospitals. #library(multiplex) #Algebraic Tools for the Analysis of Multiple Social Networks #library(Rgraphviz) #plot hasse diagram #install BiocManager::install(&quot;Rgraphviz&quot;) edge&lt;- read.csv(&quot;./Data-Use/Hosp_Network_geocoded.csv&quot;) df&lt;-edge[,c(2:dim(edge)[2])] row.names(df)&lt;-edge[,1] #bipartite matrix #select columns#remove distance data df_se&lt;-edge[,c(2:16)] row.names(df_se)&lt;-edge[,1] #bipartite matrix #south eastern hospitals #select rows df_se&lt;-df_se[c(1,6,7,11,12,13,14,17,19,20,24,31,33,34,35),] #perform Galois derivations between partially ordered subsets #galois(df_se&#39;,labeling = &quot;full&quot;) gf &lt;- galois(df_se, labeling = &quot;reduced&quot;) #partial ordering of concept po&lt;-partial.order(gf,type=&quot;galois&quot;) diagram(po, main=&quot;Hasse diagram of partial order with reduced context&quot;) #lattice diagram with reduced context diagram.levels(po) ## $`9` ## [1] &quot;{designated, ECR} {mmc}&quot; ## ## $`4` ## [1] &quot;{link_rmh} {}&quot; &quot;18&quot; &quot;25&quot; &quot;27&quot; &quot;28&quot; ## [6] &quot;31&quot; ## ## $`1` ## [1] &quot;{CT, link_mmc} {}&quot; ## ## $`3` ## [1] &quot;{X99min_rmh} {dandenongvalley, knoxprivate, seprivate}&quot; ## [2] &quot;{TPA} {}&quot; ## [3] &quot;{CTA} {}&quot; ## [4] &quot;{stroke_unit} {}&quot; ## [5] &quot;23&quot; ## ## $`2` ## [1] &quot;{X99min_mmc} {}&quot; &quot;{public} {}&quot; ## ## $`7` ## [1] &quot;{CTP} {ddh}&quot; &quot;{} {frankston}&quot; &quot;{} {latrobe}&quot; ## ## $`6` ## [1] &quot;{MRI} {}&quot; &quot;{} {marroondah}&quot; &quot;19&quot; &quot;{} {casey}&quot; ## [5] &quot;{} {bairnsdale}&quot; &quot;{} {sale}&quot; ## ## $`5` ## [1] &quot;{VST} {}&quot; &quot;{} {hampton}&quot; &quot;22&quot; &quot;24&quot; &quot;30&quot; ## ## $`8` ## [1] &quot;{neurosx} {cabrini}&quot; &quot;{} {bhh}&quot; &quot;{} {warragul}&quot; ## ## $`10` ## [1] &quot;14&quot; 6.7 Evolutionary Algorithm Evolutionary algorithm are search method which take the source of inspiration from nature such as evolution and survival of the fittest. These are seen as heuristic based method. The results from evolutionary algorithm shouldn’t be compared unless all conditions set are the same. In essence the findings are similar under the same conditions. 6.7.1 Simulated Annealing This method uses idea in metallurgy whereby metal is heated and then cooled to alter its property. #SA section is set not to run as the analysis takes a long time. # a saved run is provided below data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) #check for duplicates sum(duplicated(BreastCancer)) #remove duplicates #keep Id to avoid creation of new duplicates BreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows #convert multiple columns to numeric #lapply output a list BreastCancer2&lt;-lapply(BreastCancer1[,-c(7,11)], as.numeric) #list BreastCancer2&lt;-as.data.frame(BreastCancer2) BreastCancer2$Class&lt;-BreastCancer1$Class x=BreastCancer2[,-10] y=BreastCancer2$Class sa_ctrl &lt;- safsControl(functions = rfSA, method = &quot;repeatedcv&quot;, repeats = 3, #default is 5 improve = 50) set.seed(10) glm_sa &lt;- safs(x = x, y = y, iters = 5, #default is 250 safsControl = sa_ctrl, method=&quot;glm&quot;) #save(glm_sa,file=&quot;Logistic_SimulatedAnnealing.Rda&quot;) ############################################# # #Simulated Annealing Feature Selection # #691 samples #9 predictors #2 classes: &#39;benign&#39;, &#39;malignant&#39; # #Maximum search iterations: 5 #Restart after 50 iterations without improvement (0 restarts on average) # #Internal performance values: Accuracy, Kappa #Subset selection driven to maximize internal Accuracy # #External performance values: Accuracy, Kappa #Best iteration chose by maximizing external Accuracy #External resampling method: Cross-Validated (10 fold, repeated 3 times) #During resampling: # * the top 5 selected variables (out of a possible 9): # Bl.cromatin (56.7%), Id (46.7%), Cl.thickness (43.3%), Epith.c.size (43.3%), #Marg.adhesion (43.3%) # * on average, 3.5 variables were selected (min = 2, max = 5) # #In the final search using the entire training set: # * 2 features selected at iteration 5 including: # Cl.thickness, Cell.size # * external performance at this iteration is # # Accuracy Kappa # 0.9314 0.8479 load(&quot;./Logistic_SimulatedAnnealing.Rda&quot;) #plot output of simulated annealing plot(glm_sa) 6.7.2 Genetic Algorithm Genetic algorithm is a machine learning tool based on ideas from Darwin’s concept of natural selection. It is based on mutation, crossover and selection. Genetic algorithm can be used in any situation. The issue is in finding the fitness function to evaluate the output. Since it does not depend on gradient descent algorithm, it is less likely to be stuck in local minima compared to other machine learning methods. Genetic algorithm is available in R as part of caret and GA libraries. Genetic algorithm can be used to optimise feature selection for regression modelling at the expense of much longer running time. One potential issue with using cross-validation in genetic algorithm for feature selection is that it would be not right to use it again when feeding this data into another machine learning method. #GA library(caret) data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) #check for duplicates sum(duplicated(BreastCancer)) #remove duplicates #keep Id to avoid creation of new duplicates BreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows #convert multiple columns to numeric #lapply output a list BreastCancer2&lt;-lapply(BreastCancer1[,-c(7,11)], as.numeric) #list BreastCancer2&lt;-as.data.frame(BreastCancer2) BreastCancer2$Class&lt;-BreastCancer1$Class #check for NA anyNA(BreastCancer2) split = caTools::sample.split(BreastCancer2$Class, SplitRatio = 0.7) Train = subset(BreastCancer2, split == TRUE) Test = subset(BreastCancer2, split == FALSE) x=Train[,-10] y=Train$Class #cross validation indicates the number of cycle of the procedure from randomly generating new population of chromosome to mutate child chromosome. ga_ctrl &lt;- gafsControl(functions = rfGA, method = &quot;cv&quot;, repeats = 3, # default is 10 genParallel=TRUE, # Use parallel programming allowParallel = TRUE ) ## Use the same random number seed as the RFE process ## so that the same CV folds are used for the external ## resampling. set.seed(10) system.time(glm_ga &lt;- gafs(x = x, y = y, iters = 5, #recommended is 200 gafsControl = ga_ctrl, method=&quot;glm&quot;)) #save(glm_ga,file=&quot;Logistic_GeneticAlgorithm.Rda&quot;) ################################################################ # The output of glm_ga #Genetic Algorithm Feature Selection #484 samples #9 predictors #2 classes: &#39;benign&#39;, &#39;malignant&#39; #Maximum generations: 5 #Population per generation: 50 #Crossover probability: 0.8 #Mutation probability: 0.1 #Elitism: 0 # #Internal performance values: Accuracy, Kappa #Subset selection driven to maximize internal Accuracy # #External performance values: Accuracy, Kappa #Best iteration chose by maximizing external Accuracy #External resampling method: Cross-Validated (10 fold) # #During resampling: # * the top 5 selected variables (out of a possible 9): # Cell.shape (100%), Cl.thickness (100%), Epith.c.size (100%), Normal.nucleoli #(100%), Id (90%) # * on average, 6.7 variables were selected (min = 5, max = 8) # #In the final search using the entire training set: # * 7 features selected at iteration 2 including: # Cl.thickness, Cell.shape, Marg.adhesion, Epith.c.size, Bl.cromatin ... # * external performance at this iteration is # # Accuracy Kappa # 0.9691 0.9328 # The output from the Genetic Algorithm is plotted as mean fitness by generations. This plot shows the internal and external accuracy estimate from cross validation. load(&quot;./Logistic_GeneticAlgorithm.Rda&quot;) #plot output of genetic algorithm plot(glm_ga) 6.8 Manifold learning Manifold learning has been described as using geometry information in high dimensional space to map data into cluster in lower dimensional space. This is a non-linear reduction technique. Several manifold learning methods are described below but this list is not exhaustive. It is available through maniTools package. 6.8.1 T-Stochastic Neighbourhood Embedding T-Stochastic Neighbourhood Embedding (TSNE) is a manifold learning method which seeks to transform the complex data into low (2) dimensions while maintaining the distance between neighbouring objects. The distance between data points are can be measured using Euclidean distance or other measures of distance. The transformed data points are conditional probabilities that represents similarities. The original description of TSNE used PCA as a first step to speed up computation and reduce noise. This method is listed here as it is a form of data reduction method. This non-linear method is different from PCA in that the low dimensional output of TSNE are not intended for machine learning. TSNE is implemented in R as Rtsne. The perplexity parameter allows tuning of the proximity of the data points. The PCA step can be performed within Rtsne by setting the pca argument. The default number of iterations or max_iter is 1000. library(Rtsne) library(ggplot2) library(mice) #impute missing data data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; &quot;Cell.shape&quot; ## [5] &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; ## [9] &quot;Normal.nucleoli&quot; &quot;Mitoses&quot; &quot;Class&quot; #check for duplicates sum(duplicated(BreastCancer)) ## [1] 8 #remove duplicates #keep Id to avoid creation of new duplicates BreastCancer1&lt;-unique(BreastCancer) #reduce 699 to 691 rows #impute missing data #m is number of multiple imputation, default is 5 #output is a list imputed_Data &lt;- mice(BreastCancer1, m=5, maxit = 5, method = &#39;pmm&#39;, seed = 500) ## ## iter imp variable ## 1 1 Bare.nuclei ## 1 2 Bare.nuclei ## 1 3 Bare.nuclei ## 1 4 Bare.nuclei ## 1 5 Bare.nuclei ## 2 1 Bare.nuclei ## 2 2 Bare.nuclei ## 2 3 Bare.nuclei ## 2 4 Bare.nuclei ## 2 5 Bare.nuclei ## 3 1 Bare.nuclei ## 3 2 Bare.nuclei ## 3 3 Bare.nuclei ## 3 4 Bare.nuclei ## 3 5 Bare.nuclei ## 4 1 Bare.nuclei ## 4 2 Bare.nuclei ## 4 3 Bare.nuclei ## 4 4 Bare.nuclei ## 4 5 Bare.nuclei ## 5 1 Bare.nuclei ## 5 2 Bare.nuclei ## 5 3 Bare.nuclei ## 5 4 Bare.nuclei ## 5 5 Bare.nuclei #choose among the 5 imputed dataset completeData &lt;- complete(imputed_Data,2) #convert multiple columns to numeric #lapply output a list BreastCancer2&lt;-lapply(completeData[,-c(11)], as.numeric) #list BreastCancer2&lt;-as.data.frame(BreastCancer2) BreastCancer2$Class&lt;-BreastCancer1$Class BC_unique &lt;- unique(BreastCancer2) # Remove duplicates set.seed(42) # Sets seed for reproducibility tsne_out &lt;- Rtsne(as.matrix(BC_unique[,-11]), normalize = T, #normalise data pca=T, dims = 3, #pca before analysis perplexity=20, #tuning verbose=FALSE) # Run TSNE #plot(tsne_out$Y,col=BC_unique$Class,asp=1) # Add a new column with color mycolors &lt;- c(&#39;red&#39;, &#39;blue&#39;) BC_unique$color &lt;- mycolors[ as.numeric(BC_unique$Class) ] #turn off rgl #rgl::plot3d(x=tsne_out$Y[,1], y=tsne_out$Y[,2], z=tsne_out$Y[,3], type = &#39;p&#39;, col=BC_unique$color, size=8) #rgl::legend3d(&quot;topright&quot;, legend = names(mycolors), pch = 16, col = colors, cex=1, inset=c(0.02)) The example with Breast cancer didn’t turn out as well. Let’s try TSNE with the iris dataset. #TSNE data(iris) #5 columns Iris_unique &lt;- unique(iris) # Remove duplicates set.seed(42) # Sets seed for reproducibility tsne_out &lt;- Rtsne(as.matrix(Iris_unique[,-5]), dims = 2, perplexity=10, verbose=FALSE) # Run TSNE plot(tsne_out$Y,col=Iris_unique$Species,asp=1) 6.8.2 Self organising map Self organising map is an unsupervised machine learning method and is excellent for viewing complex data in low dimensional space i.e. a data reduction method. SOM is available as part of kohonen library. It uses competitive learning to adjust its weight in contrast to other neural network approaches which use backward propagation or gradient descent to update the weight of the features. Each node is evaluated to participate in the neural network. Input vectors that are close to each other in high dimensional space are mapped to be close to each other in low dimensional space. SOM is a competeitive neural network and has been considered as a deep learning method. The codes below are modified from https://rpubs.com/AlgoritmaAcademy/som for use in analysis of iris data. The first illustration is with unsupervised SOM. library(kohonen) ## ## Attaching package: &#39;kohonen&#39; ## The following object is masked from &#39;package:maps&#39;: ## ## map ## The following object is masked from &#39;package:listenv&#39;: ## ## map ## The following object is masked from &#39;package:class&#39;: ## ## somgrid ## The following object is masked from &#39;package:purrr&#39;: ## ## map #unsupervised SOM #use iris dataset 150 x 5 set.seed(100) #convert to numeric matrix iris.train &lt;- as.matrix(scale(iris[,-5])) # grid should be smaller than dim(iris) 150 x5 #xdim =10 and ydim=10 would be &lt; 120 iris.grid &lt;- somgrid(xdim = 10, ydim = 10, topo = &quot;hexagonal&quot;) #som model iris.model &lt;- som(iris.train, iris.grid, rlen = 500, radius = 2.5, keep.data = TRUE, dist.fcts = &quot;euclidean&quot;) plot(iris.model, type = &quot;mapping&quot;, pchs = 19, shape = &quot;round&quot;) plot(iris.model, type = &quot;codes&quot;, main = &quot;Codes Plot&quot;, palette.name = rainbow) The plot of training shows that the distance between nodes reached a plateau after 300 iterations. plot(iris.model, type = &quot;changes&quot;) Supervised SOM is now performed with the same iris data. #SOM set.seed(100) int &lt;- sample(nrow(iris), nrow(iris)*0.8) train &lt;- iris[int,] test &lt;- iris[-int,] # scaling data trainX &lt;- scale(train[,-5]) testX &lt;- scale(test[,-5], center = attr(trainX, &quot;scaled:center&quot;)) # make label #iris$species is already of class factor train.label &lt;- train[,5] test.label &lt;- test[,5] test[,5] &lt;- 916 testXY &lt;- list(independent = testX, dependent = test.label) # make a train data sets that scaled # convert them to be a numeric matrix iris.train &lt;- as.matrix(scale(train[,-5])) set.seed(100) # grid should be smaller than dim(train) 120 x5 #xdim =10 and ydim=10 would be &lt; 120 iris.grid &lt;- somgrid(xdim = 10, ydim = 10, topo = &quot;hexagonal&quot;) #som model iris.model &lt;- som(iris.train, iris.grid, rlen = 500, radius = 2.5, keep.data = TRUE, dist.fcts = &quot;euclidean&quot;) class &lt;- xyf(trainX, classvec2classmat(train.label), iris.grid, rlen = 500) plot(class, type = &quot;changes&quot;) pred &lt;- predict(class, newdata = testXY) table(Predict = pred$predictions[[2]], Actual = test.label) ## Actual ## Predict setosa versicolor virginica ## setosa 9 0 0 ## versicolor 0 8 0 ## virginica 0 0 7 Determine number of clusters. library(factoextra) ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa fviz_nbclust(iris.model$codes[[1]], kmeans, method = &quot;wss&quot;) set.seed(100) clust &lt;- kmeans(iris.model$codes[[1]], 6) plot(iris.model, type = &quot;codes&quot;, bgcol = rainbow(9)[clust$cluster], main = &quot;Cluster SOM&quot;) add.cluster.boundaries(iris.model, clust$cluster) 6.8.3 Multidimensional scaling MDS is a method of dimensionality reduction which preserves the distance between variables. This method has been used in geography. It is implemented in IsoplotR package and igraph package as layout.mds. 6.9 Deep learning Deep learning is a neural network with many layers: inner, multiple hidden and outer layer. Deep learning methods can be supervised or unsupervised. It uses gradient descent algorithm in search for the solution. One potential issue that it may be stuck in a local minima rather than the global minima. There are several R libraries for performing deep learning. It’s worth checking out the installation requirement as some require installing the library in python and uses the reticulate library to perform analysis. The examples used here are R libraries including RSNNS. The instructions for installing Miniconda from reticulate was provided in the earlier chapter on [data wrangling][Python Minconda environment]. Those instruction include installing torch and kerras. 6.9.1 Deep neural network 6.9.1.1 Multiplayer Perceptron Multilayer perceptron is a type of deep learning. . It passes information in one direction from inner to hidden and outer layer and hence is referred to as feed forward artificial neural network. It trains the data using a loss function which adapt to the parameter and optimise according to the specified learning rate. Overfitting is minimised by using an L2 regularisation penalty, termed alpha. For tabular data, deep learning may not necessarily be better than other machine learning method. By contrast, deep learning may be better for unstructured data. library(caret) library(RSNNS) ## ## Attaching package: &#39;RSNNS&#39; ## The following object is masked from &#39;package:kohonen&#39;: ## ## som ## The following object is masked from &#39;package:ModelMetrics&#39;: ## ## confusionMatrix ## The following objects are masked from &#39;package:caret&#39;: ## ## confusionMatrix, train ## The following object is masked from &#39;package:generics&#39;: ## ## train data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; &quot;Cell.shape&quot; ## [5] &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; ## [9] &quot;Normal.nucleoli&quot; &quot;Mitoses&quot; &quot;Class&quot; #remove ID column #remove column a=with NA #alternative is to impute BreastCancer&lt;-BreastCancer[,-c(1,7)]#remaining 9 columns #convert multiple columns to numeric #lapply output a list BreastCancer2&lt;-lapply(BreastCancer[,-c(9)], as.numeric) BreastCancer2&lt;-as.data.frame(BreastCancer2) BreastCancer2&lt;-merge(BreastCancer2, BreastCancer$Class) #note Class is benign or malignant of class factor #column Bare.nuclei removed due to NA #split data set.seed(123) BreastCancer2Values &lt;- BreastCancer2[,c(1:8)] BreastCancer2Targets &lt;- decodeClassLabels(BreastCancer2[,9]) #this returns the orginal file as a list BreastCancer2 &lt;- splitForTrainingAndTest(BreastCancer2Values, BreastCancer2Targets, ratio=0.15) #ratio is percentage for test data BreastCancer2 &lt;- normTrainingAndTestSet(BreastCancer2) #put out a list object model &lt;- mlp(BreastCancer2$inputsTrain, BreastCancer2$targetsTrain, size=5, #number of unit in hidden layer learnFuncParams=c(0.1), maxit=50, #number of iteration to learn inputsTest=BreastCancer2$inputsTest, targetsTest=BreastCancer2$targetsTest) summary(model) ## SNNS network definition file V1.4-3D ## generated at Sat Feb 10 22:43:54 2024 ## ## network name : RSNNS_untitled ## source files : ## no. of units : 15 ## no. of connections : 50 ## no. of unit types : 0 ## no. of site types : 0 ## ## ## learning function : Std_Backpropagation ## update function : Topological_Order ## ## ## unit default section : ## ## act | bias | st | subnet | layer | act func | out func ## ---------|----------|----|--------|-------|--------------|------------- ## 0.00000 | 0.00000 | i | 0 | 1 | Act_Logistic | Out_Identity ## ---------|----------|----|--------|-------|--------------|------------- ## ## ## unit definition section : ## ## no. | typeName | unitName | act | bias | st | position | act func | out func | sites ## ----|----------|------------------|----------|----------|----|----------|--------------|----------|------- ## 1 | | Input_1 | -0.14849 | -0.12745 | i | 1,0,0 | Act_Identity | | ## 2 | | Input_2 | 1.59566 | 0.17298 | i | 2,0,0 | Act_Identity | | ## 3 | | Input_3 | 1.61379 | -0.05461 | i | 3,0,0 | Act_Identity | | ## 4 | | Input_4 | 0.76866 | 0.22981 | i | 4,0,0 | Act_Identity | | ## 5 | | Input_5 | 0.35424 | 0.26428 | i | 5,0,0 | Act_Identity | | ## 6 | | Input_6 | 2.69324 | -0.27267 | i | 6,0,0 | Act_Identity | | ## 7 | | Input_7 | 0.37127 | 0.01686 | i | 7,0,0 | Act_Identity | | ## 8 | | Input_8 | -0.35178 | 0.23545 | i | 8,0,0 | Act_Identity | | ## 9 | | Hidden_2_1 | 0.01505 | -4.19609 | h | 1,2,0 ||| ## 10 | | Hidden_2_2 | 0.01511 | -4.18364 | h | 2,2,0 ||| ## 11 | | Hidden_2_3 | 0.01446 | -4.17636 | h | 3,2,0 ||| ## 12 | | Hidden_2_4 | 0.01596 | -4.17513 | h | 4,2,0 ||| ## 13 | | Hidden_2_5 | 0.01394 | -4.20955 | h | 5,2,0 ||| ## 14 | | Output_benign | 0.71619 | 0.92553 | o | 1,4,0 ||| ## 15 | | Output_malignant | 0.28380 | -0.92463 | o | 2,4,0 ||| ## ----|----------|------------------|----------|----------|----|----------|--------------|----------|------- ## ## ## connection definition section : ## ## target | site | source:weight ## -------|------|--------------------------------------------------------------------------------------------------------------------- ## 9 | | 8:-0.00576, 7: 0.00727, 6:-0.00682, 5:-0.01802, 4: 0.01969, 3:-0.08011, 2: 0.09269, 1:-0.00830 ## 10 | | 8:-0.00727, 7: 0.02291, 6:-0.00065, 5:-0.01455, 4: 0.01343, 3: 0.07341, 2:-0.07951, 1:-0.00231 ## 11 | | 8:-0.02047, 7: 0.00020, 6:-0.03410, 5: 0.01364, 4: 0.05111, 3:-0.04571, 2: 0.04224, 1:-0.00804 ## 12 | | 8: 0.00616, 7: 0.00433, 6: 0.01931, 5:-0.03026, 4: 0.00085, 3:-0.02034, 2: 0.02802, 1:-0.00241 ## 13 | | 8:-0.01292, 7: 0.00408, 6: 0.00104, 5: 0.00315, 4: 0.04242, 3:-0.07730, 2: 0.02324, 1: 0.03049 ## 14 | | 13: 0.02422, 12:-0.07496, 11:-0.05541, 10: 0.00556, 9: 0.11299 ## 15 | | 13: 0.00929, 12:-0.09087, 11:-0.07272, 10:-0.00928, 9: 0.09646 ## -------|------|--------------------------------------------------------------------------------------------------------------------- weightMatrix(model) ## Input_1 Input_2 Input_3 Input_4 Input_5 Input_6 Input_7 Input_8 ## Input_1 0 0 0 0 0 0 0 0 ## Input_2 0 0 0 0 0 0 0 0 ## Input_3 0 0 0 0 0 0 0 0 ## Input_4 0 0 0 0 0 0 0 0 ## Input_5 0 0 0 0 0 0 0 0 ## Input_6 0 0 0 0 0 0 0 0 ## Input_7 0 0 0 0 0 0 0 0 ## Input_8 0 0 0 0 0 0 0 0 ## Hidden_2_1 0 0 0 0 0 0 0 0 ## Hidden_2_2 0 0 0 0 0 0 0 0 ## Hidden_2_3 0 0 0 0 0 0 0 0 ## Hidden_2_4 0 0 0 0 0 0 0 0 ## Hidden_2_5 0 0 0 0 0 0 0 0 ## Output_benign 0 0 0 0 0 0 0 0 ## Output_malignant 0 0 0 0 0 0 0 0 ## Hidden_2_1 Hidden_2_2 Hidden_2_3 Hidden_2_4 Hidden_2_5 ## Input_1 -0.008297254 -0.0023068141 -0.0080354111 -0.0024115480 0.030488508 ## Input_2 0.092688218 -0.0795127451 0.0422427431 0.0280199945 0.023240086 ## Input_3 -0.080105409 0.0734057277 -0.0457132570 -0.0203414690 -0.077304244 ## Input_4 0.019694204 0.0134255355 0.0511073507 0.0008500156 0.042423774 ## Input_5 -0.018022288 -0.0145457229 0.0136382505 -0.0302589945 0.003147935 ## Input_6 -0.006821295 -0.0006512092 -0.0341025740 0.0193094723 0.001040343 ## Input_7 0.007272924 0.0229056627 0.0001989322 0.0043286891 0.004078272 ## Input_8 -0.005761526 -0.0072661606 -0.0204701889 0.0061648567 -0.012916351 ## Hidden_2_1 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_2 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_3 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_4 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_5 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Output_benign 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Output_malignant 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Output_benign Output_malignant ## Input_1 0.000000000 0.000000000 ## Input_2 0.000000000 0.000000000 ## Input_3 0.000000000 0.000000000 ## Input_4 0.000000000 0.000000000 ## Input_5 0.000000000 0.000000000 ## Input_6 0.000000000 0.000000000 ## Input_7 0.000000000 0.000000000 ## Input_8 0.000000000 0.000000000 ## Hidden_2_1 0.112992622 0.096456863 ## Hidden_2_2 0.005558367 -0.009282685 ## Hidden_2_3 -0.055406742 -0.072724856 ## Hidden_2_4 -0.074957542 -0.090871565 ## Hidden_2_5 0.024224803 0.009294559 ## Output_benign 0.000000000 0.000000000 ## Output_malignant 0.000000000 0.000000000 extractNetInfo(model) ## $infoHeader ## name value ## 1 no. of units 15 ## 2 no. of connections 50 ## 3 no. of unit types 0 ## 4 no. of site types 0 ## 5 learning function Std_Backpropagation ## 6 update function Topological_Order ## ## $unitDefinitions ## unitNo unitName unitAct unitBias type posX posY posZ ## 1 1 Input_1 -0.14849401 -0.12745351 UNIT_INPUT 1 0 0 ## 2 2 Input_2 1.59566152 0.17298311 UNIT_INPUT 2 0 0 ## 3 3 Input_3 1.61378837 -0.05461386 UNIT_INPUT 3 0 0 ## 4 4 Input_4 0.76865852 0.22981048 UNIT_INPUT 4 0 0 ## 5 5 Input_5 0.35424057 0.26428038 UNIT_INPUT 5 0 0 ## 6 6 Input_6 2.69324446 -0.27266610 UNIT_INPUT 6 0 0 ## 7 7 Input_7 0.37127367 0.01686329 UNIT_INPUT 7 0 0 ## 8 8 Input_8 -0.35178334 0.23545146 UNIT_INPUT 8 0 0 ## 9 9 Hidden_2_1 0.01505136 -4.19608831 UNIT_HIDDEN 1 2 0 ## 10 10 Hidden_2_2 0.01510898 -4.18364382 UNIT_HIDDEN 2 2 0 ## 11 11 Hidden_2_3 0.01445734 -4.17636061 UNIT_HIDDEN 3 2 0 ## 12 12 Hidden_2_4 0.01596117 -4.17512655 UNIT_HIDDEN 4 2 0 ## 13 13 Hidden_2_5 0.01393711 -4.20955276 UNIT_HIDDEN 5 2 0 ## 14 14 Output_benign 0.71619302 0.92553055 UNIT_OUTPUT 1 4 0 ## 15 15 Output_malignant 0.28380045 -0.92462683 UNIT_OUTPUT 2 4 0 ## actFunc outFunc sites ## 1 Act_Identity Out_Identity ## 2 Act_Identity Out_Identity ## 3 Act_Identity Out_Identity ## 4 Act_Identity Out_Identity ## 5 Act_Identity Out_Identity ## 6 Act_Identity Out_Identity ## 7 Act_Identity Out_Identity ## 8 Act_Identity Out_Identity ## 9 Act_Logistic Out_Identity ## 10 Act_Logistic Out_Identity ## 11 Act_Logistic Out_Identity ## 12 Act_Logistic Out_Identity ## 13 Act_Logistic Out_Identity ## 14 Act_Logistic Out_Identity ## 15 Act_Logistic Out_Identity ## ## $fullWeightMatrix ## Input_1 Input_2 Input_3 Input_4 Input_5 Input_6 Input_7 Input_8 ## Input_1 0 0 0 0 0 0 0 0 ## Input_2 0 0 0 0 0 0 0 0 ## Input_3 0 0 0 0 0 0 0 0 ## Input_4 0 0 0 0 0 0 0 0 ## Input_5 0 0 0 0 0 0 0 0 ## Input_6 0 0 0 0 0 0 0 0 ## Input_7 0 0 0 0 0 0 0 0 ## Input_8 0 0 0 0 0 0 0 0 ## Hidden_2_1 0 0 0 0 0 0 0 0 ## Hidden_2_2 0 0 0 0 0 0 0 0 ## Hidden_2_3 0 0 0 0 0 0 0 0 ## Hidden_2_4 0 0 0 0 0 0 0 0 ## Hidden_2_5 0 0 0 0 0 0 0 0 ## Output_benign 0 0 0 0 0 0 0 0 ## Output_malignant 0 0 0 0 0 0 0 0 ## Hidden_2_1 Hidden_2_2 Hidden_2_3 Hidden_2_4 Hidden_2_5 ## Input_1 -0.008297254 -0.0023068141 -0.0080354111 -0.0024115480 0.030488508 ## Input_2 0.092688218 -0.0795127451 0.0422427431 0.0280199945 0.023240086 ## Input_3 -0.080105409 0.0734057277 -0.0457132570 -0.0203414690 -0.077304244 ## Input_4 0.019694204 0.0134255355 0.0511073507 0.0008500156 0.042423774 ## Input_5 -0.018022288 -0.0145457229 0.0136382505 -0.0302589945 0.003147935 ## Input_6 -0.006821295 -0.0006512092 -0.0341025740 0.0193094723 0.001040343 ## Input_7 0.007272924 0.0229056627 0.0001989322 0.0043286891 0.004078272 ## Input_8 -0.005761526 -0.0072661606 -0.0204701889 0.0061648567 -0.012916351 ## Hidden_2_1 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_2 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_3 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_4 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Hidden_2_5 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Output_benign 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Output_malignant 0.000000000 0.0000000000 0.0000000000 0.0000000000 0.000000000 ## Output_benign Output_malignant ## Input_1 0.000000000 0.000000000 ## Input_2 0.000000000 0.000000000 ## Input_3 0.000000000 0.000000000 ## Input_4 0.000000000 0.000000000 ## Input_5 0.000000000 0.000000000 ## Input_6 0.000000000 0.000000000 ## Input_7 0.000000000 0.000000000 ## Input_8 0.000000000 0.000000000 ## Hidden_2_1 0.112992622 0.096456863 ## Hidden_2_2 0.005558367 -0.009282685 ## Hidden_2_3 -0.055406742 -0.072724856 ## Hidden_2_4 -0.074957542 -0.090871565 ## Hidden_2_5 0.024224803 0.009294559 ## Output_benign 0.000000000 0.000000000 ## Output_malignant 0.000000000 0.000000000 par(mfrow=c(2,2)) plotIterativeError(model) predictions &lt;- predict(model,BreastCancer2$inputsTest) plotRegressionError(predictions[,2], BreastCancer2$targetsTest[,2]) confusionMatrix(BreastCancer2$targetsTrain,fitted.values(model)) ## predictions ## targets 1 ## 1 262125 ## 2 153185 confusionMatrix(BreastCancer2$targetsTest,predictions) ## predictions ## targets 1 ## 1 58017 ## 2 15274 plotROC(fitted.values(model)[,2], BreastCancer2$targetsTrain[,2]) plotROC(predictions[,2], BreastCancer2$targetsTest[,2]) probs &lt;- predictions / rowSums(predictions) #confusion matrix with 402040-method confusionMatrix(BreastCancer2$targetsTrain, encodeClassLabels(fitted.values(model), method=&quot;402040&quot;, l=0.4, h=0.6)) ## predictions ## targets 1 ## 1 262125 ## 2 153185 6.9.1.2 Deep survival neural network There are several libraries available in survivalmodels which interface with pycox from Python. First, we illustrate the use of deepsurv. library(survivalmodels) ## Warning: package &#39;survivalmodels&#39; was built under R version 4.3.2 data(Melanoma, package = &quot;MASS&quot;) times &lt;- Melanoma$time times &lt;- ceiling(times/7) ## weeks #1 died from melanoma, 2 alive, 3 dead from other causes. ##delta: 0=censored, 1=dead delta=ifelse(Melanoma$status==2,0,1) ## matrix of observed covariates x.train &lt;- cbind(Melanoma$sex, Melanoma$age, Melanoma$thickness) deepsurv(data = Melanoma, frac = 0.3, #Fraction of data to use for validation activation = &quot;relu&quot;, num_nodes = c(4L, 8L, 4L, 2L), dropout = 0.1, early_stopping = TRUE, epochs = 100L, #number of epochs. batch_size = 32L) ## ## DeepSurv Neural Network ## ## Call: ## deepsurv(data = Melanoma, frac = 0.3, activation = &quot;relu&quot;, num_nodes = c(4L, 8L, 4L, 2L), dropout = 0.1, early_stopping = TRUE, batch_size = 32L, epochs = 100L) ## ## Response: ## Surv(time, status) ## Features: ## {sex, age, year, thickness, ulcer} Using the same Melanoma dataset, we illustrate DeepHit. DH&lt;-deephit(data = Melanoma, frac = 0.3, #Fraction of data to use for validation activation = &quot;relu&quot;, num_nodes = c(4L, 8L, 4L, 2L), dropout = 0.1, early_stopping = TRUE, epochs = 100L, #number of epochs. batch_size = 32L) summary(DH) ## ## DeepHit Neural Network ## ## Call: ## deephit(data = Melanoma, frac = 0.3, activation = &quot;relu&quot;, num_nodes = c(4L, 8L, 4L, 2L), dropout = 0.1, early_stopping = TRUE, batch_size = 32L, epochs = 100L) ## ## Response: ## Surv(time, status) ## Features: ## {sex, age, year, thickness, ulcer} library(mlr3) ## Warning: package &#39;mlr3&#39; was built under R version 4.3.2 ## ## Attaching package: &#39;mlr3&#39; ## The following object is masked from &#39;package:R.utils&#39;: ## ## resample ## The following object is masked from &#39;package:signal&#39;: ## ## resample ## The following object is masked from &#39;package:raster&#39;: ## ## resample ## The following object is masked from &#39;package:terra&#39;: ## ## resample library(mlr3proba) ## Warning: package &#39;mlr3proba&#39; was built under R version 4.3.2 library(mlr3extralearners) library(mlr3pipelines) ## Warning: package &#39;mlr3pipelines&#39; was built under R version 4.3.2 ## ## Attaching package: &#39;mlr3pipelines&#39; ## The following object is masked from &#39;package:bitops&#39;: ## ## %&gt;&gt;% ## The following object is masked from &#39;package:BiocGenerics&#39;: ## ## pos library(mlr3tuning) ## Warning: package &#39;mlr3tuning&#39; was built under R version 4.3.2 ## Loading required package: paradox ## ## Attaching package: &#39;mlr3tuning&#39; ## The following object is masked from &#39;package:multiplex&#39;: ## ## ti ## The following object is masked from &#39;package:hardhat&#39;: ## ## tune ## The following object is masked from &#39;package:ModelMetrics&#39;: ## ## tnr ## The following object is masked from &#39;package:randomForestSRC&#39;: ## ## tune ## The following object is masked from &#39;package:mgcv&#39;: ## ## ti ## The following object is masked from &#39;package:e1071&#39;: ## ## tune ## get the `whas` task from mlr3proba whas &lt;- tsk(&quot;whas&quot;) ## create our own task from the Melanoma dataset Melanoma_data &lt;- MASS::Melanoma Melanoma_data$status=ifelse(Melanoma_data$status==2,0,1) ## convert characters to factors Melanoma_data$sex &lt;- factor(Melanoma_data$sex) MelanomaTS &lt;- TaskSurv$new(&quot;Melanoma&quot;, Melanoma_data, time = &quot;time&quot;, event = &quot;status&quot;) #1 died from melanoma, 2 alive, 3 dead from other causes. ##delta: 0=censored, 1=dead delta=ifelse(Melanoma_data$status==2,0,1) ## matrix of observed covariates x.train &lt;- cbind(Melanoma_data$sex, Melanoma_data$age, Melanoma_data$thickness) ## combine in list tasks &lt;- list(whas, MelanomaTS) library(paradox) search_space &lt;- ps( ## p_dbl for numeric valued parameters dropout = p_dbl(lower = 0, upper = 1), weight_decay = p_dbl(lower = 0, upper = 0.5), learning_rate = p_dbl(lower = 0, upper = 1), ## p_int for integer valued parameters nodes = p_int(lower = 1, upper = 32), k = p_int(lower = 1, upper = 4) ) search_space$trafo &lt;- function(x, param_set) { x$num_nodes = rep(x$nodes, x$k) x$nodes = x$k = NULL return(x) } create_autotuner &lt;- function(learner) { AutoTuner$new( learner = learner, search_space = search_space, resampling = rsmp(&quot;holdout&quot;), measure = msr(&quot;surv.cindex&quot;), terminator = trm(&quot;evals&quot;, n_evals = 2), tuner = tnr(&quot;random_search&quot;) ) } ## load learners learners &lt;- lrns( #paste0(&quot;surv.&quot;, c(&quot;deephit&quot;, &quot;deepsurv&quot;)), #crash when running multiple learners paste0(&quot;surv.&quot;, c( &quot;deepsurv&quot;)), frac = 0.3, early_stopping = TRUE, epochs = 10, optimizer = &quot;adam&quot; ) # apply our function learners &lt;- lapply(learners, create_autotuner) create_pipeops &lt;- function(learner) { po(&quot;encode&quot;) %&gt;&gt;% po(&quot;scale&quot;) %&gt;&gt;% po(&quot;learner&quot;, learner) } ## apply our function learners &lt;- lapply(learners, create_pipeops) ## select holdout as the resampling strategy resampling &lt;- rsmp(&quot;cv&quot;, folds = 2) ## add KM and CPH learners &lt;- c(learners, lrns(c(&quot;surv.kaplan&quot;, &quot;surv.coxph&quot;))) #learners &lt;- c(learners, lrns(c(&quot;surv.coxph&quot;))) design &lt;- benchmark_grid(tasks, learners, resampling) bm &lt;- benchmark(design) ## INFO [22:45:30.466] [mlr3] Running benchmark with 12 resampling iterations ## INFO [22:45:30.536] [mlr3] Applying learner &#39;encode.scale.surv.deepsurv.tuned&#39; on task &#39;whas&#39; (iter 1/2) ## INFO [22:45:30.956] [bbotk] Starting to optimize 5 parameter(s) with &#39;&lt;OptimizerRandomSearch&gt;&#39; and &#39;&lt;TerminatorEvals&gt; [n_evals=2, k=0]&#39; ## INFO [22:45:31.023] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:31.065] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:31.083] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;whas&#39; (iter 1/1) ## INFO [22:45:31.306] [mlr3] Finished benchmark ## INFO [22:45:31.407] [bbotk] Result of batch 1: ## INFO [22:45:31.411] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:31.411] [bbotk] 0.6247406 0.4554209 0.8456182 11 3 0.6066725 0 0 ## INFO [22:45:31.411] [bbotk] runtime_learners uhash ## INFO [22:45:31.411] [bbotk] 0.23 3ea7cab2-7ba1-4d11-a121-f97867a32362 ## INFO [22:45:31.426] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:31.487] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:31.505] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;whas&#39; (iter 1/1) ## INFO [22:45:31.682] [mlr3] Finished benchmark ## INFO [22:45:31.769] [bbotk] Result of batch 2: ## INFO [22:45:31.773] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:31.773] [bbotk] 0.1573471 0.4268247 0.6872897 27 1 0.6738367 0 0 ## INFO [22:45:31.773] [bbotk] runtime_learners uhash ## INFO [22:45:31.773] [bbotk] 0.18 14ba6949-ab43-4e4b-9b15-dcadc5181700 ## INFO [22:45:31.798] [bbotk] Finished optimizing after 2 evaluation(s) ## INFO [22:45:31.800] [bbotk] Result: ## INFO [22:45:31.803] [bbotk] dropout weight_decay learning_rate nodes k learner_param_vals x_domain ## INFO [22:45:31.803] [bbotk] 0.1573471 0.4268247 0.6872897 27 1 &lt;list[8]&gt; &lt;list[4]&gt; ## INFO [22:45:31.803] [bbotk] surv.cindex ## INFO [22:45:31.803] [bbotk] 0.6738367 ## INFO [22:45:32.334] [mlr3] Applying learner &#39;encode.scale.surv.deepsurv.tuned&#39; on task &#39;whas&#39; (iter 2/2) ## INFO [22:45:32.686] [bbotk] Starting to optimize 5 parameter(s) with &#39;&lt;OptimizerRandomSearch&gt;&#39; and &#39;&lt;TerminatorEvals&gt; [n_evals=2, k=0]&#39; ## INFO [22:45:32.738] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:32.776] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:32.793] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;whas&#39; (iter 1/1) ## INFO [22:45:33.788] [mlr3] Finished benchmark ## INFO [22:45:33.891] [bbotk] Result of batch 1: ## INFO [22:45:33.897] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:33.897] [bbotk] 0.9949267 0.2285935 0.8684118 17 4 0.627995 0 0 ## INFO [22:45:33.897] [bbotk] runtime_learners uhash ## INFO [22:45:33.897] [bbotk] 0.97 ee7d69c6-a523-42b5-87bf-cc74c6e3050a ## INFO [22:45:33.919] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:33.969] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:33.992] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;whas&#39; (iter 1/1) ## INFO [22:45:34.222] [mlr3] Finished benchmark ## INFO [22:45:34.338] [bbotk] Result of batch 2: ## INFO [22:45:34.343] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:34.343] [bbotk] 0.8363856 0.2299571 0.9908099 13 2 0.4897016 0 0 ## INFO [22:45:34.343] [bbotk] runtime_learners uhash ## INFO [22:45:34.343] [bbotk] 0.2 969eb2e1-87b3-4270-9687-2b42b2221661 ## INFO [22:45:34.370] [bbotk] Finished optimizing after 2 evaluation(s) ## INFO [22:45:34.371] [bbotk] Result: ## INFO [22:45:34.373] [bbotk] dropout weight_decay learning_rate nodes k learner_param_vals x_domain ## INFO [22:45:34.373] [bbotk] 0.9949267 0.2285935 0.8684118 17 4 &lt;list[8]&gt; &lt;list[4]&gt; ## INFO [22:45:34.373] [bbotk] surv.cindex ## INFO [22:45:34.373] [bbotk] 0.627995 ## INFO [22:45:34.908] [mlr3] Applying learner &#39;surv.kaplan&#39; on task &#39;whas&#39; (iter 1/2) ## INFO [22:45:34.973] [mlr3] Applying learner &#39;surv.kaplan&#39; on task &#39;whas&#39; (iter 2/2) ## INFO [22:45:35.026] [mlr3] Applying learner &#39;surv.coxph&#39; on task &#39;whas&#39; (iter 1/2) ## INFO [22:45:35.129] [mlr3] Applying learner &#39;surv.coxph&#39; on task &#39;whas&#39; (iter 2/2) ## INFO [22:45:35.202] [mlr3] Applying learner &#39;encode.scale.surv.deepsurv.tuned&#39; on task &#39;Melanoma&#39; (iter 1/2) ## INFO [22:45:35.582] [bbotk] Starting to optimize 5 parameter(s) with &#39;&lt;OptimizerRandomSearch&gt;&#39; and &#39;&lt;TerminatorEvals&gt; [n_evals=2, k=0]&#39; ## INFO [22:45:35.635] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:35.675] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:35.692] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;Melanoma&#39; (iter 1/1) ## INFO [22:45:35.842] [mlr3] Finished benchmark ## INFO [22:45:35.921] [bbotk] Result of batch 1: ## INFO [22:45:35.925] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:35.925] [bbotk] 0.4570555 0.2741379 0.8601232 20 1 0.7703704 0 0 ## INFO [22:45:35.925] [bbotk] runtime_learners uhash ## INFO [22:45:35.925] [bbotk] 0.13 29542f3a-2e87-4685-a5dc-4846d9dc658f ## INFO [22:45:35.941] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:35.979] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:35.997] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;Melanoma&#39; (iter 1/1) ## INFO [22:45:36.154] [mlr3] Finished benchmark ## INFO [22:45:36.245] [bbotk] Result of batch 2: ## INFO [22:45:36.249] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:36.249] [bbotk] 0.7608218 0.479834 0.7939875 20 3 0.6814815 0 0 ## INFO [22:45:36.249] [bbotk] runtime_learners uhash ## INFO [22:45:36.249] [bbotk] 0.13 9e732a8a-5c8e-4dae-b5fe-6d2e69c8b9b7 ## INFO [22:45:36.272] [bbotk] Finished optimizing after 2 evaluation(s) ## INFO [22:45:36.274] [bbotk] Result: ## INFO [22:45:36.276] [bbotk] dropout weight_decay learning_rate nodes k learner_param_vals x_domain ## INFO [22:45:36.276] [bbotk] 0.4570555 0.2741379 0.8601232 20 1 &lt;list[8]&gt; &lt;list[4]&gt; ## INFO [22:45:36.276] [bbotk] surv.cindex ## INFO [22:45:36.276] [bbotk] 0.7703704 ## INFO [22:45:36.749] [mlr3] Applying learner &#39;encode.scale.surv.deepsurv.tuned&#39; on task &#39;Melanoma&#39; (iter 2/2) ## INFO [22:45:37.111] [bbotk] Starting to optimize 5 parameter(s) with &#39;&lt;OptimizerRandomSearch&gt;&#39; and &#39;&lt;TerminatorEvals&gt; [n_evals=2, k=0]&#39; ## INFO [22:45:37.165] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:37.205] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:37.223] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;Melanoma&#39; (iter 1/1) ## INFO [22:45:37.391] [mlr3] Finished benchmark ## INFO [22:45:37.470] [bbotk] Result of batch 1: ## INFO [22:45:37.474] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:37.474] [bbotk] 0.1140623 0.2846605 0.22349 10 4 0.6612903 0 0 ## INFO [22:45:37.474] [bbotk] runtime_learners uhash ## INFO [22:45:37.474] [bbotk] 0.16 690f0fef-632b-4560-b9f3-158d3d2c0f43 ## INFO [22:45:37.490] [bbotk] Evaluating 1 configuration(s) ## INFO [22:45:37.528] [mlr3] Running benchmark with 1 resampling iterations ## INFO [22:45:37.547] [mlr3] Applying learner &#39;surv.deepsurv&#39; on task &#39;Melanoma&#39; (iter 1/1) ## INFO [22:45:37.857] [mlr3] Finished benchmark ## INFO [22:45:37.947] [bbotk] Result of batch 2: ## INFO [22:45:37.950] [bbotk] dropout weight_decay learning_rate nodes k surv.cindex warnings errors ## INFO [22:45:37.950] [bbotk] 0.2337101 0.3132217 0.5829765 8 3 0.4903226 0 0 ## INFO [22:45:37.950] [bbotk] runtime_learners uhash ## INFO [22:45:37.950] [bbotk] 0.3 bb3cb79c-2903-4a94-bf60-5e3b21585b9d ## INFO [22:45:37.973] [bbotk] Finished optimizing after 2 evaluation(s) ## INFO [22:45:37.974] [bbotk] Result: ## INFO [22:45:37.977] [bbotk] dropout weight_decay learning_rate nodes k learner_param_vals x_domain ## INFO [22:45:37.977] [bbotk] 0.1140623 0.2846605 0.22349 10 4 &lt;list[8]&gt; &lt;list[4]&gt; ## INFO [22:45:37.977] [bbotk] surv.cindex ## INFO [22:45:37.977] [bbotk] 0.6612903 ## INFO [22:45:38.450] [mlr3] Applying learner &#39;surv.kaplan&#39; on task &#39;Melanoma&#39; (iter 1/2) ## INFO [22:45:38.491] [mlr3] Applying learner &#39;surv.kaplan&#39; on task &#39;Melanoma&#39; (iter 2/2) ## INFO [22:45:38.532] [mlr3] Applying learner &#39;surv.coxph&#39; on task &#39;Melanoma&#39; (iter 1/2) ## INFO [22:45:38.583] [mlr3] Applying learner &#39;surv.coxph&#39; on task &#39;Melanoma&#39; (iter 2/2) ## INFO [22:45:38.634] [mlr3] Finished benchmark ## Aggreggate with Harrell&#39;s C and Integrated Graf Score msrs &lt;- msrs(c(&quot;surv.cindex&quot;, &quot;surv.graf&quot;)) bm$aggregate(msrs)[, c(3, 4, 7, 8)] ## task_id learner_id surv.cindex surv.graf ## 1: whas encode.scale.surv.deepsurv.tuned 0.6280322 Inf ## 2: whas surv.kaplan 0.5000000 Inf ## 3: whas surv.coxph 0.7502472 Inf ## 4: Melanoma encode.scale.surv.deepsurv.tuned 0.6223551 Inf ## 5: Melanoma surv.kaplan 0.5000000 Inf ## 6: Melanoma surv.coxph 0.7383489 Inf library(mlr3benchmark) ## Warning: package &#39;mlr3benchmark&#39; was built under R version 4.3.2 ## ## Attaching package: &#39;mlr3benchmark&#39; ## The following object is masked from &#39;package:survivalmodels&#39;: ## ## requireNamespaces ## create mlr3benchmark object bma &lt;- as.BenchmarkAggr(bm) ## Warning: &#39;as.BenchmarkAggr&#39; is deprecated. ## Use &#39;as_benchmark_aggr&#39; instead. ## See help(&quot;Deprecated&quot;) ## run global Friedman test bma$friedman_test() ## ## Friedman rank sum test ## ## data: cindex and learner_id and task_id ## Friedman chi-squared = 4, df = 2, p-value = 0.1353 6.9.2 Tabnet 6.9.3 CNN Convolution neural network or CNN is an artifical neural network method that is well suited to classification of image data. CNN is able to develop an internal representation of the image. 6.9.4 RNN Recurrent neural network or RNN is an artifical neural network method that is well suited to data with repeated patterns such as natural language processing. However, this architecture is less suited for tabular or imaging data. 6.9.5 Reinforcement learning References "],["machine-learning-part-2.html", "Chapter 7 Machine Learning Part 2 7.1 Bag of words 7.2 Wordcloud 7.3 Bigram analysis 7.4 Trigram 7.5 Topic modeling or thematic analysis", " Chapter 7 Machine Learning Part 2 This section deals with handling of text data and machine learning. R has several excellent libraries such as tm, tidytext, textmineR and quanteda for handling text data. 7.1 Bag of words Bag of words or unigram analysis describe data in which words in a sentence were separated or tokenised. Within this bag of words the order of words within the document is not retained. Depending on how this process is performed the negative connotation may be loss. Consider “not green” and after cleaning of the document, only the color “green” remain. The following codes illustrate the processing steps to clean up a document. These include turning words to lower case as R is case sensitive. Next stop word filter is used to remove phrases like “I”, “he”, “she”, “they” etc. 7.1.1 TFIDF Term frequency defines the frequency of a term in a document. The document frequency defines how often a term is used across document. The inverse document frequency can be seen as a weight to decrease the importance of commonly words used across documents. Term frequency inverse document frequency is a process used to down weight common terms and highlight important terms in the document. In the example under topic modeling, an example of creating tfidf is shown. Other packages like tidytext, textmineR have functions for creating tfidf 7.1.2 Extracting data from web This is an example using RISmed library to extract data from PubMed on electronic medcical record and text mining for 2021. #library(adjutant) library(RISmed) ## ## Attaching package: &#39;RISmed&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## myeloma ## The following object is masked from &#39;package:survival&#39;: ## ## myeloma ## The following object is masked from &#39;package:survminer&#39;: ## ## myeloma library(ggplot2) library(dplyr) library(SnowballC) library(wordcloud) library(lattice) library(tm) library (dplyr) library(tidytext) library(tidyr) library(stringr) The function to extract data from PubMed. Data such as tear of publications can be easily extracted. Create list for high and low impact factor journals Plot of journal publications normalised by year. Plot of journal publications normalised by journal. Corpus The steps in processing and creating a Corpus from tm library is illustrated. Here, the same preprocessing steps are performed using tidytext. 7.2 Wordcloud A trick with wordcloud is setting the right number of words, the range of size of the words to be plotted. Plot Wordcloud with negative and positive sentiment from Bing library. Other sentiment libraries include afinn, loughran and nrc. graph analysis of word relationship 7.3 Bigram analysis The relationship among the bigrams are illustrated here. 7.4 Trigram The relationship among the trigrams are illustrated here. 7.5 Topic modeling or thematic analysis Two methods for unsupervised thematic analysis, NMF and probabilistic topic model, are illustrated. 7.5.1 Probabilistic topic model Probabilistic topic modelling is a machine learning method that generates topics or discovers themes among a collection of documents. This step was performed using the Latent Dirichlet Allocation algorithm via the topicmodels package in R. An issue with topic modeling is that the number of topics are not known. It can be estimated empirically or by examining the harmonic means of the log likelihood . Estimate the number of topics based on the log likelihood of P(topics|documents) at each iterations The previous analysis show that there are 40 topics. For ease of illustrations LDA is perform with 5 topics. Compare differences in words between topics. 7.5.2 NMF In the previous chapter, NMF was used as a method to cluster data. Here, it can be framed as a method for topic modeling. The term document matrix is used. "],["bayesian-analysis.html", "Chapter 8 Bayesian Analysis 8.1 Baysian belief 8.2 Markov model 8.3 INLA, Stan and BUGS", " Chapter 8 Bayesian Analysis 8.1 Baysian belief Bayes Theorem deals with the impact of information update on current belief expresses in terms of probability. P(A|B) is the posterior probability of A given knowledge of B. P(A) is the prior probability. P(B|A) is the conditional probability of B given A. P(A|B)=P(B|A) x P(A)/P(B) Bayes’ theorem can be expressed in terms of odds and post-test probabilities. This concept had been used in the analysis of the post-test probabilities of stroke related to the use of ABCD2. In this case the likelihood ratio and pretest probability of stroke in patients with TIA is used to calculate the post-test probability. Table 13. 2 x 2 table Disease + Disease - Test + eg ABCD2≥4 TP FP Test – eg ABCD2&lt;4 FN TN The true positive rate (TPR) can be expressed as conditional probability P(T+|D+). TP is the joint distribution of T+ and D+ while TN is the joint distribution of T- and D-. The true negative rate (TNR) can be expressed as conditional probability P(T-|D-). In the setting of ABCD2, the pre-test odds is derived from cases of TIA and stroke outcome at 90 days. The positive likelihood ratio (PLR) in this case is derived from the sensitivity of ABCD2 for stroke and the one minus specificity of ABCD for stroke. One can interpret the likelihood ratio in terms of Bayes theorem. To derive the post-test odds, the PLR is multiplied by the pre-test odds at the voxel level. The post-test odds is given by the product of the pre-test odds and the likelihood ratio. In turn, the post-test probabilities is calculated from the post-test odds by: 8.1.1 Conditional probability Two events are independent if Or It follows that the events A and B occur simultaneously P(A∩B) is given by the probability of event A and probability of event B given that A has occurred. If events A and B are independent then P(A∩B) is given by the probability of event A and probability of event B. 8.1.1.1 Bayes Rule For a sample space (A) containing n different subspaces (A1, A2..An) and A is a subset of the larger sample space B+ and B-, the probability that A is a member of B+an be given by P(A|B+). This can be divised as a tree structure with one branch given by the product of P(B+) and P(A|B+) and the other P(B-) and P(A|B-). The probability of B is given by the sum of P(B+)P(A|B+) and P(B-)P(A|B). To make a decision on which of An events to choose, one evaluate the conditional probability of each subset e.g. P(A1∩B)/P(B), P(A2∩B)/P(B)…P(An∩B)/P(B). The probability of P(B) is given by the sum of P(A1∩B), P(A2∩B)…P(An∩B). Here P(An∩B) is the same as P(An)P(B|An). The subspace with the highest conditional probability may yield the optimal result. 8.1.1.2 Conditional independence The conditional distribution is given by Conditional independence states that two events (A, B) are independent on a third set (C) if those two sets are independent in their conditional probability distribution given C782. This is stated as 8.2 Markov model The Markov chain describes a chain of memoryless states which transit from one state to another without dependency on previous states. The Markov property The transition matrix describes the probabilities of changing from one state to another. A property of this Markov matrix is that the column data sum to one. An example is provided here \\(\\left[\\begin{array}{cc}.8 &amp; .7\\\\.2 &amp;.3\\end{array}\\right]\\). The column probabilities sum to 1. The PageRank method that we discuss in chapter on Graph Theory is a special form of Markov chain. 8.3 INLA, Stan and BUGS In the example here, the GBD 2016 life time risk of stroke is re-used. The model is shown next to the simple linear regression. INLA is used to infer the probability of a set of data given the defined parameters. In the regression example below the linear regression return coefficient of 0.788 and the INLA version returns a mean value of 0.789 as well. Note that Bayesian methods do not provide p-values. 8.3.1 Linear regression library(INLA) ## This is INLA_23.09.09 built 2023-10-16 17:29:11 UTC. ## - See www.r-inla.org/contact-us for how to get help. load(&quot;./Data-Use/world_stroke.Rda&quot;) #perform ordinary linear regression for comparison fit&lt;-lm(LifeExpectancy~MeanLifetimeRisk, data =world_sfdf) summary(fit) ## ## Call: ## lm(formula = LifeExpectancy ~ MeanLifetimeRisk, data = world_sfdf) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.5141 -4.8413 0.1315 5.3673 10.5146 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.27490 1.59843 35.21 &lt;2e-16 *** ## MeanLifetimeRisk 0.78852 0.07655 10.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.223 on 173 degrees of freedom ## (71 observations deleted due to missingness) ## Multiple R-squared: 0.3802, Adjusted R-squared: 0.3766 ## F-statistic: 106.1 on 1 and 173 DF, p-value: &lt; 2.2e-16 Here the output of Bayesian analysis contrasts with the frequentist output above. #need to subset data as world_sfdf is of class &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; fitINLA&lt;-inla(LifeExpectancy~MeanLifetimeRisk, family = &quot;gaussian&quot;, data =world_sfdf[,c(23,12)]) summary(fitINLA) ## ## Call: ## c(&quot;inla.core(formula = formula, family = family, contrasts = contrasts, &quot;, &quot; ## data = data, quantiles = quantiles, E = E, offset = offset, &quot;, &quot; scale = ## scale, weights = weights, Ntrials = Ntrials, strata = strata, &quot;, &quot; lp.scale ## = lp.scale, link.covariates = link.covariates, verbose = verbose, &quot;, &quot; ## lincomb = lincomb, selection = selection, control.compute = control.compute, ## &quot;, &quot; control.predictor = control.predictor, control.family = control.family, ## &quot;, &quot; control.inla = control.inla, control.fixed = control.fixed, &quot;, &quot; ## control.mode = control.mode, control.expert = control.expert, &quot;, &quot; ## control.hazard = control.hazard, control.lincomb = control.lincomb, &quot;, &quot; ## control.update = control.update, control.lp.scale = control.lp.scale, &quot;, &quot; ## control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, &quot;, &quot; ## inla.call = inla.call, inla.arg = inla.arg, num.threads = num.threads, &quot;, &quot; ## keep = keep, working.directory = working.directory, silent = silent, &quot;, &quot; ## inla.mode = inla.mode, safe = FALSE, debug = debug, .parent.frame = ## .parent.frame)&quot; ) ## Time used: ## Pre = 1.15, Running = 2.4, Post = 0.24, Total = 3.79 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 56.275 1.599 53.137 56.275 59.413 56.275 0 ## MeanLifetimeRisk 0.789 0.077 0.638 0.789 0.939 0.789 0 ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant ## Precision for the Gaussian observations 0.026 0.003 0.021 0.026 0.032 ## mode ## Precision for the Gaussian observations 0.026 ## ## Marginal log-Likelihood: -587.98 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) Performing Bayesian analysis with rstan can be tricky as the text file for the stan code require an empty line at the end. The brms library contains the same precompiled stan code and is easier to run. #This may not run if there is incompatibility between Rtools and version of R #check system(&quot;g++ -v&quot;) library(brms) fitBRM&lt;-brm(LifeExpectancy~MeanLifetimeRisk, data =world_sfdf[,c(23,12)]) summary(fitBRM) Extract the posterior samples for plotting. 8.3.2 Logistic regression This example illustrates the use of INLA for logistic regression. #library(INLA) data(&quot;BreastCancer&quot;,package = &quot;mlbench&quot;) colnames(BreastCancer) ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; &quot;Cell.shape&quot; ## [5] &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; ## [9] &quot;Normal.nucleoli&quot; &quot;Mitoses&quot; &quot;Class&quot; #note Class is benign or malignant of class factor #need to convert this to numeric values #first convert to character BreastCancer$Class&lt;-as.character(BreastCancer$Class) BreastCancer$Class[BreastCancer$Class==&quot;benign&quot;]&lt;-0 BreastCancer$Class[BreastCancer$Class==&quot;malignant&quot;]&lt;-1 #convert factors to numeric BreastCancer2&lt;-lapply(BreastCancer[,-c(1,7)], as.numeric) BreastCancer2&lt;-as.data.frame(BreastCancer2) #return Class to data #convert character back to numeric BreastCancer2$Class&lt;-as.numeric(BreastCancer$Class) Dx&lt;-inla(Class ~Epith.c.size+Cl.thickness+Cell.shape, family=&quot;binomial&quot;, data = BreastCancer2) summary(Dx) ## ## Call: ## c(&quot;inla.core(formula = formula, family = family, contrasts = contrasts, &quot;, &quot; ## data = data, quantiles = quantiles, E = E, offset = offset, &quot;, &quot; scale = ## scale, weights = weights, Ntrials = Ntrials, strata = strata, &quot;, &quot; lp.scale ## = lp.scale, link.covariates = link.covariates, verbose = verbose, &quot;, &quot; ## lincomb = lincomb, selection = selection, control.compute = control.compute, ## &quot;, &quot; control.predictor = control.predictor, control.family = control.family, ## &quot;, &quot; control.inla = control.inla, control.fixed = control.fixed, &quot;, &quot; ## control.mode = control.mode, control.expert = control.expert, &quot;, &quot; ## control.hazard = control.hazard, control.lincomb = control.lincomb, &quot;, &quot; ## control.update = control.update, control.lp.scale = control.lp.scale, &quot;, &quot; ## control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, &quot;, &quot; ## inla.call = inla.call, inla.arg = inla.arg, num.threads = num.threads, &quot;, &quot; ## keep = keep, working.directory = working.directory, silent = silent, &quot;, &quot; ## inla.mode = inla.mode, safe = FALSE, debug = debug, .parent.frame = ## .parent.frame)&quot; ) ## Time used: ## Pre = 1.1, Running = 0.319, Post = 0.119, Total = 1.54 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) -8.496 0.709 -9.886 -8.496 -7.106 -8.496 0 ## Epith.c.size 0.483 0.123 0.242 0.483 0.724 0.483 0 ## Cl.thickness 0.644 0.094 0.459 0.644 0.829 0.644 0 ## Cell.shape 0.941 0.121 0.703 0.941 1.178 0.941 0 ## ## Marginal log-Likelihood: -114.89 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) The same data is used for logistic regression with stan. DxBRM&lt;-brm(Class ~Epith.c.size+Cl.thickness+Cell.shape, family=bernoulli(link = &quot;logit&quot;), data = BreastCancer2) summary(DxBRM) Extract the posterior sample from logistic regression. post_samples_DxBRM &lt;- brms::posterior_samples(DxBRM) post_samples_DxBRM %&gt;% select(-lp__) %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_wrap(~name, scales = &quot;free&quot;) 8.3.3 Mixed model Plotting world_sfdf to identify characteristics of the data library(ggplot2) ggplot(data=world_sfdf, aes(x=LifeExpectancy, y=MeanLifetimeRisk, color=Income, shape=Continent))+ geom_point()+ geom_jitter() Intercept model with INLA # Set prior on precision prec.prior &lt;- list(prec = list(param = c(0.001, 0.001))) Inla_Income&lt;-inla(MeanLifetimeRisk~1+LifeExpectancy+f(Income, model = &quot;iid&quot;, hyper = prec.prior), data =world_sfdf[,c(23,12, 15, 20)], control.predictor = list(compute = TRUE)) summary(Inla_Income) ## ## Call: ## c(&quot;inla.core(formula = formula, family = family, contrasts = contrasts, &quot;, &quot; ## data = data, quantiles = quantiles, E = E, offset = offset, &quot;, &quot; scale = ## scale, weights = weights, Ntrials = Ntrials, strata = strata, &quot;, &quot; lp.scale ## = lp.scale, link.covariates = link.covariates, verbose = verbose, &quot;, &quot; ## lincomb = lincomb, selection = selection, control.compute = control.compute, ## &quot;, &quot; control.predictor = control.predictor, control.family = control.family, ## &quot;, &quot; control.inla = control.inla, control.fixed = control.fixed, &quot;, &quot; ## control.mode = control.mode, control.expert = control.expert, &quot;, &quot; ## control.hazard = control.hazard, control.lincomb = control.lincomb, &quot;, &quot; ## control.update = control.update, control.lp.scale = control.lp.scale, &quot;, &quot; ## control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, &quot;, &quot; ## inla.call = inla.call, inla.arg = inla.arg, num.threads = num.threads, &quot;, &quot; ## keep = keep, working.directory = working.directory, silent = silent, &quot;, &quot; ## inla.mode = inla.mode, safe = FALSE, debug = debug, .parent.frame = ## .parent.frame)&quot; ) ## Time used: ## Pre = 0.919, Running = 0.451, Post = 0.294, Total = 1.66 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) 24.891 8477384.45 -17169403.82 22.019 17169473.002 22.853 0 ## LifeExpectancy 0.029 0.02 -0.01 0.029 0.068 0.029 0 ## ## Random effects: ## Name Model ## Income IID model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant ## Precision for the Gaussian observations 0.046 0.003 0.041 0.046 0.051 ## Precision for Income 0.000 0.000 0.000 0.000 0.000 ## mode ## Precision for the Gaussian observations 0.045 ## Precision for Income 0.000 ## ## Marginal log-Likelihood: -821.97 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) Intercept model with BRMS BRMS_Income &lt;- brm(MeanLifetimeRisk ~ 1 + LifeExpectancy+(1 | Income), data = world_sfdf, warmup = 100, iter = 200, chains = 2, inits = &quot;random&quot;, cores = 2) #use 2 CPU cores simultaneously instead of just 1. summary(BRMS_Income) post_samples_BRMS_Income &lt;- brms::posterior_samples(BRMS_Income) post_samples_BRMS_Income %&gt;% select(-lp__) %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_wrap(~name, scales = &quot;free&quot;) 8.3.4 Bayesian Metaanalysis A Bayesian approach towards metaanalysis is provided below using the package meta4diag (Guo and Riebler 2015). This approach uses the Integrated Nested Laplacian Approximations (INLA). This package takes a has an advantage over the mada package which does not provide a bivariate method for performing summary sensitivity and specificity. #install.packages(&quot;INLA&quot;,repos=c(getOption(&quot;repos&quot;),INLA=&quot;https://inla.r-inla-download.org/R/stable&quot;), dep=TRUE) library(meta4diag) ## Loading required package: shinyBS ## Loading required package: caTools ## ## Attaching package: &#39;caTools&#39; ## The following objects are masked from &#39;package:pracma&#39;: ## ## combs, trapz ## The following objects are masked from &#39;package:base64enc&#39;: ## ## base64decode, base64encode ## ## Attaching package: &#39;meta4diag&#39; ## The following objects are masked from &#39;package:meta&#39;: ## ## forest, funnel ## The following objects are masked from &#39;package:mada&#39;: ## ## AUC, crosshair, forest ## The following objects are masked from &#39;package:metafor&#39;: ## ## forest, funnel library(INLA) #data from spot sign metaanalysis Dat&lt;-read.csv(&quot;./Data-Use/ss150718.csv&quot;) #remove duplicates dat&lt;-subset(Dat, Dat$retain==&quot;yes&quot;) #the data can be viewed under res$data res &lt;- meta4diag(data = dat) #perform SROC SROC(res, crShow = T) Forest plot of sensitivity using the meta4diag package. #sensitivity is specified under accuracy.type #note that there are several different forest plot: mada, metafor, meta4diag meta4diag::forest(res, accuracy.type=&quot;sens&quot;, est.type=&quot;mean&quot;, p.cex=&quot;scaled&quot;, p.pch=15, p.col=&quot;black&quot;, nameShow=&quot;right&quot;, dataShow=&quot;center&quot;, estShow=&quot;left&quot;, text.cex=1, shade.col=&quot;gray&quot;, arrow.col=&quot;black&quot;, arrow.lty=1, arrow.lwd=1, cut=TRUE, intervals=c(0.025,0.975), main=&quot;Forest plot of Sensitivity&quot;, main.cex=1.5, axis.cex=1) Forest plot of specificity using the meta4diag package. #specificity is specified under accuracy.type #note that there are several different forest plot: mada, metafor, meta4diag meta4diag::forest(res, accuracy.type=&quot;spec&quot;, est.type=&quot;mean&quot;, p.cex=&quot;scaled&quot;, p.pch=15, p.col=&quot;black&quot;, nameShow=&quot;right&quot;, dataShow=&quot;center&quot;, estShow=&quot;left&quot;, text.cex=1, shade.col=&quot;gray&quot;, arrow.col=&quot;black&quot;, arrow.lty=1, arrow.lwd=1, cut=TRUE, intervals=c(0.025,0.975), main=&quot;Forest plot of Specificity&quot;, main.cex=1.5, axis.cex=1) 8.3.5 Cost The example below is provided by hesim. library(&quot;hesim&quot;) ## ## Attaching package: &#39;hesim&#39; ## The following object is masked from &#39;package:survMisc&#39;: ## ## autoplot ## The following object is masked from &#39;package:Matrix&#39;: ## ## expand ## The following object is masked from &#39;package:reshape&#39;: ## ## expand ## The following object is masked from &#39;package:tidyr&#39;: ## ## expand library(&quot;data.table&quot;) strategies &lt;- data.table(strategy_id = c(1, 2)) n_patients &lt;- 1000 patients &lt;- data.table(patient_id = 1:n_patients, age = rnorm(n_patients, mean = 70, sd = 10), female = rbinom(n_patients, size = 1, prob = .4)) states &lt;- data.table(state_id = c(1, 2), state_name = c(&quot;Healthy&quot;, &quot;Sick&quot;)) # Non-death health states tmat &lt;- rbind(c(NA, 1, 2), c(3, NA, 4), c(NA, NA, NA)) colnames(tmat) &lt;- rownames(tmat) &lt;- c(&quot;Healthy&quot;, &quot;Sick&quot;, &quot;Dead&quot;) transitions &lt;- create_trans_dt(tmat) transitions[, trans := factor(transition_id)] hesim_dat &lt;- hesim_data(strategies = strategies, patients = patients, states = states, transitions = transitions) print(hesim_dat) ## $strategies ## strategy_id ## 1: 1 ## 2: 2 ## ## $patients ## patient_id age female ## 1: 1 59.93828 1 ## 2: 2 71.29811 0 ## 3: 3 82.90199 1 ## 4: 4 77.88516 0 ## 5: 5 58.12814 1 ## --- ## 996: 996 63.90423 0 ## 997: 997 60.96895 0 ## 998: 998 76.53153 0 ## 999: 999 71.08916 0 ## 1000: 1000 82.42916 0 ## ## $states ## state_id state_name ## 1: 1 Healthy ## 2: 2 Sick ## ## $transitions ## transition_id from to from_name to_name trans ## 1: 1 1 2 Healthy Sick 1 ## 2: 2 1 3 Healthy Dead 2 ## 3: 3 2 1 Sick Healthy 3 ## 4: 4 2 3 Sick Dead 4 ## ## attr(,&quot;class&quot;) ## [1] &quot;hesim_data&quot; References "],["operational-research.html", "Chapter 9 Operational Research 9.1 Queueing theory 9.2 Linear Programming 9.3 Forecasting 9.4 Process mapping 9.5 Supply chains 9.6 Health economics", " Chapter 9 Operational Research 9.1 Queueing theory Queueing theory describes the movement of a queue such as customer arrival in bank, shop or emergency department. It seeks to balance supply and demand for a service. It begun with the study queue waiting on Danish telephones in 1909. Little’s theorem describes the linear relationship between the number of customer L to the customer arrival rate \\(\\lambda\\) and the customer served per time peiod, \\(\\mu\\). This can also be used to determine the number of beds needed for coronary care unit given 4 patients being admitted to cardiology unit, one of whom needs to be admitted to coronary care unit and would stay for an average of 3 days. Queueing system is described in terms of Kendall’s notation, M/M/c/k, with exponential arrival time. Using this terminology, MM1 system has 1 server and infinite queue. An MM/2/3 system has 2 (c) servers and 1 (k-c) position in the queue. The M refers to Markov chain. An example of a single server providing full service is a car wash. Example of a single multiphase server include different single stations in bank of withdrawing, deposit, information A counter at the airport or train station for economy and business passengers is considered multiserver single phase queue. A laundromat with different queues for washing and drying is an example of multiphase multiservers. A traditional queue at a shop can also be seen as first in first out with the first customer served first and leave first. An issue with FIFO is that people may queue early such as overnight queue for the latest iPhone. Alternatives include last in first out queue and priority queueing in emergency department. Let’s create a simple queue with 2 customers arriving per minute and 3 customers served per minute. The PO is the probability that the server is idle. library(queueing) lambda &lt;- 2 # 2 customers arriving per minute mu &lt;- 3 # 3 customers served per minute # MM1 mm1 &lt;- NewInput.MM1(lambda = 2, mu = 3, n = 0) # Create queue class object mm1_out &lt;- QueueingModel(mm1) # Report Report(mm1_out) ## The inputs of the M/M/1 model are: ## lambda: 2, mu: 3, n: 0 ## ## The outputs of the M/M/1 model are: ## ## The probability (p0, p1, ..., pn) of the n = 0 clients in the system are: ## 0.3333333 ## The traffic intensity is: 0.666666666666667 ## The server use is: 0.666666666666667 ## The mean number of clients in the system is: 2 ## The mean number of clients in the queue is: 1.33333333333333 ## The mean number of clients in the server is: 0.666666666666667 ## The mean time spend in the system is: 1 ## The mean time spend in the queue is: 0.666666666666667 ## The mean time spend in the server is: 0.333333333333333 ## The mean time spend in the queue when there is queue is: 1 ## The throughput is: 2 # Summary summary(mm1_out) ## lambda mu c k m RO P0 Lq Wq X L W Wqq Lqq ## 1 2 3 1 NA NA 0.6666667 0.3333333 1.333333 0.6666667 2 2 1 1 3 curve(dpois(x, mm1$lambda), from = 0, to = 20, type = &quot;b&quot;, lwd = 2, xlab = &quot;Number of customers&quot;, ylab = &quot;Probability&quot;, main = &quot;Poisson Distribution for Arrival Process&quot;, ylim = c(0, 0.4), n = 21) Lets examine M/M/3 queue with exponential inter-arrival times, exponential service times and 3 servers. library(queuecomputer) n &lt;- 100 arrivals &lt;- cumsum(rexp(n, 1.9)) service &lt;- rexp(n) mm3 &lt;- queue_step(arrivals = arrivals, service = service, servers = 3) Plot the arrival and departure times plot(mm3)[1] ## [[1]] Plot waiting time plot(mm3)[2] ## [[1]] Plot customer in queue plot(mm3)[3] ## [[1]] Plot customer and server status plot(mm3)[4] ## [[1]] Plot arrival and departure time plot(mm3)[5] ## [[1]] ## Discrete Event Simulations Discrete event simulation can be consider as modeling a complexity of system with multiple processes over time. This is different from continuous modeling of a system which evolve continuously with time. Discrete event simulation can be apply to the study of queue such as bank teller with a first in first out system. 9.1.1 Simulate capacity of system The example below is a based on examples provided in the simmer website for laundromat. library(simmer) ## ## Attaching package: &#39;simmer&#39; ## The following objects are masked from &#39;package:Rgraphviz&#39;: ## ## from, to ## The following object is masked from &#39;package:graph&#39;: ## ## join ## The following objects are masked from &#39;package:R.utils&#39;: ## ## reset, wrap ## The following object is masked from &#39;package:lava&#39;: ## ## wait ## The following object is masked from &#39;package:rmarkdown&#39;: ## ## run ## The following object is masked from &#39;package:raster&#39;: ## ## select ## The following object is masked from &#39;package:terra&#39;: ## ## wrap ## The following object is masked from &#39;package:bit&#39;: ## ## clone ## The following object is masked from &#39;package:crayon&#39;: ## ## reset ## The following object is masked from &#39;package:future&#39;: ## ## run ## The following object is masked from &#39;package:strucchange&#39;: ## ## monitor ## The following object is masked from &#39;package:modeltools&#39;: ## ## clone ## The following object is masked from &#39;package:rstatix&#39;: ## ## select ## The following object is masked from &#39;package:httr&#39;: ## ## timeout ## The following object is masked from &#39;package:R.oo&#39;: ## ## clone ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following object is masked from &#39;package:eegUtils&#39;: ## ## select ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## reset ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following objects are masked from &#39;package:tidygraph&#39;: ## ## activate, select ## The following object is masked from &#39;package:plyr&#39;: ## ## join ## The following objects are masked from &#39;package:lubridate&#39;: ## ## now, rollback ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## The following object is masked from &#39;package:tidyr&#39;: ## ## separate ## The following object is masked from &#39;package:NMF&#39;: ## ## run library(parallel) library(simmer.plot) ## Registered S3 method overwritten by &#39;simmer.plot&#39;: ## method from ## plot.list DALEX ## ## Attaching package: &#39;simmer.plot&#39; ## The following objects are masked from &#39;package:simmer&#39;: ## ## get_mon_arrivals, get_mon_attributes, get_mon_resources NUM_ANGIO &lt;- 1 # Number of machines for performing ECR ECRTIME &lt;- 1 # hours it takes to perform ECR~ 90/60 T_INTER &lt;- 13 # new patient every ~365*24/700 hours SIM_TIME &lt;- 24*30 # Simulation time over 30 days # setup set.seed(42) env &lt;- simmer() patient &lt;- trajectory() %&gt;% log_(&quot;arrives at the ECR&quot;) %&gt;% seize(&quot;removeclot&quot;, 1) %&gt;% log_(&quot;enters the ECR&quot;) %&gt;% timeout(ECRTIME) %&gt;% set_attribute(&quot;clot_removed&quot;, function() sample(50:99, 1)) %&gt;% log_(function() paste0(get_attribute(env, &quot;clot_removed&quot;), &quot;% of clot was removed&quot;)) %&gt;% release(&quot;removeclot&quot;, 1) %&gt;% log_(&quot;leaves the ECR&quot;) env %&gt;% add_resource(&quot;removeclot&quot;, NUM_ANGIO) %&gt;% # feed the trajectory with 4 initial patients add_generator(&quot;patient_initial&quot;, patient, at(rep(0, 4))) %&gt;% # new patient approx. every T_INTER minutes add_generator(&quot;patient&quot;, patient, function() sample((T_INTER-2):(T_INTER+2), 1)) %&gt;% # start the simulation run(SIM_TIME) ## 0: patient_initial0: arrives at the ECR ## 0: patient_initial0: enters the ECR ## 0: patient_initial1: arrives at the ECR ## 0: patient_initial2: arrives at the ECR ## 0: patient_initial3: arrives at the ECR ## 1: patient_initial0: 86% of clot was removed ## 1: patient_initial0: leaves the ECR ## 1: patient_initial1: enters the ECR ## 2: patient_initial1: 50% of clot was removed ## 2: patient_initial1: leaves the ECR ## 2: patient_initial2: enters the ECR ## 3: patient_initial2: 74% of clot was removed ## 3: patient_initial2: leaves the ECR ## 3: patient_initial3: enters the ECR ## 4: patient_initial3: 59% of clot was removed ## 4: patient_initial3: leaves the ECR ## 11: patient0: arrives at the ECR ## 11: patient0: enters the ECR ## 12: patient0: 67% of clot was removed ## 12: patient0: leaves the ECR ## 25: patient1: arrives at the ECR ## 25: patient1: enters the ECR ## 26: patient1: 98% of clot was removed ## 26: patient1: leaves the ECR ## 37: patient2: arrives at the ECR ## 37: patient2: enters the ECR ## 38: patient2: 74% of clot was removed ## 38: patient2: leaves the ECR ## 51: patient3: arrives at the ECR ## 51: patient3: enters the ECR ## 52: patient3: 95% of clot was removed ## 52: patient3: leaves the ECR ## 66: patient4: arrives at the ECR ## 66: patient4: enters the ECR ## 67: patient4: 75% of clot was removed ## 67: patient4: leaves the ECR ## 80: patient5: arrives at the ECR ## 80: patient5: enters the ECR ## 81: patient5: 96% of clot was removed ## 81: patient5: leaves the ECR ## 92: patient6: arrives at the ECR ## 92: patient6: enters the ECR ## 93: patient6: 90% of clot was removed ## 93: patient6: leaves the ECR ## 105: patient7: arrives at the ECR ## 105: patient7: enters the ECR ## 106: patient7: 76% of clot was removed ## 106: patient7: leaves the ECR ## 116: patient8: arrives at the ECR ## 116: patient8: enters the ECR ## 117: patient8: 86% of clot was removed ## 117: patient8: leaves the ECR ## 130: patient9: arrives at the ECR ## 130: patient9: enters the ECR ## 131: patient9: 54% of clot was removed ## 131: patient9: leaves the ECR ## 145: patient10: arrives at the ECR ## 145: patient10: enters the ECR ## 146: patient10: 83% of clot was removed ## 146: patient10: leaves the ECR ## 159: patient11: arrives at the ECR ## 159: patient11: enters the ECR ## 160: patient11: 89% of clot was removed ## 160: patient11: leaves the ECR ## 173: patient12: arrives at the ECR ## 173: patient12: enters the ECR ## 174: patient12: 82% of clot was removed ## 174: patient12: leaves the ECR ## 186: patient13: arrives at the ECR ## 186: patient13: enters the ECR ## 187: patient13: 73% of clot was removed ## 187: patient13: leaves the ECR ## 198: patient14: arrives at the ECR ## 198: patient14: enters the ECR ## 199: patient14: 64% of clot was removed ## 199: patient14: leaves the ECR ## 211: patient15: arrives at the ECR ## 211: patient15: enters the ECR ## 212: patient15: 57% of clot was removed ## 212: patient15: leaves the ECR ## 223: patient16: arrives at the ECR ## 223: patient16: enters the ECR ## 224: patient16: 53% of clot was removed ## 224: patient16: leaves the ECR ## 237: patient17: arrives at the ECR ## 237: patient17: enters the ECR ## 238: patient17: 94% of clot was removed ## 238: patient17: leaves the ECR ## 249: patient18: arrives at the ECR ## 249: patient18: enters the ECR ## 250: patient18: 54% of clot was removed ## 250: patient18: leaves the ECR ## 263: patient19: arrives at the ECR ## 263: patient19: enters the ECR ## 264: patient19: 83% of clot was removed ## 264: patient19: leaves the ECR ## 277: patient20: arrives at the ECR ## 277: patient20: enters the ECR ## 278: patient20: 84% of clot was removed ## 278: patient20: leaves the ECR ## 289: patient21: arrives at the ECR ## 289: patient21: enters the ECR ## 290: patient21: 75% of clot was removed ## 290: patient21: leaves the ECR ## 300: patient22: arrives at the ECR ## 300: patient22: enters the ECR ## 301: patient22: 55% of clot was removed ## 301: patient22: leaves the ECR ## 312: patient23: arrives at the ECR ## 312: patient23: enters the ECR ## 313: patient23: 52% of clot was removed ## 313: patient23: leaves the ECR ## 324: patient24: arrives at the ECR ## 324: patient24: enters the ECR ## 325: patient24: 51% of clot was removed ## 325: patient24: leaves the ECR ## 339: patient25: arrives at the ECR ## 339: patient25: enters the ECR ## 340: patient25: 59% of clot was removed ## 340: patient25: leaves the ECR ## 351: patient26: arrives at the ECR ## 351: patient26: enters the ECR ## 352: patient26: 82% of clot was removed ## 352: patient26: leaves the ECR ## 366: patient27: arrives at the ECR ## 366: patient27: enters the ECR ## 367: patient27: 88% of clot was removed ## 367: patient27: leaves the ECR ## 377: patient28: arrives at the ECR ## 377: patient28: enters the ECR ## 378: patient28: 94% of clot was removed ## 378: patient28: leaves the ECR ## 391: patient29: arrives at the ECR ## 391: patient29: enters the ECR ## 392: patient29: 58% of clot was removed ## 392: patient29: leaves the ECR ## 403: patient30: arrives at the ECR ## 403: patient30: enters the ECR ## 404: patient30: 61% of clot was removed ## 404: patient30: leaves the ECR ## 418: patient31: arrives at the ECR ## 418: patient31: enters the ECR ## 419: patient31: 58% of clot was removed ## 419: patient31: leaves the ECR ## 432: patient32: arrives at the ECR ## 432: patient32: enters the ECR ## 433: patient32: 84% of clot was removed ## 433: patient32: leaves the ECR ## 445: patient33: arrives at the ECR ## 445: patient33: enters the ECR ## 446: patient33: 65% of clot was removed ## 446: patient33: leaves the ECR ## 460: patient34: arrives at the ECR ## 460: patient34: enters the ECR ## 461: patient34: 77% of clot was removed ## 461: patient34: leaves the ECR ## 475: patient35: arrives at the ECR ## 475: patient35: enters the ECR ## 476: patient35: 77% of clot was removed ## 476: patient35: leaves the ECR ## 490: patient36: arrives at the ECR ## 490: patient36: enters the ECR ## 491: patient36: 67% of clot was removed ## 491: patient36: leaves the ECR ## 502: patient37: arrives at the ECR ## 502: patient37: enters the ECR ## 503: patient37: 67% of clot was removed ## 503: patient37: leaves the ECR ## 513: patient38: arrives at the ECR ## 513: patient38: enters the ECR ## 514: patient38: 95% of clot was removed ## 514: patient38: leaves the ECR ## 528: patient39: arrives at the ECR ## 528: patient39: enters the ECR ## 529: patient39: 85% of clot was removed ## 529: patient39: leaves the ECR ## 543: patient40: arrives at the ECR ## 543: patient40: enters the ECR ## 544: patient40: 85% of clot was removed ## 544: patient40: leaves the ECR ## 554: patient41: arrives at the ECR ## 554: patient41: enters the ECR ## 555: patient41: 67% of clot was removed ## 555: patient41: leaves the ECR ## 566: patient42: arrives at the ECR ## 566: patient42: enters the ECR ## 567: patient42: 62% of clot was removed ## 567: patient42: leaves the ECR ## 579: patient43: arrives at the ECR ## 579: patient43: enters the ECR ## 580: patient43: 68% of clot was removed ## 580: patient43: leaves the ECR ## 594: patient44: arrives at the ECR ## 594: patient44: enters the ECR ## 595: patient44: 78% of clot was removed ## 595: patient44: leaves the ECR ## 608: patient45: arrives at the ECR ## 608: patient45: enters the ECR ## 609: patient45: 93% of clot was removed ## 609: patient45: leaves the ECR ## 619: patient46: arrives at the ECR ## 619: patient46: enters the ECR ## 620: patient46: 70% of clot was removed ## 620: patient46: leaves the ECR ## 630: patient47: arrives at the ECR ## 630: patient47: enters the ECR ## 631: patient47: 97% of clot was removed ## 631: patient47: leaves the ECR ## 643: patient48: arrives at the ECR ## 643: patient48: enters the ECR ## 644: patient48: 87% of clot was removed ## 644: patient48: leaves the ECR ## 658: patient49: arrives at the ECR ## 658: patient49: enters the ECR ## 659: patient49: 62% of clot was removed ## 659: patient49: leaves the ECR ## 669: patient50: arrives at the ECR ## 669: patient50: enters the ECR ## 670: patient50: 58% of clot was removed ## 670: patient50: leaves the ECR ## 684: patient51: arrives at the ECR ## 684: patient51: enters the ECR ## 685: patient51: 92% of clot was removed ## 685: patient51: leaves the ECR ## 696: patient52: arrives at the ECR ## 696: patient52: enters the ECR ## 697: patient52: 91% of clot was removed ## 697: patient52: leaves the ECR ## 708: patient53: arrives at the ECR ## 708: patient53: enters the ECR ## 709: patient53: 78% of clot was removed ## 709: patient53: leaves the ECR ## 719: patient54: arrives at the ECR ## 719: patient54: enters the ECR ## simmer environment: anonymous | now: 720 | next: 720 ## { Monitor: in memory } ## { Resource: removeclot | monitored: TRUE | server status: 1(1) | queue status: 0(Inf) } ## { Source: patient_initial | monitored: 1 | n_generated: 4 } ## { Source: patient | monitored: 1 | n_generated: 56 } Plot the schematics of the simulation. plot(patient) Plot resource usage resource &lt;- get_mon_resources(env) plot(resource) Total queue size sum(resource$queue) #total queue size ## [1] 9 Number of people in queue of size 1 sum(resource$queue==1) #number of people in queue ## [1] 2 Number of peope in queue of size 2 sum(resource$queue==2) ## [1] 2 Plot arrival versus flow #View(get_mon_arrivals(env)) plot(env,what=&quot;arrivals&quot;,metric=&quot;flow_time&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; Next we simulate the process of running a stroke code. In this simulation the activities occur sequentially. #simulate ST6 patient &lt;- trajectory(&quot;patients&#39; path&quot;) %&gt;% ## add an intake activity - nurse arrive first on scene in ED to triage #seize specify priority seize(&quot;ed nurse&quot;, 1) %&gt;% #timeout return one random value from 5 mean+/-5 SD timeout(function() rnorm(1,5,5)) %&gt;% release(&quot;ed nurse&quot;, 1) %&gt;% ## add a registrar activity - stroke registrar arrive after stroke code seize(&quot;stroke reg&quot;, 1) %&gt;% timeout(function() rnorm(1, 10,5)) %&gt;% release(&quot;stroke reg&quot;, 1) %&gt;% #add CT scanning - the process takes 15 minutes seize(&quot;CT scan&quot;, 1) %&gt;% timeout(function() rnorm(1, 15,15)) %&gt;% release(&quot;CT scan&quot;, 1) %&gt;% #stroke reg re-enter #seize(&quot;stroke reg&quot;, 1) %&gt;% #timeout(function() rnorm(1, 10,5)) %&gt;% #release(&quot;stroke reg&quot;, 1) %&gt;% #branch ## add stroke consultant - to review scan and makes decision seize(&quot;stroke consultant&quot;, 1) %&gt;% timeout(function() rnorm(1, 5,10)) %&gt;% release(&quot;stroke consultant&quot;, 1) %&gt;% ## add a thrombectomy decision activity seize(&quot;inr&quot;, 1) %&gt;% timeout(function() rnorm(1, 5,5)) %&gt;% release(&quot;inr&quot;, 1) envs &lt;- mclapply(1:100, function(i) { simmer(&quot;SuperDuperSim&quot;) %&gt;% add_resource(&quot;ed nurse&quot;, 1) %&gt;% add_resource(&quot;stroke reg&quot;, 1) %&gt;% add_resource(&quot;CT scan&quot;, 1) %&gt;% add_resource(&quot;stroke consultant&quot;, 1) %&gt;% add_resource(&quot;inr&quot;, 1) %&gt;% add_generator(&quot;patient&quot;, patient, function() rnorm(1, 10, 2)) %&gt;% run(100) %&gt;% wrap() }) Plot patient flow plot(patient) #plot.simmer resources &lt;- get_mon_resources(envs) # p1&lt;-plot(resources, metric = &quot;usage&quot;, c(&quot;ed nurse&quot;,&quot;stroke reg&quot;,&quot;CT scan&quot;, &quot;stroke consultant&quot;,&quot;inr&quot;), items = &quot;serve&quot;) #resource usage p2&lt;-plot(get_mon_resources(envs[[6]]), metric = &quot;usage&quot;, &quot;stroke consultant&quot;, items = &quot;server&quot;, steps = TRUE) #resource utilisation p3&lt;-plot(resources, metric=&quot;utilization&quot;, c(&quot;ed nurse&quot;, &quot;stroke reg&quot;,&quot;CT scan&quot;)) #Flow time evolution arrivals &lt;- get_mon_arrivals(envs) p4&lt;-plot(arrivals, metric = &quot;flow_time&quot;) #combine plot gridExtra::grid.arrange(p1,p2, p3,p4) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; # activity&lt;-get_mon_arrivals(envs) plot(activity,metric=&quot;activity_time&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; Now we will fork another path in the patient flow 9.1.2 Queuing network mean_pkt_size &lt;- 100 # bytes lambda1 &lt;- 2 # pkts/s lambda3 &lt;- 0.5 # pkts/s lambda4 &lt;- 0.6 # pkts/s rate &lt;- 2.2 * mean_pkt_size # bytes/s # set an exponential message size of mean mean_pkt_size set_msg_size &lt;- function(.) set_attribute(., &quot;size&quot;, function() rexp(1, 1/mean_pkt_size)) # seize an M/D/1 queue by id; the timeout is function of the message size md1 &lt;- function(., id) seize(., paste0(&quot;md1_&quot;, id), 1) %&gt;% timeout(function() get_attribute(env, &quot;size&quot;) / rate) %&gt;% release(paste0(&quot;md1_&quot;, id), 1) to_queue_1 &lt;- trajectory() %&gt;% set_msg_size() %&gt;% md1(1) %&gt;% leave(0.25) %&gt;% md1(2) %&gt;% branch( function() (runif(1) &gt; 0.65) + 1, continue=c(F, F), trajectory() %&gt;% md1(3), trajectory() %&gt;% md1(4) ) to_queue_3 &lt;- trajectory() %&gt;% set_msg_size() %&gt;% md1(3) to_queue_4 &lt;- trajectory() %&gt;% set_msg_size() %&gt;% md1(4) env &lt;- simmer() for (i in 1:4) env %&gt;% add_resource(paste0(&quot;md1_&quot;, i)) env %&gt;% add_generator(&quot;arrival1_&quot;, to_queue_1, function() rexp(1, lambda1), mon=2) %&gt;% add_generator(&quot;arrival3_&quot;, to_queue_3, function() rexp(1, lambda3), mon=2) %&gt;% add_generator(&quot;arrival4_&quot;, to_queue_4, function() rexp(1, lambda4), mon=2) %&gt;% run(4000) ## simmer environment: anonymous | now: 4000 | next: 4000.01785654125 ## { Monitor: in memory } ## { Resource: md1_1 | monitored: TRUE | server status: 1(1) | queue status: 15(Inf) } ## { Resource: md1_2 | monitored: TRUE | server status: 1(1) | queue status: 1(Inf) } ## { Resource: md1_3 | monitored: TRUE | server status: 1(1) | queue status: 0(Inf) } ## { Resource: md1_4 | monitored: TRUE | server status: 0(1) | queue status: 0(Inf) } ## { Source: arrival1_ | monitored: 2 | n_generated: 8023 } ## { Source: arrival3_ | monitored: 2 | n_generated: 2031 } ## { Source: arrival4_ | monitored: 2 | n_generated: 2399 } res &lt;- get_mon_arrivals(env, per_resource = TRUE) %&gt;% subset(resource %in% c(&quot;md1_3&quot;, &quot;md1_4&quot;), select=c(&quot;name&quot;, &quot;resource&quot;)) arr &lt;- get_mon_arrivals(env) %&gt;% transform(waiting_time = end_time - (start_time + activity_time)) %&gt;% transform(generator = regmatches(name, regexpr(&quot;arrival[[:digit:]]&quot;, name))) %&gt;% merge(res) aggregate(waiting_time ~ generator + resource, arr, function(x) sum(x)/length(x)) ## generator resource waiting_time ## 1 arrival1 md1_3 6.8729025 ## 2 arrival3 md1_3 0.8427541 ## 3 arrival1 md1_4 6.7479945 ## 4 arrival4 md1_4 0.4598295 get_n_generated(env, &quot;arrival1_&quot;) + get_n_generated(env, &quot;arrival4_&quot;) ## [1] 10422 aggregate(waiting_time ~ generator + resource, arr, length) ## generator resource waiting_time ## 1 arrival1 md1_3 3824 ## 2 arrival3 md1_3 2030 ## 3 arrival1 md1_4 2188 ## 4 arrival4 md1_4 2398 9.2 Linear Programming Linear programming is an optimisation process to maximise profit and minimise cost with multiple parts of the model having linear relationship. There are several different libraries useful for linear programming. The lpSolve library is used here as illustration. library(lpSolve) #solve using linear programming n &lt;-2.5 # Numbers of techs. 1 EFT means a person is employed for 40 hours a week and 0.5 EFT means a person is employed for 20 hours a week. set_up_eeg &lt;- 40 # 40 minutes to_do_eeg &lt;- 30 # 30 minutes clean_equipment &lt;- 20 # 20 minutes annotate_eeg &lt;- 10 # 10 minutes # put some error for EEG time # if error=1 that mean NO errors happen error &lt;- 0.8 #change from 0.93 #Calculate time for EEG in hour eeg_case_time &lt;- ((set_up_eeg+to_do_eeg+clean_equipment+annotate_eeg)/60)*error # limit for EEG per day # we can put different limits for EEGs limit_eeg &lt;- round(8*eeg_case_time, digits = 0) #s[i] - numbers of cases for each i-EEG&#39;s machines #Setting the coefficients of s[i]-decision variables #In a future can put some efficiency or some cost objective.in=c(1,1,1,1,1) #Constraint Matrix const.mat=matrix(c(1,0,0,0,0, 0,1,0,0,0, 0,0,1,0,0, 0,0,0,1,0, 0,0,0,0,1, 1,1,1,1,1),nrow = 6,byrow = T) #defining constraints const_num_1=limit_eeg #in cases const_num_2=limit_eeg #in cases const_num_3=limit_eeg #in cases const_num_4=limit_eeg #in cases const_num_5=limit_eeg #in cases const_res= n*7 # limit per sessions #RHS for constraints const.rhs=c(const_num_1,const_num_2,const_num_3,const_num_4,const_num_5, const_res) #Direction for constraints constr.dir &lt;- rep(&quot;&lt;=&quot;,6) #Finding the optimum solution opt=lp(direction = &quot;max&quot;,objective.in,const.mat,constr.dir,const.rhs) #summary(opt) #Objective values of s[i] opt$solution ## [1] 11.0 6.5 0.0 0.0 0.0 Estimate for day (Value of objective function at optimal point) ## [1] 17.5 Estimate EEG per month based on staff EFT- only 2.5 ## [1] 366 Assuming that the time spend on a report by neurologists (1 report = 30 min) then in a 3.5 hour session a neurologist can report 7 EEG. neurologist_session=estimate_week/7 neurologist_session ## [1] 13.07143 9.3 Forecasting Forecasting is useful in predicting trends. In health care it can be used for estimating seasonal trends and bed requirement. Below is a forecast of mortality from COVID-19 in 2020. This forecast is an example and is not meant to be used in practice as mortality from COVID depends on the number of factors including infected cases, age, socioeconomic group, and comorbidity. library(tidyverse) library(prophet) library(hrbrthemes) ## Registering Windows fonts with R library(lubridate) library(readr) #use read_csv to read csv rather than base R covid&lt;-read_csv(&quot;./Data-Use/Covid_Table100420.csv&quot;) ## New names: ## • `` -&gt; `...1` ## • `COVID-19 Deaths` -&gt; `COVID-19 Deaths...5` ## • `COVID-19 Deaths` -&gt; `COVID-19 Deaths...6` ## • `Pneumonia Deaths*` -&gt; `Pneumonia Deaths*...7` ## • `Pneumonia Deaths*` -&gt; `Pneumonia Deaths*...8` ## • `Influenza Deaths` -&gt; `Influenza Deaths...9` ## • `Influenza Deaths` -&gt; `Influenza Deaths...10` ## Rows: 27 Columns: 12 ## ── Column specification ────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (8): ...1, Year, Week, COVID-19 Deaths...6, Pneumonia Deaths*...8, Influenza ... ## num (3): Total Deaths, COVID-19 Deaths...5, Pneumonia Deaths*...7 ## date (1): Date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. colnames(covid) ## [1] &quot;...1&quot; &quot;Year&quot; &quot;Week&quot; ## [4] &quot;Total Deaths&quot; &quot;COVID-19 Deaths...5&quot; &quot;COVID-19 Deaths...6&quot; ## [7] &quot;Pneumonia Deaths*...7&quot; &quot;Pneumonia Deaths*...8&quot; &quot;Influenza Deaths...9&quot; ## [10] &quot;Influenza Deaths...10&quot; &quot;Total.Deaths&quot; &quot;Date&quot; # A data frame with columns ds &amp; y (datetimes &amp; metrics) covid&lt;-rename(covid, ds =Date, y=Total.Deaths) covid2 &lt;- covid[c(1:12),] m&lt;-prophet(covid2)#create prophet object ## Disabling yearly seasonality. Run prophet with yearly.seasonality=TRUE to override this. ## Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this. ## Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. ## n.changepoints greater than number of observations. Using 8 # Extend dataframe 12 weeks into the future future &lt;- make_future_dataframe(m, freq=&quot;week&quot; , periods = 26) # Generate forecast for next 500 days forecast &lt;- predict(m, future) # What&#39;s the forecast for July 2020? forecasted_rides &lt;- forecast %&gt;% arrange(desc(ds)) %&gt;% dplyr::slice(1) %&gt;% pull(yhat) %&gt;% round() forecasted_rides ## [1] 67488 # Visualize forecast_p &lt;- plot(m, forecast) + labs(x = &quot;&quot;, y = &quot;mortality&quot;, title = &quot;Projected COVID-19 world mortality&quot;, subtitle = &quot;based on data truncated in January 2020&quot;) + ylim(20000,80000)+ theme_ipsum_rc() #forecast_p 9.3.1 Bed requirement 9.3.2 Length of stay 9.3.3 Customer churns Customer churns or turnover is an issue of interest in marketing. The corollary within healthcare is patients attendance at outpatient clinics, Insurance. The classical method used is GLM. 9.4 Process mapping library(DiagrammeR) a.plot&lt;-mermaid(&quot; graph TB A((Triage)) A--&gt;|2.3 hr|B(Imaging-No Stroke Code) A--&gt;|0.6 hr|B1(Imaging-Stroke Code) B--&gt;|14.6 hr|B2(Dysphagia Screen) B1--&gt;|no TPA 10.7 hr|B2(Dysphagia Screen) C(Stop NBM) B2--&gt;|0 hr|C C--&gt;|Oral route 1.7 hr|E{Antithrombotics} D1--&gt;|7.5 hr|E B--&gt;|PR route 6.8 hr|E B1--&gt;|PR route 3.8 hr|E B1--&gt;|TPA 24.7 hr|D1(Post TPA Scan) style A fill:#ADF, stroke:#333, stroke-width:2px style B fill:#9AA, stroke:#333, stroke-width:2px style B2 fill:#9AA, stroke:#333, stroke-width:2px style B1 fill:#879, stroke:#333, stroke-width:2px style C fill:#9AA, stroke:#333, stroke-width:2px style D1 fill:#879, stroke:#333, stroke-width:2px style E fill:#9C2, stroke:#9C2, stroke-width:2px &quot;) a.plot library(bupaR) ## ## Attaching package: &#39;bupaR&#39; ## The following object is masked from &#39;package:simmer&#39;: ## ## select ## The following object is masked from &#39;package:xgboost&#39;: ## ## slice ## The following object is masked from &#39;package:R.utils&#39;: ## ## timestamp ## The following object is masked from &#39;package:signal&#39;: ## ## filter ## The following object is masked from &#39;package:listenv&#39;: ## ## mapping ## The following object is masked from &#39;package:raster&#39;: ## ## select ## The following object is masked from &#39;package:matrixStats&#39;: ## ## count ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## slice ## The following object is masked from &#39;package:reshape&#39;: ## ## rename ## The following objects are masked from &#39;package:plyr&#39;: ## ## arrange, count, mutate, rename, summarise ## The following object is masked from &#39;package:oro.nifti&#39;: ## ## slice ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:utils&#39;: ## ## timestamp 9.5 Supply chains 9.6 Health economics 9.6.1 Cost library(&quot;hesim&quot;) library(&quot;data.table&quot;) strategies &lt;- data.table(strategy_id = c(1, 2)) n_patients &lt;- 1000 patients &lt;- data.table(patient_id = 1:n_patients, age = rnorm(n_patients, mean = 70, sd = 10), female = rbinom(n_patients, size = 1, prob = .4)) states &lt;- data.table(state_id = c(1, 2), state_name = c(&quot;Healthy&quot;, &quot;Sick&quot;)) # Non-death health states tmat &lt;- rbind(c(NA, 1, 2), c(3, NA, 4), c(NA, NA, NA)) colnames(tmat) &lt;- rownames(tmat) &lt;- c(&quot;Healthy&quot;, &quot;Sick&quot;, &quot;Dead&quot;) transitions &lt;- create_trans_dt(tmat) transitions[, trans := factor(transition_id)] hesim_dat &lt;- hesim_data(strategies = strategies, patients = patients, states = states, transitions = transitions) print(hesim_dat) ## $strategies ## strategy_id ## 1: 1 ## 2: 2 ## ## $patients ## patient_id age female ## 1: 1 61.99932 1 ## 2: 2 65.95321 0 ## 3: 3 56.97051 0 ## 4: 4 89.82190 1 ## 5: 5 48.45879 0 ## --- ## 996: 996 73.50196 1 ## 997: 997 81.33720 0 ## 998: 998 52.63094 1 ## 999: 999 80.48849 0 ## 1000: 1000 35.97193 0 ## ## $states ## state_id state_name ## 1: 1 Healthy ## 2: 2 Sick ## ## $transitions ## transition_id from to from_name to_name trans ## 1: 1 1 2 Healthy Sick 1 ## 2: 2 1 3 Healthy Dead 2 ## 3: 3 2 1 Sick Healthy 3 ## 4: 4 2 3 Sick Dead 4 ## ## attr(,&quot;class&quot;) ## [1] &quot;hesim_data&quot; Data from WHO on mortality rate can be extracted directly from WHO or by calling get_who_mr in heemod library. library(heemod) ## ## Attaching package: &#39;heemod&#39; ## The following object is masked from &#39;package:simmer&#39;: ## ## join ## The following object is masked from &#39;package:graph&#39;: ## ## join ## The following object is masked from &#39;package:mice&#39;: ## ## pool ## The following object is masked from &#39;package:terra&#39;: ## ## project ## The following object is masked from &#39;package:CHNOSZ&#39;: ## ## mix ## The following object is masked from &#39;package:plyr&#39;: ## ## join ## The following object is masked from &#39;package:purrr&#39;: ## ## modify There are several data in BCEA library such as Vaccine. library(BCEA) ## The BCEA version loaded is: 2.4.4 ## ## Attaching package: &#39;BCEA&#39; ## The following object is masked from &#39;package:raster&#39;: ## ## contour ## The following object is masked from &#39;package:terra&#39;: ## ## contour ## The following object is masked from &#39;package:graphics&#39;: ## ## contour #use Vaccine data from BCEA data(Vaccine) ints=c(&quot;Standard care&quot;,&quot;Vaccination&quot;) # Runs the health economic evaluation using BCEA m &lt;- bcea( e=eff, c=cost, # defines the variables of # effectiveness and cost ref=2, # selects the 2nd row of (e, c) # as containing the reference intervention interventions=treats, # defines the labels to be associated # with each intervention Kmax=50000, # maximum value possible for the willingness # to pay threshold; implies that k is chosen # in a grid from the interval (0, Kmax) plot=TRUE # plots the results ) "],["graph-theory.html", "Chapter 10 Graph Theory 10.1 Special graphs 10.2 Centrality Measures 10.3 Community 10.4 Visualising graph 10.5 Social Media and Network Analysis", " Chapter 10 Graph Theory Graph based methods are used daily without realisation of their origin. Examples include Google, Facebook social network, and roads. These methods have emerged as tools for interpreting and analysing connected network structures (Fornito 2016). For example, the brain structures and their association with disability can be framed as a network analysis. Matrix can be used to represent graphical data, relevant for the network analysis. An adjacency matrix represents the adjacency between different nodes. The nodes are represented in rows and colums. A value of one represent that node A is adjancence to node B. This graph is symmetrical as the value is also assigned between B and A. The diagonal of this graph consists of zeros as the relationship between A and A is zero. Direction can be introduced to a graph by representing the direction from A to B as one. In this case, a value of zero is attached from B to A. A graph consists of vertices (nodes) and edges (links). The edges can have direction in which case it is known as directed graph (digraph). Edge direction indicates flow from a source node to a destination node. The nodes and their directed edges can be represented as an adjacency matrix. A directed graph has an asymmetric adjacency matrix. library(igraph) aspects&lt;-graph(c(&quot;M1&quot;,&quot;M2&quot;,&quot;M2&quot;,&quot;Disability&quot;,&quot;tpa&quot;,&quot;M1&quot;,&quot;M2&quot;,&quot;M1&quot;,&quot;tpa&quot;,&quot;M2&quot;)) plot(aspects) #aspects is the name of the graph 10.1 Special graphs 10.1.1 Laplacian matrix The Laplacian matrix is the difference between the degree and adjacency matrix. The degree matrix represents the number of direct connection of a node. The diagonal of Laplacian matrix retains the diagonal values of the degree matrix (since the diagonal of the adjacency matrix consists of zeroes). The smallest eigenvalue of the Laplacian describes whether the graph is connected or not. The second eigenvalue of the Laplacian matrix is the algebraic connectivity or Fiedler value. High Fiedler value indicates greater number of connected components and consequently, resilience to breakdown in flow of information between members(Jamakovic and Mieghem 2008) . 10.1.2 Bimodal (bipartite) graph Bimodal graphs are of interest in social network and in analysis of food ecosystem in nature. A well studied example is the Zachary karate club network.This dataset is included in the igraphdata package. library(latentnet) ## Loading required package: network ## ## &#39;network&#39; 1.18.1 (2023-01-24), part of the Statnet Project ## * &#39;news(package=&quot;network&quot;)&#39; for changes since last version ## * &#39;citation(&quot;network&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## ## Attaching package: &#39;network&#39; ## The following objects are masked from &#39;package:igraph&#39;: ## ## %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices, ## get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite, ## is.directed, list.edge.attributes, list.vertex.attributes, ## set.edge.attribute, set.vertex.attribute ## The following object is masked from &#39;package:Hmisc&#39;: ## ## is.discrete ## The following object is masked from &#39;package:plyr&#39;: ## ## is.discrete ## Loading required package: ergm ## Registered S3 methods overwritten by &#39;ergm&#39;: ## method from ## simulate.formula lme4 ## simulate.formula_lhs lme4 ## summary.formula Hmisc ## ## &#39;ergm&#39; 4.5.0 (2023-05-27), part of the Statnet Project ## * &#39;news(package=&quot;ergm&quot;)&#39; for changes since last version ## * &#39;citation(&quot;ergm&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## &#39;ergm&#39; 4 is a major update that introduces some backwards-incompatible ## changes. Please type &#39;news(package=&quot;ergm&quot;)&#39; for a list of major changes. ## ## Attaching package: &#39;ergm&#39; ## The following object is masked from &#39;package:survMisc&#39;: ## ## gof ## The following object is masked from &#39;package:lava&#39;: ## ## gof ## ## &#39;latentnet&#39; 2.10.6 (2022-05-11), part of the Statnet Project ## * &#39;news(package=&quot;latentnet&quot;)&#39; for changes since last version ## * &#39;citation(&quot;latentnet&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## NOTE: BIC calculation prior to latentnet 2.7.0 had a bug in the calculation of the effective number of parameters. See help(summary.ergmm) for details. ## NOTE: Prior to version 2.8.0, handling of fixed effects for directed networks had a bug. See help(&quot;ergmm-terms&quot;) for details. #davis of social network data(davis) davis.fit&lt;-ergmm(davis~bilinear(d=2)+rsociality) plot(davis.fit,pie=TRUE,rand.eff=&quot;sociality&quot;,labels=TRUE) #davis[,1:10] 10.2 Centrality Measures The nodes in this case are variables such as ASPECTS regions, demographic and risk factors. This graph below uses data from a paper on the use of PageRank where a graph of 4 variables were used to illustrate the PageRank method for searching brain regions related to disability (Beare et al. 2015). #the network data is provided above on ASPECTS regions #degree is available in igraph, sna d&lt;-igraph::degree(aspects) #closeness is available in igraph, sna cl&lt;-igraph::closeness(aspects) #betweenness b&lt;-igraph::betweenness(aspects) #page rank p&lt;-page.rank(aspects) df&lt;-data.frame(&quot;degree&quot;=round(d,2),&quot;closeness&quot;=round(cl,2),&quot;betweenness&quot; =round(b,2),&quot;PageRank&quot; =round(p$vector,2)) knitr::kable(df) degree closeness betweenness PageRank M1 3 0.33 0 0.29 M2 4 0.50 2 0.37 Disability 1 NaN 0 0.25 tpa 2 0.25 0 0.09 10.2.1 Local centrality measures Centrality measures assign a measure of “importance” to nodes and can therefore indicate whether some nodes are more critical than others in a given network. When network nodes represent variables, centrality measures may indicate relevance of variables to a model. The simplest centrality measure, degree centrality, is the count of links for each node and is a purely local measure of importance. Node strength, used in weighted networks, is the sum of weights of edges entering or leaving (or both) the node. Other measures, such as betweenness centrality, describe more global structure - the degree of participation of a node as conduit of information between other nodes in a network. 10.2.2 Global centrality measures 10.2.2.1 Page Rank PageRank is one member of a family of graph eigenvector centrality measures, all of which incorporate the idea that the score of a node depends, at least in part, on the scores of neighbors connecting to the node. Thus a page may have a high PageRank score if many pages link to it, or if a few important or authoritative pages link to it. Others include eigenvector centrality (which works best with undirected graphs), alpha centrality and Katz centrality. PageRank uses a different scaling for connections (by the number of links leaving the node) and importance is based on incoming connections rather than outgoing connections(Brin and Page 1998) . Eigenvector centrality measure a node’s centrality in terms of node parameters and centrality of neighboring nodes. PageRank has several differences with respect to other eigenvector centrality methods, expanded below, which make it better suited for digraphs. PageRank was originally described in terms of a web user/surfer randomly clicking links, and the PageRank of a web page corresponds to the probability of the random surfer arriving at the page of interest. The model of the random surfer used in the PageRank computation includes a damping factor, which represents the chance of the random surfer becoming bored and selecting a completely different page at random (teleporting to a random page). Similarly, if a page is a sink (i.e. has no outgoing links), then the random web surfer may click on to a random page. A number of different approaches are available for computing the PageRank for nodes in a network. The conceptually simplest is to assign an equal initial score to each node, and then iteratively update PageRank scores. This is easy to perform algebraically for a small number of nodes but can take a long time with larger data. In practice, it is performed using eigenvector methode. PageRank analysis can be performed using igraph package. (Beare et al. 2015) Graph based methods have emerged as tools for interpreting and analysing connected network structures and in this case network structures associated with disability.These types of analysis are attractive because they assess the connectedness of each region of interest (ROI) with respect to other ROIs over the entire brain network.Eigenvector centrality methods have been used to explore connectedness of brain regions. PageRank is a variant of eigenvector centrality and is an ideal method for analysis ofdirected graph (the edges between adjacent nodes(regions) have direction). This method was initially developed as the basis of the search engine for Google. PageRank offered a considerable improvement over pure text based methods in ranking search results,and had the advantage of being content independent (i.e. the search is based on links between the web pages). PageRank emphasises web pages based on the number of links directed to a page and the importance of the sources of those links. Thus a small number of links from influential pages can greatly enhance the importance of the destination page. 10.3 Community 10.4 Visualising graph There are many packages for visualising graph such as igraph, sna, ggraph, _Rgraphviz, visnetwork, networkD3. 10.4.1 Visnetwork library(igraph) library(RColorBrewer) library(visNetwork) edge&lt;- read.csv(&quot;./Data-Use/TPA_edge.csv&quot;) #3 columns:V1 V2 time node&lt;-read.csv(&quot;./Data-Use/TPA_node.csv&quot;) df&lt;-graph_from_data_frame(d=edge[,c(1,2)],vertices = node,directed=F) V(df)$type&lt;-V(df)$name %in% edge[,1] #assign color vertex.label&lt;-V(df)$membership # Generate colors based on type: colrs &lt;- c(&quot;gray50&quot;, &quot;gold&quot;) V(df)$color &lt;- colrs[V(df)$membership] #assign shape shape &lt;- c(&quot;circle&quot;, &quot;square&quot;) V(df)$shape&lt;-shape[V(df)$membership] #extract data for visNetwork data&lt;-toVisNetworkData(df) visNetwork(nodes=data$nodes,edges=data$edges, main=&quot;TPA ECR Network&quot;)%&gt;% visNodes(color = list(hover = &quot;red&quot;)) %&gt;% visInteraction(hover = TRUE) 10.4.2 Large graph There are very few softwares capable of handling very large graph with the exception of Gephi , Cytoscape and Neo4J. 10.5 Social Media and Network Analysis Social media platform such as twitter, youtube, facebook and instagram are rich source of information for graph theory analysis as well as textmining. The following section only covers twitter and youtube as both are accessible to the public. There’s restricted access for facebook and instagram. 10.5.1 Twitter Analysis of Twitter requires creating an account on Twitter. This step will generate a series of keys listed below. These keys should be stored in a secret location. There are several different ways to access Twitter data. It should be noted that the data covers a range of 9 days and a maximum of 18000 tweets can be downloaded each day. The location of the tweeter can also be accessed if you have created an account with Google Maps API. library(rtweet) create_token( app= &quot;&quot;, consumer_key=&quot;&quot;, consumer_secret = &quot;&quot;, access_token = &quot;&quot;, access_secret = &quot;&quot; ) # searching for tweets on WHO CW &lt;- search_tweets(&quot;WHO&quot;, n = 18000, include_rts = TRUE) # searching for tweets confined y location searchTerm_t= (geocode= (&quot;-37.81363,144.9631,5km&quot;)) myTwitterData &lt;- Authenticate(&quot;twitter&quot;, apiKey=myapikey, apiSecret=myapisecret, accessToken=myaccesstoken, accessTokenSecret=myaccesstokensecret) %&gt;% Collect(searchTerm=searchTerm_t, numTweets=100, writeToFile=FALSE,verbose=TRUE) #alternately confined search for tweets on MS from Australia MStw &lt;- search_tweets( &quot;multiple sclerosis&quot;, geocode = lookup_coords(&quot;AUS&quot;),n = 18000, include_rts = FALSE ) rt &lt;- lat_lng(MStw)#extract lat and lon from tweets ## plot lat and lng points onto map with(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75))) leaflet:: leaflet(data=rt) %&gt;% addTiles () %&gt;% addCircles(lat=~lat,lng=~lng) ## plot time series of tweets ts_plot(MStw, &quot;weeks&quot;) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = &quot;bold&quot;)) ggplot2::labs( x = NULL, y = NULL, title = &quot;Frequency of #MS Twitter statuses from past 9 days&quot;, subtitle = &quot;Twitter status (tweet) counts aggregated using three-hour intervals&quot;, caption = &quot;\\nSource: Data collected from Twitter&#39;s REST API via rtweet&quot; ) 10.5.2 Youtube To perform analysis of comments on Youtube, a Google Developer’s account should be created. The apikey shoud be saved in a secret location. The analysis can be done by identifying the video of interest. library(SocialMediaLab) apiKey &lt;- &quot;&quot; videoIDs&lt;-c(&quot;YHzz2cXBlGk&quot;) #123 comments #406,253 views 2/2/18 #extract g_youtube_actor &lt;- Authenticate(&quot;youtube&quot;, apiKey= apiKey) %&gt;% Collect(videoIDs = videoIDs, writeToFile=TRUE) %&gt;% Create(&quot;Actor&quot;) References "],["geospatial-analysis.html", "Chapter 11 Geospatial analysis 11.1 Geocoding 11.2 Sp and sf objects 11.3 Thematic map 11.4 Spatial regression 11.5 Machine learning in spatial analysis 11.6 Spatio-temporal regression", " Chapter 11 Geospatial analysis This section deals with the use of geospatial analysis in healthcare. There is a detailed tutorial online available here 11.1 Geocoding There are several packages avalable for obtaining geocode or longitude and latitude of location. The tmaptools package provide free geocoding using OpenStreetMap or OSM overpass API. Both ggmap and googleway access Google Maps API and will require a key and payment for access. 11.1.1 OpenStreetMap This is a simple example to obtain the geocode of Monash Medical Centre. However, such a simple example does not always work without the full address. There are other libraries for accessing other data from OSM such as parks, restaurants etc. library(tmaptools) mmc&lt;-geocode_OSM (&quot;monash medical centre, clayton&quot;) mmc ## $query ## [1] &quot;monash medical centre, clayton&quot; ## ## $coords ## x y ## 145.12072 -37.92088 ## ## $bbox ## xmin ymin xmax ymax ## 145.12067 -37.92094 145.12077 -37.92083 The osmdata library includes function opg for extracting data from Overpass query. The list is available at https://wiki.openstreetmap.org/wiki/Map_features#Transportation. library(tidyverse ) library(osmdata) ## Data (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright library(sf) Note that getbb now returns an error “HTTP 405 Method Not Allowed”. It seems that there is an error with getbb function. # build a query query &lt;- getbb(bbox = &quot;Brisbane, QLD, australia&quot;) %&gt;% opq() %&gt;% add_osm_feature(key = &quot;amenity&quot;, value = &quot;community_centre&quot;) Vector for bbox can be obtained using nominatimlite. nominatim_polygon &lt;- nominatimlite::geo_lite_sf(address = &quot;Fairfield, NSW, Australia&quot;, points_only = FALSE) bboxsf &lt;- sf::st_bbox(nominatim_polygon) bboxsf ## xmin ymin xmax ymax ## 150.81120 -33.91416 150.99060 -33.82074 Extracting data on community centre in Brisbane. query &lt;- bboxsf %&gt;% opq() %&gt;% add_osm_feature(key = &quot;amenity&quot;, value = &quot;community_centre&quot;) query ## $bbox ## [1] &quot;-33.9141628,150.8112007,-33.8207365,150.9906033&quot; ## ## $prefix ## [1] &quot;[out:xml][timeout:25];\\n(\\n&quot; ## ## $suffix ## [1] &quot;);\\n(._;&gt;;);\\nout body;&quot; ## ## $features ## [1] &quot;[\\&quot;amenity\\&quot;=\\&quot;community_centre\\&quot;]&quot; ## ## $osm_types ## [1] &quot;node&quot; &quot;way&quot; &quot;relation&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;list&quot; &quot;overpass_query&quot; ## attr(,&quot;nodes_only&quot;) ## [1] FALSE Extracting data on fast food eateries in Brisbane. query_FF &lt;- bboxsf %&gt;% opq() %&gt;% add_osm_feature(key = &quot;amenity&quot;, value = &quot;fast_food&quot;) query_FF ## $bbox ## [1] &quot;-33.9141628,150.8112007,-33.8207365,150.9906033&quot; ## ## $prefix ## [1] &quot;[out:xml][timeout:25];\\n(\\n&quot; ## ## $suffix ## [1] &quot;);\\n(._;&gt;;);\\nout body;&quot; ## ## $features ## [1] &quot;[\\&quot;amenity\\&quot;=\\&quot;fast_food\\&quot;]&quot; ## ## $osm_types ## [1] &quot;node&quot; &quot;way&quot; &quot;relation&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;list&quot; &quot;overpass_query&quot; ## attr(,&quot;nodes_only&quot;) ## [1] FALSE Get information on tags for highway. Wikipedia has a page OpenStreetMap Wiki on highway. available_tags(&quot;highway&quot;) %&gt;% head() ## # A tibble: 6 × 2 ## Key Value ## &lt;chr&gt; &lt;chr&gt; ## 1 highway bridleway ## 2 highway bus_guideway ## 3 highway bus_stop ## 4 highway busway ## 5 highway construction ## 6 highway corridor Use the tags from above to define wide street. Plot the wide street using ggplot2. wide_streets &lt;- bboxsf%&gt;% opq()%&gt;% add_osm_feature(key = &quot;highway&quot;, value = c(&quot;motorway&quot;, &quot;primary&quot;, &quot;motorway_link&quot;, &quot;primary_link&quot;, &quot;secondary&quot;, &quot;secondary_link&quot;)) %&gt;% osmdata_sf() ggplot() + geom_sf(data = wide_streets$osm_lines, inherit.aes = FALSE, color = &quot;black&quot;) Service or lane way can be defined by service. narrow_streets &lt;- bboxsf%&gt;% opq()%&gt;% add_osm_feature(key = &quot;highway&quot;, value = c(&quot;service&quot;)) %&gt;% osmdata_sf() ggplot() + geom_sf(data = narrow_streets$osm_lines, inherit.aes = FALSE, color = &quot;blue&quot;) 11.1.2 Google Maps API The equivalent code in ggmap is provided below. Note that a key is required from Google Maps API. library(ggmap) register_google(key=&quot;Your Key&quot;) #geocode geocode (&quot;monash medical centre, clayton&quot;) #trip #mapdist(&quot;5 stud rd dandenong&quot;,&quot;monash medical centre&quot;) The next demonstration is the extraction of geocodes from multiple addresses embedded in a column of data within a data frame. This is more efficient compared to performing geocoding line by line. An example is provided on how to create your own icon on the leaflet document as well as taking a picture for publication. library(dplyr) library(tidyr) #unite verb from tidyr library(readr) library(tmaptools) library(leaflet) library(sf) clinic&lt;-read_csv(&quot;./Data-Use/TIA_clinics.csv&quot;) ## Rows: 25 Columns: 11 ## ── Column specification ────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (10): City, Country, Setting, Pre-COVID-Assessment, Post-COVID-Assessment, Pre... ## dbl (1): COVID Alert Level ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. clinic2&lt;-clinic %&gt;% unite (&quot;address&quot;,City:Country,sep = &quot;,&quot;)%&gt;% filter(!is.na(`Clinic-Status`)) load(&quot;./Data-Use/TIAclinics_geo.Rda&quot;) clinics_geo&lt;-left_join(clinics_geo,clinic2, by=c(&quot;query&quot;=&quot;address&quot;)) #create icon markers #icon markers icons_blue &lt;- awesomeIcons( icon= &#39;medkit&#39;, iconColor = &#39;black&#39;, library = &#39;ion&#39;, markerColor = &quot;blue&quot; ) icons_red &lt;- awesomeIcons( icon= &#39;medkit&#39;, iconColor = &#39;black&#39;, library = &#39;ion&#39;, markerColor = &quot;red&quot; ) #subset clinics_geo_active&lt;-clinics_geo %&gt;%filter(`Clinic-Status`==&quot;Active&quot;) clinics_geo_inactive&lt;-clinics_geo %&gt;%filter(`Clinic-Status` !=&quot;Active&quot;) m&lt;-leaflet(data=clinics_geo) %&gt;% addTiles() %&gt;% #default is OSM addAwesomeMarkers(lat=clinics_geo_active$lat,lng=clinics_geo_active$lon, icon=icons_blue,label = ~as.character(clinics_geo_active$query) ) %&gt;% addAwesomeMarkers(lat=clinics_geo_inactive$lat,lng=clinics_geo_inactive$lon, icon=icons_red,label = ~as.character(clinics_geo_inactive$query)) #make pics using mapshot mapview::mapshot(m, url = paste0(getwd(), file=&quot;./Data-Use/TIAclinic_world.html&quot;), file = paste0(getwd(), &quot;./Data-Use/TIAclinic_world.png&quot;)) m Googleway has the flexibility of easily interrogating Google Maps API for time of trip and traffic condition. library(googleway) key=&quot;Your Key&quot; #trip to MMC #traffic model can be optimistic, best guess, pessimistic google_distance(&quot;5 stud rd dandenong&quot;,&quot;monash medical centre&quot;, key=key, departure_time=as.POSIXct(&quot;2019-12-03 08:15:00 AEST&quot;),traffic_model = &quot;optimistic&quot;) 11.2 Sp and sf objects There are several methods for reading the shapefile data. Previously rgdal library was used. This approach creates files which can be described as S4 object in that there are slots for different data. The spatial feature sf approach is much easier to handle and the data can easily be subset and merged if needed. An example of conversion between sp and sf is provided. Here, base R plot is used to illustrate the shapefile of Melbourne and after parcellation into Voronois, centred by the hospital location. library(dismo) ## ## Attaching package: &#39;dismo&#39; ## The following object is masked from &#39;package:graph&#39;: ## ## threshold ## The following object is masked from &#39;package:R.utils&#39;: ## ## evaluate ## The following object is masked from &#39;package:evaluate&#39;: ## ## evaluate ## The following object is masked from &#39;package:party&#39;: ## ## response ## The following object is masked from &#39;package:generics&#39;: ## ## evaluate library(ggvoronoi) library(tidyverse) library(sf) par(mfrow=c(1,2)) #plot 2 objects in 1 row msclinic&lt;-read.csv(&quot;./Data-Use/msclinic.csv&quot;) %&gt;% filter(clinic==1, metropolitan==1) #convert to spatialpointdataframe coordinates(msclinic) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) #proj4string(msclinic) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) proj4string(msclinic) &lt;- CRS(&quot;+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs&quot;) #create voronoi from msclinic data #msclinic object is in sp msv&lt;-voronoi(msclinic) #create voronoi from msclinic data #object is in sp msv&lt;-voronoi(msclinic) #subset Greater Melbourne Melb&lt;-st_read(&quot;./Data-Use/GCCSA_2016_AUST.shp&quot;) %&gt;% filter(STE_NAME16==&quot;Victoria&quot;,GCC_NAME16==&quot;Greater Melbourne&quot;) ## Reading layer `GCCSA_2016_AUST&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\GCCSA_2016_AUST.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 34 features and 5 fields (with 18 geometries empty) ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 96.81694 ymin: -43.74051 xmax: 167.998 ymax: -9.142176 ## Geodetic CRS: GDA94 #transform sf to sp object Melb&lt;-as(Melb,Class=&quot;Spatial&quot;) plot(Melb) #voronoi bound by greater melbourne vor &lt;- dismo::voronoi(msclinic, ext=extent(Melb)) #intersect is present in base R, dplyr, raster, lubridate etc r &lt;- raster::intersect(vor, Melb) #r&lt;-intersect(msv,gccsa_fsp) # msclinic$hospital&lt;-as.character(msclinic$hospital) plot(r, col=rainbow(length(r)), lwd=3) #sp back to sf #error with epsg conversion back msclinic_sf&lt;-st_as_sf(msclinic,crs=4283) 11.2.1 Obtain the Brisbane data as sf object. ComCentre &lt;- osmdata::osmdata_sf(query) names(ComCentre$osm_points) ## [1] &quot;osm_id&quot; &quot;name&quot; &quot;addr:city&quot; &quot;addr:housenumber&quot; ## [5] &quot;addr:postcode&quot; &quot;addr:province&quot; &quot;addr:street&quot; &quot;amenity&quot; ## [9] &quot;operator&quot; &quot;phone&quot; &quot;geometry&quot; Extract the centroid and polygons. Check that the coordinate reference system is EPSG 4326 or World Geodetic System (WGS) 84. ComPoint&lt;-ComCentre$osm_points %&gt;% filter(amenity==&quot;community_centre&quot;) ComPoly &lt;- ComCentre$osm_polygons %&gt;% st_centroid() ## Warning: st_centroid assumes attributes are constant over geometries Com &lt;- bind_rows(ComPoly, ComPoint) st_crs(Com) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] Plot the map of community centres in Ballarat with tmap. The view argument in tmap_mode set this plotting to interactive viewing with tmap with leaflet. The leaflet library can be called directly using tmap_leaflet #extract bounding box for Brisbane #Brisbane &lt;- osmdata::getbb(&quot;Brisbane, australia&quot;, format_out = &quot;sf_polygon&quot;)$multipolygon Brisbane&lt;-nominatim_polygon$geometry library(tmap) #set interactive view tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing #plot outline of Ballarat tm_shape(Brisbane)+ tm_borders()+ #plot community centres tm_shape(Com) + tm_dots() 11.3 Thematic map In the first chapter we provided a thematic map example with ggplot2. here we will illustrate with mapview library using open data on Finland. library(geofi) library(ggplot2) library(tmap) library(tidyverse) library(tidyr) library(pxweb) ## pxweb 0.16.2: R tools for the PX-WEB API. ## https://github.com/ropengov/pxweb library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following object is masked from &#39;package:raster&#39;: ## ## crosstab ## The following object is masked from &#39;package:terra&#39;: ## ## crosstab ## The following object is masked from &#39;package:rstatix&#39;: ## ## make_clean_names ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test #data on Finland #https://pxnet2.stat.fi/PXWeb/pxweb/en/Kuntien_avainluvut/ #Kuntien_avainluvut__2021/kuntien_avainluvut_2021_viimeisin.px/table/ #tableViewLayout1/ FinPop&lt;-readxl::read_xlsx(&quot;./Data-Use/Kuntien avainluvut_20210704-153529.xlsx&quot;, skip = 2) ## New names: ## • `` -&gt; `...1` FinPop2&lt;-FinPop[-c(1),] #remove data for entire Finland #get shapefile for municipality municipalities &lt;- get_municipalities(year = 2020, scale = 4500) #sf object) ## Requesting response from: http://geo.stat.fi/geoserver/wfs?service=WFS&amp;version=1.0.0&amp;request=getFeature&amp;typename=tilastointialueet%3Akunta4500k_2020 ## Warning: Coercing CRS to epsg:3067 (ETRS89 / TM35FIN) ## Data is licensed under: Attribution 4.0 International (CC BY 4.0) #join shapefile and population data municipalities2&lt;-right_join(municipalities, FinPop2, by=c(&quot;name&quot;=&quot;...1&quot;)) %&gt;% rename(Pensioner=`Proportion of pensioners of the population, %, 2019`,Age65=`Share of persons aged over 64 of the population, %, 2020`) %&gt;% na.omit() #plot map using mapview mapview::mapview(municipalities2[&quot;Age65&quot;],layer.name=&quot;Age65&quot;) This example illustrates how to add arguments in mapview library(mapview) library(sf) library(tmaptools) #NY Shape file NYsf&lt;-st_read(&quot;./Data-Use/Borough_Boundaries/geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp&quot;) ## Reading layer `geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\Borough_Boundaries\\geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 5 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -74.25559 ymin: 40.49612 xmax: -73.70001 ymax: 40.91553 ## Geodetic CRS: WGS84(DD) #NY subway -does not go to Staten Island NYsubline&lt;-st_read(&quot;./Data-Use/NYsubways/geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp&quot;) ## Reading layer `geo_export_147781bc-e472-4c12-8cd2-5f9859f90706&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\NYsubways\\geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 742 features and 6 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -74.03088 ymin: 40.57559 xmax: -73.75541 ymax: 40.90312 ## Geodetic CRS: WGS84(DD) #NY subway stations NYsubstation&lt;-st_read(&quot;./Data-Use/NYsubways/geo_export_0dab2fcf-79b8-409a-b940-7c98778a4418.shp&quot;) ## Reading layer `geo_export_0dab2fcf-79b8-409a-b940-7c98778a4418&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\NYsubways\\geo_export_0dab2fcf-79b8-409a-b940-7c98778a4418.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 473 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -74.03088 ymin: 40.57603 xmax: -73.7554 ymax: 40.90313 ## Geodetic CRS: WGS84(DD) #list of NY hospitals Hosp&lt;-c(&quot;North Shore University Hospital&quot;,&quot;Long Island Jewish Medical Centre&quot;,&quot;Staten Island University Hospital&quot;,&quot;Lennox Hill Hospital&quot;,&quot;Long Island Jewish Forest Hills&quot;,&quot;Long Island Jewish Valley Stream&quot;,&quot;Plainview Hospital&quot;,&quot;Cohen Children&#39;s Medical Center&quot;,&quot;Glen Cove Hospital&quot;,&quot;Syosset Hospital&quot;) #geocode NY hospitals and return sf object Hosp_geo&lt;-geocode_OSM(paste0(Hosp,&quot;,&quot;,&quot;New York&quot;,&quot;,&quot;,&quot;USA&quot;),as.sf = TRUE) ## No results found for &quot;Long Island Jewish Medical Centre,New York,USA&quot;. ## No results found for &quot;Lennox Hill Hospital,New York,USA&quot;. #data from jama paper on variation in mortality from covid mapview(NYsf, zcol=&quot;boro_name&quot;)+ mapview(NYsubline, zol=&quot;name&quot;)+ #cex is the circle size default=6 mapview(NYsubstation, zol=&quot;line&quot;,cex=1)+ mapview(Hosp_geo, zcol=&quot;query&quot;, cex=3) The example shown under Data wrangling on how to extract data from pdf is now put to use to create thematic map of stroke number per region in Denmark. library(dplyr) library(tidyverse) library(sf) library(eurostat) library(leaflet) library(mapview) library(tmaptools) #####################code to generate HospLocations.Rda load (&quot;./Data-Use/europeRGDK.Rda&quot;) #create data on hospitals #hosp_addresses &lt;- c( # AarhusHospital = &quot;aarhus university hospital, aarhus, Denmark&quot;, # AalborgHospital = &quot;Hobrovej 18-22 9100 aalborg, Denmark&quot;, # HolstebroHospital = &quot;Lægårdvej 12 7500 Holstebro, Holstebro, Denmark&quot;, VejleHospital=&quot;Vejle Sygehus,Beriderbakken 4, Vejle, Denmark&quot;, # EsbjergHospital=&quot;Esbjerg Sygehus, Esbjerg, Denmark&quot;, #SoenderborgHospital=&quot;Sydvang 1C 6400 Sønderborg, Denmark&quot;, #OdenseHospital=&quot;Odense Sygehus, Odense, Denmark&quot;, #RoskildeHospital=&quot;Roskilde Sygehus, Roskilde, Denmark&quot;, #BlegdamsvejHospital=&quot;Rigshospitalet blegdamsvej, 9 Blegdamsvej, København, Denmark&quot;, #GlostrupHospital=&quot;Rigshospitalet Glostrup, Glostrup, Denmark&quot;) #geocode hospitals using OpenStreetMap. This function works better with street addresses #HospLocations &lt;- tmaptools::geocode_OSM(hosp_addresses, as.sf=TRUE) #convert data into sf object #HospLocations &lt;- sf::st_transform(HospLocations, sf::st_crs(europeRGDK)) #CSC comprehensive stroke centre #PSC primary stroke centre #HospLocations$Center&lt;-c(&quot;CSC&quot;, &quot;PSC&quot;, &quot;PSC&quot;, &quot;PSC&quot;, &quot;PSC&quot;, &quot;PSC&quot;, &quot;CSC&quot;, &quot;PSC&quot;, &quot;CSC&quot;, &quot;PSC&quot;) #save HospLocations #save(HospLocations, &quot;HospLocations.Rda&quot;) load(&quot;./Data-Use/HospLocations.Rda&quot;) ##https://ec.europa.eu/eurostat/web/nuts/background load(&quot;./Data-Use/euro_nuts2_sf.Rda&quot;) DKnuts2_sf&lt;- euro_nuts2_sf%&gt;% filter(str_detect(NUTS_ID,&quot;^DK&quot;)) ## old-style crs object detected; please recreate object with a recent sf::st_crs() #convert pdf to csv file dk&lt;-read.csv(&quot;./Data-Use/denmarkstrokepdf.csv&quot;) #extract only data on large regions=NUTS2 dk2&lt;-dk[c(4:8),] #clean up column X.1 containing stroke data #remove numerator before back slash then remove number before slash sign dk2$strokenum&lt;-str_replace(dk2$X.1,&quot;[0-9]*&quot;,&quot;&quot;) %&gt;% str_replace(&quot;/&quot;,&quot;\\\\&quot;) %&gt;% as.numeric() dk2$Uoplyst&lt;-str_replace(dk2$Uoplyst,&quot;SjÃ¦lland&quot;,&quot;Sjælland&quot;) #merge sf file for DK nuts2 with stroke number DKnuts2_sf2&lt;-right_join(DKnuts2_sf,dk2,by=c(&quot;NUTS_NAME&quot;=&quot;Uoplyst&quot;)) #add population from Statistics Denmark 2018 DKnuts2_sf2$pop&lt;-c(589148,1822659,835024, 1313596,1220763) DKnuts2_sf2$male&lt;-c(297679,894553,416092,657817,610358) DKnuts2_sf2$maleper&lt;-round(with(DKnuts2_sf2,pop/male),2) #plot map mapview(DKnuts2_sf2[&quot;strokenum&quot;], layer.name=&quot;Stroke Number&quot;) +mapview(HospLocations, zcol=&quot;Center&quot;, layer.name=&quot;Hospital Designation&quot;) ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() 11.3.1 Calculate distance to Hospital-OpenStreetMap Determine distance hospital to centroid of kommunes rather than the larger regions of Denmark. This can be performed with OpenStreetMap or Google Maps API. dist_to_loc &lt;- function (geometry, location){ units::set_units(st_distance(st_centroid (geometry), location)[,1], km) } #set distance 10 km #change to 100 km dist_range &lt;- units::set_units(100, km) ## europeRGDK &lt;- mutate(europeRGDK, DirectDistanceToAarhus = dist_to_loc(geometry,HospLocations[&quot;AarhusHospital&quot;, ]), DirectDistanceToAalborg = dist_to_loc(geometry,HospLocations[&quot;AalborgHospital&quot;, ]), DirectDistanceToHolstebro = dist_to_loc(geometry,HospLocations[&quot;HolstebroHospital&quot;, ]), DirectDistanceToVejle = dist_to_loc(geometry,HospLocations[&quot;VejleHospital&quot;, ]), DirectDistanceToEsbjerg = dist_to_loc(geometry,HospLocations[&quot;EsbjergHospital&quot;, ]), DirectDistanceToSoenderborg = dist_to_loc(geometry,HospLocations[&quot;SoenderborgHospital&quot;, ]), DirectDistanceToOdense = dist_to_loc(geometry,HospLocations[&quot;OdenseHospital&quot;, ]), DirectDistanceToRoskilde = dist_to_loc(geometry,HospLocations[&quot;RoskildeHospital&quot;, ]), DirectDistanceToBlegdamsvej = dist_to_loc(geometry,HospLocations[&quot;BlegdamsvejHospital&quot;, ]), DirectDistanceToGlostrup = dist_to_loc(geometry,HospLocations[&quot;GlostrupHospital&quot;, ]), # DirectDistanceToNearest = pmin(DirectDistanceToAarhus, DirectDistanceToAalborg,DirectDistanceToHolstebro, DirectDistanceToVejle,DirectDistanceToEsbjerg, DirectDistanceToSoenderborg,DirectDistanceToOdense, DirectDistanceToRoskilde,DirectDistanceToBlegdamsvej, DirectDistanceToGlostrup)) ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() ## old-style crs object detected; please recreate object with a recent sf::st_crs() StrokeHosp &lt;- filter(europeRGDK, DirectDistanceToNearest &lt; dist_range) %&gt;% mutate(Postcode = as.numeric(COMM_ID)) ## old-style crs object detected; please recreate object with a recent sf::st_crs() head(StrokeHosp) ## Simple feature collection with 6 features and 15 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 8.709358 ymin: 56.83344 xmax: 8.940572 ymax: 56.98483 ## Geodetic CRS: +proj=longlat +ellps=GRS80 +no_defs ## COMM_ID Shape_Leng Shape_Area geometry ## 1 DK10817738668 0.3605216 0.0030530589 MULTIPOLYGON (((8.940572 56... ## 2 DK10817738669 0.1539856 0.0012336619 MULTIPOLYGON (((8.908073 56... ## 3 DK10817738670 0.1630955 0.0009363543 MULTIPOLYGON (((8.886687 56... ## 4 DK10817738671 0.2010890 0.0019239206 MULTIPOLYGON (((8.822215 56... ## 5 DK10817738672 0.1926252 0.0011456026 MULTIPOLYGON (((8.750229 56... ## 6 DK10817738673 0.1793186 0.0011254477 MULTIPOLYGON (((8.83318 56.... ## DirectDistanceToAarhus DirectDistanceToAalborg DirectDistanceToHolstebro ## 1 117.7155 [km] 68.11356 [km] 64.41291 [km] ## 2 115.7763 [km] 65.96041 [km] 64.48420 [km] ## 3 114.0335 [km] 67.75456 [km] 60.72506 [km] ## 4 118.1121 [km] 73.63410 [km] 59.09770 [km] ## 5 119.4927 [km] 76.87186 [km] 57.33305 [km] ## 6 114.0855 [km] 72.38971 [km] 55.92370 [km] ## DirectDistanceToVejle DirectDistanceToEsbjerg DirectDistanceToSoenderborg ## 1 140.9514 [km] 163.4109 [km] 230.7453 [km] ## 2 139.8147 [km] 163.2912 [km] 229.6966 [km] ## 3 136.7127 [km] 159.5629 [km] 226.5046 [km] ## 4 138.2621 [km] 158.3096 [km] 227.7350 [km] ## 5 138.0619 [km] 156.6019 [km] 227.3109 [km] ## 6 134.1065 [km] 154.9979 [km] 223.6369 [km] ## DirectDistanceToOdense DirectDistanceToRoskilde DirectDistanceToBlegdamsvej ## 1 195.3515 [km] 245.3869 [km] 266.3579 [km] ## 2 193.8254 [km] 243.2411 [km] 264.1433 [km] ## 3 191.2311 [km] 242.1564 [km] 263.3819 [km] ## 4 193.8937 [km] 246.9101 [km] 268.4274 [km] ## 5 194.3217 [km] 248.7312 [km] 270.4758 [km] ## 6 189.6643 [km] 243.1370 [km] 264.8318 [km] ## DirectDistanceToGlostrup DirectDistanceToNearest Postcode ## 1 258.4642 [km] 64.41291 [km] 28310 ## 2 256.2729 [km] 64.48420 [km] 28311 ## 3 255.3919 [km] 60.72506 [km] 28312 ## 4 260.3345 [km] 59.09770 [km] 28313 ## 5 262.3009 [km] 57.33305 [km] 28314 ## 6 256.6704 [km] 55.92370 [km] 28315 11.4 Spatial regression This is data published in Jama 29/4/2020 on COVD-19 in New York. The New York borough shapefiles were obtained from New York Open Data at https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm. For those wishing to evaluate other datasets, there’s lung and lip cancer data in SpatialEpi library, leukemia in DClusterm library. Key aspect of spatial regression is that neighbouring regions are similar and distant regions are less so. It uses the polyn2nb in spdep library to create the neighbourhood weight. The map of residual for the New York data does not suggest spatial association of residuals. The Moran’s I is used to test for global spatial autocorrelation among adjacent regions in multidimensional space. Moran’s I requires calculation of neighbourhood. It is related to the number of regions and sum of all spatial weights. 11.4.1 New York COVID-19 mortality The example below illustrate the need to look at the data. It is not possible to perform spatial regression given the small size of the areal data (4 connected boroughs); Staten Island does not have adjoining border with the others nor share subway system. An intriguing possibility is that areal data analysis at the neighbourhood level would allow a granular examination of socioeconomic effect of COVID-19 on mortality data. To plot the railway lines with tmap the argument tm_lines is required whereas the argument tm_polygons is better suited for plotting the shape file. It library(leaflet) library(SpatialEpi) ## ## Attaching package: &#39;SpatialEpi&#39; ## The following object is masked from &#39;package:ggraph&#39;: ## ## circle library(spdep) ## Loading required package: spData ## To access larger datasets in this package, install the spDataLarge package ## with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` ## ## Attaching package: &#39;spdep&#39; ## The following object is masked from &#39;package:bslib&#39;: ## ## card ## The following object is masked from &#39;package:ade4&#39;: ## ## mstree library(spatialreg) #some of spdep moved to spatialreg ## ## Attaching package: &#39;spatialreg&#39; ## The following objects are masked from &#39;package:spdep&#39;: ## ## get.ClusterOption, get.coresOption, get.mcOption, get.VerboseOption, ## get.ZeroPolicyOption, set.ClusterOption, set.coresOption, set.mcOption, ## set.VerboseOption, set.ZeroPolicyOption library(tidyverse) library(tmap) library(sf) library(dplyr) library(MASS) dfj&lt;-data.frame( Borough=c(&quot;Bronx&quot;,&quot;Brooklyn&quot;,&quot;Manhattan&quot;,&quot;Queens&quot;,&quot;Staten Island&quot;), Pop=c(1432132,2582830,1628701,2278906,476179), Age65=c(12.8,13.9,16.5,15.7,16.2), White=c(25.1,46.6,59.2,39.6,75.1), Hispanic=c(56.4,19.1,25.9,28.1,18.7), Afro.American=c(38.3,33.5,16.9,19.9,11.5), Asian=c(4.6,13.4,14,27.5,11), Others=c(36.8,10.4,15.4,17,5.2), Income=c(38467,61220,85066,69320,82166), Beds=c(336,214,534,144,234), COVIDtest=c(4599,2970,2844,3800,5603), COVIDhosp=c(634,400,331,560,370), COVIDdeath=c(224,181,122,200,143), COVIDdeathlab=c(173,132,91,154,117), Diabetes=c(16,27,15,22,25), Obesity=c(32,12,8,11,8), Hypertension=c(36,29,23,28,25)) %&gt;% #reverse prevalence per 100000 to raw mutate(Age65raw=round(Age65/100*Pop,0), Bedsraw=round(Beds/100000*Pop,0), COVIDtestraw=round(COVIDtest/100000*Pop,0), COVIDhospraw=round(COVIDhosp/100000*Pop,0), COVIDdeathraw=round(COVIDdeath/100000*Pop),0) #Expected rate&lt;-sum(dfj$COVIDdeathraw)/sum(dfj$Pop) dfj$Expected&lt;-with(dfj, Pop*rate ) #SMR standardised mortality ratio dfj$SMR&lt;-with(dfj, COVIDdeathraw/Expected) #NY Shape file - see this file open from above chunk NYsf&lt;-st_read(&quot;./Data-Use/Borough_Boundaries/geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp&quot;) ## Reading layer `geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\Borough_Boundaries\\geo_export_7d3b2726-20d8-4aa4-a41f-24ba74eb6bc0.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 5 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -74.25559 ymin: 40.49612 xmax: -73.70001 ymax: 40.91553 ## Geodetic CRS: WGS84(DD) #join dataset NYsf&lt;-left_join(NYsf, dfj,by=c(&quot;boro_name&quot;=&quot;Borough&quot;)) #contiguity based neighbourhood NY.nb&lt;-poly2nb(NYsf) is.symmetric.nb(NY.nb) # TRUE ## [1] TRUE #NY subway NYsubline&lt;-st_read(&quot;./Data-Use/NYsubways/geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp&quot;) ## Reading layer `geo_export_147781bc-e472-4c12-8cd2-5f9859f90706&#39; from data source ## `C:\\Users\\phant\\Documents\\Book\\HealthcareRbook\\Data-Use\\NYsubways\\geo_export_147781bc-e472-4c12-8cd2-5f9859f90706.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 742 features and 6 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -74.03088 ymin: 40.57559 xmax: -73.75541 ymax: 40.90312 ## Geodetic CRS: WGS84(DD) #raw data tm_shape(NYsf) + tm_polygons(col=&#39;SMR&#39;,title=&#39;COVID raw&#39;) + tm_shape(NYsubline)+tm_lines(col=&#39;name&#39;) The standardized mortality ratio (ratio of mortality divided by expected value for each borough) for the boroughs were: Bronx (1.245), Brooklyn (1.006), Manhattan (0.678) Queens (1.111) and Staten Island (0.794). The Figure shows a strong relationship between standardized mortality ratio and Income (R2=0.816). #plot regression lines linear vs robust linear ggplot(data=NYsf,aes(x=Income,y=COVIDdeath)) + geom_point() + geom_smooth(method=&#39;lm&#39;,col=&#39;darkblue&#39;,fill=&#39;blue&#39;) + geom_smooth(method=&#39;rlm&#39;,col=&#39;darkred&#39;,fill=&#39;red&#39;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Standardized mortality ratio and Income among different ethnic groups in Boroughs of New York. #varies by ethinicty dfj_long&lt;-pivot_longer(data=dfj, names_to = &quot;Ethnicity&quot;, values_to = &quot;Popsize&quot;,cols=c(White,Afro.American,Asian,Hispanic,Others)) commonplot&lt;-list( scale_size_continuous(name = &quot;Population size&quot;),xlab(&quot;Income (Thousand)&quot;),facet_wrap(~Ethnicity,nrow=2) ) ggplot(data=dfj_long, mapping=aes(x=Income/1000, y=SMR, size=Popsize,colour=Borough))+geom_point()+commonplot Robust regression to obtain residual for plotting with thematic maps. #robust linear models NYsf$resids &lt;- rlm(COVIDdeathraw~Pop+Age65raw,data=NYsf)$res #tmap robust linear model-residual #plot using color blind argument par(mfrow=c(2,1)) tm_shape(NYsf) + tm_polygons(col=&#39;resids&#39;,title=&#39;Residuals&#39;) ## Variable(s) &quot;resids&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. tm_shape(NYsf) + tm_polygons(col=&#39;resids&#39;,title=&#39;Residuals&#39;)+ tm_style(&quot;col_blind&quot;) ## Variable(s) &quot;resids&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. #create spatial weights for neighbour lists r.id&lt;-attr(NYsf,&quot;region.id&quot;) lw &lt;- nb2listw(NY.nb,zero.policy = TRUE) #W=row standardised In this example, the Moran’s I test suggest that the null test can be rejected. This implies random distribution of stroke across the regions of Denmark. #globaltest spatial autocorrelation using Moran I test from spdep gm&lt;-moran.test(NYsf$SMR,listw = lw , na.action = na.omit, zero.policy = T) gm ## ## Moran I test under randomisation ## ## data: NYsf$SMR ## weights: lw n reduced by no-neighbour observations ## ## ## Moran I statistic standard deviate = 0.12086, p-value = 0.4519 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.31010080 -0.33333333 0.03694814 There’s no evidence of spatial autocorrelation at local level. Note that this data is too small and may not be the ideal to evaluate local Moran’s. #local test of autocorrelation lm&lt;-localmoran(NYsf$SMR,listw = nb2listw(NY.nb, zero.policy = TRUE, style = &quot;C&quot;) , na.action = na.omit, zero.policy = T) lm ## Ii E.Ii Var.Ii Z.Ii Pr(z != E(Ii)) ## 1 -0.37737978 -0.362800450 0.264360322 -0.02835563 0.9773785 ## 2 0.00000000 0.000000000 0.000000000 NaN NaN ## 3 -0.05282102 -0.007107618 0.009392627 -0.47168273 0.6371533 ## 4 0.03765408 -0.147207581 0.086099458 0.63000884 0.5286888 ## 5 -1.25295769 -0.588823897 0.199930608 -1.48530599 0.1374628 ## attr(,&quot;call&quot;) ## localmoran(x = NYsf$SMR, listw = nb2listw(NY.nb, zero.policy = TRUE, ## style = &quot;C&quot;), zero.policy = T, na.action = na.omit) ## attr(,&quot;class&quot;) ## [1] &quot;localmoran&quot; &quot;matrix&quot; &quot;array&quot; ## attr(,&quot;quadr&quot;) ## mean median pysal ## 1 High-Low High-Low High-Low ## 2 Low-Low Low-Low Low-Low ## 3 High-Low Low-Low High-Low ## 4 High-High High-High High-High ## 5 Low-High Low-High Low-High 11.4.2 Danish Stroke Registry We now return to spatial regression with the data from Danish stroke registry described above. First we will calculate the SMR #Expected rate&lt;-sum(DKnuts2_sf2$strokenum)/sum(DKnuts2_sf2$pop) DKnuts2_sf2$Expected&lt;-with(DKnuts2_sf2, pop*rate ) #SMR standardised mortality ratio DKnuts2_sf2$SMR&lt;-with(DKnuts2_sf2, strokenum/Expected) tm_shape(DKnuts2_sf2) + tm_polygons(col=&#39;SMR&#39;,title=&#39;Stroke&#39;) Now we plot the neighborhood weight to see if the Zealand island is linked to Jutland peninsula. The data is in the .nb file. It looks like we may need to recalculate the neighborhood as there are bridges between Zealand and Zealand. This is different situation from Staten Island and the other boroughs of New York. #contiguity based neighbourhood DKnut2.nb&lt;-poly2nb(DKnuts2_sf2) DKnut2.nb ## Neighbour list object: ## Number of regions: 5 ## Number of nonzero links: 6 ## Percentage nonzero weights: 24 ## Average number of links: 1.2 is.symmetric.nb(DKnut2.nb) ## [1] TRUE Plotting in base R with sf object requires extracting the geometry and coordinates. Need to modify the link between Zealand and Jutland. plot(st_geometry(DKnuts2_sf2)) plot(DKnut2.nb,coords=st_coordinates(st_centroid(st_geometry(DKnuts2_sf2))), add=TRUE,pch=16,col=&#39;darkred&#39;) The solution is provided in stack overflow. https://gis.stackexchange.com/questions/413159/how-to-assign-a-neighbour-status-to-unlinked-polygons DKconnect &lt;- function(polys, nb, distance=&quot;centroid&quot;){ if(distance == &quot;centroid&quot;){ coords = sf::st_coordinates(sf::st_centroid(sf::st_geometry(polys))) dmat = as.matrix(dist(coords)) }else if(distance == &quot;polygon&quot;){ dmat = sf::st_distance(polys) + 1 # offset for adjacencies diag(dmat) = 0 # no self-intersections }else{ stop(&quot;Unknown distance method&quot;) } gfull = igraph::graph.adjacency(dmat, weighted=TRUE, mode=&quot;undirected&quot;) gmst = igraph::mst(gfull) edgemat = as.matrix(igraph::as_adj(gmst)) edgelistw = spdep::mat2listw(edgemat) edgenb = edgelistw$neighbour attr(edgenb,&quot;region.id&quot;) = attr(nb, &quot;region.id&quot;) allnb = spdep::union.nb(nb, edgenb) allnb } #run function DKnut2_connected.nb = DKconnect(DKnuts2_sf2,DKnut2.nb) ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; ## Found more than one class &quot;dist&quot; in cache; using the first, from namespace &#39;BiocGenerics&#39; ## Also defined by &#39;spam&#39; ## Warning in spdep::mat2listw(edgemat): style is M (missing); style should be set to a ## valid value plot(st_geometry(DKnuts2_sf2)) plot(DKnut2_connected.nb, coords=st_coordinates(st_centroid(st_geometry(DKnuts2_sf2))), add=TRUE,pch=16,col=&#39;darkred&#39;) #create spatial weights for neighbour lists r.id&lt;-attr(DKnuts2_sf2,&quot;id&quot;) lw &lt;- nb2listw(DKnut2_connected.nb,zero.policy = TRUE) #W=row standardised Perform robust regression #robust linear models DKnuts2_sf2$resids &lt;- MASS::rlm(SMR~maleper,data=DKnuts2_sf2)$res #tmap robust linear model-residual #plot using color blind argument tm_shape(DKnuts2_sf2) + tm_polygons(col=&#39;resids&#39;,title=&#39;Residuals&#39;)+ tm_style(&quot;col_blind&quot;) ## Variable(s) &quot;resids&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. Check for spatial autocorrelation #globaltest spatial autocorrelation using Moran I test from spdep gm&lt;-moran.test(DKnuts2_sf2$SMR,listw = lw , na.action = na.omit, zero.policy = T) gm ## ## Moran I test under randomisation ## ## data: DKnuts2_sf2$SMR ## weights: lw ## ## Moran I statistic standard deviate = -0.98413, p-value = 0.8375 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.7354200 -0.2500000 0.2432931 Local test of autocorrelation #local test of autocorrelation lm&lt;-localmoran(DKnuts2_sf2$SMR, listw = nb2listw(DKnut2_connected.nb, zero.policy = TRUE, style = &quot;C&quot;) , na.action = na.omit, zero.policy = T) lm ## Ii E.Ii Var.Ii Z.Ii Pr(z != E(Ii)) ## 1 -0.4777173 -0.20238585 0.4276556 -0.4210259 0.6737362 ## 2 -0.9220960 -0.21612919 0.4418446 -1.0620611 0.2882079 ## 3 -1.3492564 -0.49175508 0.6214513 -1.0877554 0.2767031 ## 4 -0.2490245 -0.14095220 0.2605379 -0.2117284 0.8323189 ## 5 -0.1984676 -0.09276265 0.1789140 -0.2499040 0.8026616 ## attr(,&quot;call&quot;) ## localmoran(x = DKnuts2_sf2$SMR, listw = nb2listw(DKnut2_connected.nb, ## zero.policy = TRUE, style = &quot;C&quot;), zero.policy = T, na.action = na.omit) ## attr(,&quot;class&quot;) ## [1] &quot;localmoran&quot; &quot;matrix&quot; &quot;array&quot; ## attr(,&quot;quadr&quot;) ## mean median pysal ## 1 High-Low High-Low High-Low ## 2 Low-Low Low-Low Low-High ## 3 High-High High-Low High-Low ## 4 Low-High Low-High Low-High ## 5 Low-High Low-High Low-High Spatial regression with spdep. The spatial filtering removes spatial dependency for regression analysis. ##spdep &amp; spatialreg fit.ols&lt;-lm(SMR~maleper, data=DKnuts2_sf2, listw=lw,zero.policy=T, type=&quot;lag&quot;, method=&quot;spam&quot;) summary(fit.ols) SAR - Lag model fit.lag&lt;-lagsarlm(SMR~maleper, data=DKnuts2_sf2, listw=lw,zero.policy=T, type=&quot;lag&quot;, method=&quot;spam&quot;) summary(fit.lag, Nagelkerke=T) Spatial Durbin Model #spatialreg fit.durb&lt;-lagsarlm(SMR~maleper,data=DKnuts2_sf2, listw=lw,zero.policy=T, type=&quot;mixed&quot;, method=&quot;spam&quot;) summary(fit.durb, Nagelkerke=T) Spatial Durbin Error Model fit.errdurb&lt;-errorsarlm(SMR~maleper, data=DKnuts2_sf2, listw=lw,zero.policy=T,etype=&quot;emixed&quot;, method=&quot;spam&quot;) summary(fit.errdurb, Nagelkerke=T) SAC Model fit.sac&lt;-sacsarlm(SMR~maleper,data=DKnuts2_sf2, listw=lw,zero.policy=T, type=&quot;sac&quot;, method=&quot;MC&quot;) summary(fit.sac, Nagelkerke=T) spatial filtering #function from spatialreg #Set ExactEV=TRUE to use exact expectations and variances rather than the expectation and variance of Moran&#39;s I from the previous iteration, default FALSE DKFilt&lt;-SpatialFiltering(SMR~maleper, data=DKnuts2_sf2, nb=DKnut2_connected.nb, #ExactEV = TRUE, zero.policy = TRUE,style=&quot;W&quot;) 11.4.3 INLA This section uses Bayesian modeling for regression with fitting of the model by Integrated Nested Lapace Approximation (INLA). https://www.r-bloggers.com/spatial-data-analysis-with-inla/. For those wanting to analyse leukemia in New York instead of COVID-19, the dataset NY8 is available from DClusterm. INLA approximates the posterior distribution as latent Gaussian Markov random field. In this baseline analysis, the poisson model is performed without any random effect terms. library(INLA) nb2INLA(&quot;DKnut2.graph&quot;, DKnut2_connected.nb) #This create a file called ``LDN-INLA.adj&#39;&#39; with the graph for INLA DK.adj &lt;- paste(getwd(),&quot;/DK.graph&quot;,sep=&quot;&quot;) #Poisson model with no random latent effect-ideal baseline model m1&lt;-inla(SMR~ 1+maleper, data=DKnuts2_sf2, family=&quot;poisson&quot;, E=DKnuts2_sf2$Expected,control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE), verbose = T ) R1&lt;-summary(m1) In this next analysis, the Poisson model was repeated with random effect terms. This step was facilitated by adding the index term. #Poisson model with random effect #index to identify random effect ID DKnuts2_sf2$ID &lt;- 1:nrow(DKnuts2_sf2) m2&lt;-inla(SMR~ 1+ maleper +f(ID, model = &quot;iid&quot;), data=DKnuts2_sf2, family=&quot;poisson&quot;, E=DKnuts2_sf2$Expected,control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE) ) R2&lt;-summary(m2) DKnut2_sf2$FIXED.EFF &lt;- m1$summary.fitted[, &quot;mean&quot;] DKnut2_sf2$IID.EFF &lt;- m2$summary.fitted[, &quot;mean&quot;] #plot regression on map tSMR&lt;-tm_shape(DKnuts2_sf2)+tm_polygons(&quot;SMR&quot;) tFIXED&lt;-tm_shape(DKnuts2_sf2)+tm_polygons(&quot;FIXED.EFF&quot;) tIID&lt;-tm_shape(DKnuts2_sf2)+tm_polygons(&quot;IID.EFF&quot;) This next paragraph involves the use of spatial random effects in regression models. Examples include conditional autoregressive (CAR) and intrinsic CAR (ICAR). # Create sparse adjacency matrix DK.mat &lt;- as(nb2mat(DKnut2_connected.nb, style = &quot;B&quot;,zero.policy = TRUE),&quot;Matrix&quot;) #S=variance stabilise # Fit model m.icar &lt;- inla(SMR ~ 1+maleper+ f(ID, model = &quot;besag&quot;, graph = DK.mat), data = DKnuts2_sf2, E = DKnuts2_sf2$Expected, family =&quot;poisson&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE)) R3&lt;-summary(m.icar) The Besag-York-Mollie (BYM) now accounts for spatial dependency of neighbours. It includes random effect from ICA and index. m.bym = inla(SMR ~ -1+ maleper+ f(ID, model = &quot;bym&quot;, graph = DK.mat), data = DKnuts2_sf2, E = DKnuts2_sf2$Expected, family =&quot;poisson&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE)) R4&lt;-summary(m.bym) ICARmatrix &lt;- Diagonal(nrow(DK.mat), apply(DK.mat, 1, sum)) - DK.mat Cmatrix &lt;- Diagonal(nrow(DKnuts2_sf2), 1) - ICARmatrix max(eigen(Cmatrix)$values) m.ler = inla(SMR ~ -1+maleper+ f(ID, model = &quot;generic1&quot;, Cmatrix = Cmatrix), data = DKnuts2_sf2, E = DKnuts2_sf2$Expected, family =&quot;poisson&quot;, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE)) R5&lt;-summary(m.ler) Spatial econometric model usch as spatial lag model includes covariates and autoregres on the response variable. #X mmatrix &lt;- model.matrix(SMR ~ 1, DKnuts2_sf2) #W W &lt;- as(nb2mat(DKnut2_connected.nb, style = &quot;W&quot;, zero.policy = TRUE), &quot;Matrix&quot;) #Q Q.beta = Diagonal(n = ncol(mmatrix), x = 0.001) #Range of rho rho.min&lt;- -1 rho.max&lt;- 1 #Arguments for &#39;slm&#39; args.slm = list( rho.min = rho.min , rho.max = rho.max, W = W, X = mmatrix, Q.beta = Q.beta ) #Prior on rho hyper.slm = list( prec = list( prior = &quot;loggamma&quot;, param = c(0.01, 0.01)), rho = list(initial=0, prior = &quot;logitbeta&quot;, param = c(1,1)) ) #SLM model m.slm &lt;- inla( SMR ~ -1+maleper+ f(ID, model = &quot;slm&quot;, args.slm = args.slm, hyper = hyper.slm), data = DKnuts2_sf2, family = &quot;poisson&quot;, E = DKnuts2_sf2$Expected, control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE, waic = TRUE) ) R6&lt;-summary(m.slm) marg.rho.internal &lt;- m.slm$marginals.hyperpar[[&quot;Rho for ID&quot;]] marg.rho &lt;- inla.tmarginal( function(x) { rho.min + x * (rho.max - rho.min) }, marg.rho.internal) inla.zmarginal(marg.rho, FALSE) plot(marg.rho, type = &quot;l&quot;, main = &quot;Spatial autocorrelation&quot;) Spatial model selection DKnut2_sf2$ICAR &lt;- m.icar$summary.fitted.values[, &quot;mean&quot;] DKnut2_sf2$BYM &lt;- m.bym$summary.fitted.values[, &quot;mean&quot;] DKnut2_sf2$LEROUX &lt;- m.ler$summary.fitted.values[, &quot;mean&quot;] DKnut2_sf2$SLM &lt;- m.slm$summary.fitted.values[, &quot;mean&quot;] labels&lt;-c(&quot;Fixed&quot;,&quot;IID&quot;, &quot;ICAR&quot;,&quot;BYM&quot;,&quot;LEROUX&quot;,&quot;SLM&quot;) Marginal_Likelihood&lt;-c(R1$mlik[1],R2$mlik[1],R3$mlik[1],R4$mlik[1],R5$mlik[1], R6$mlik[1]) Marginal_Likelihood&lt;-round(Marginal_Likelihood,2) WAIC&lt;-c(R1$waic[[1]],R2$waic[[1]],R3$waic[[1]],R4$waic[[1]],R5$waic[[1]], R6$waic[[1]]) WAIC&lt;-round(WAIC,2) DIC&lt;-c(R1$dic[[1]],R2$dic[[1]],R3$dic[[1]],R4$dic[[1]],R5$dic[[1]],R6$dic[[1]]) DIC&lt;-round(DIC,2) Results&lt;-data.frame(labels,Marginal_Likelihood,WAIC,DIC) knitr::kable(Results) #plot maps tICAR&lt;-tm_shape(DKnut2_sf2)+tm_polygons(&quot;ICAR&quot;) tBYM&lt;-tm_shape(DKnut2_sf2)+tm_polygons(&quot;BYM&quot;) tLEROUX&lt;-tm_shape(DKnut2_sf2)+tm_polygons(&quot;LEROUX&quot;) tSLM&lt;-tm_shape(DKnut2_sf2)+tm_polygons(&quot;SLM&quot;) #arrange in grid using tmap arrange current.mode &lt;- tmap_mode(&quot;plot&quot;) tmap_arrange(tFIXED,tIID,tICAR,tBYM,tLEROUX,tSLM) tmap_mode(current.mode) 11.4.4 Stan library(brms) ## Loading &#39;brms&#39; package (version 2.19.0). Useful instructions ## can be found by typing help(&#39;brms&#39;). A more detailed introduction ## to the package is available through vignette(&#39;brms_overview&#39;). ## ## Attaching package: &#39;brms&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## kidney ## The following object is masked from &#39;package:dismo&#39;: ## ## kfold ## The following object is masked from &#39;package:lme4&#39;: ## ## ngrps ## The following object is masked from &#39;package:kernlab&#39;: ## ## prior ## The following objects are masked from &#39;package:lava&#39;: ## ## categorical, mixture, multinomial ## The following objects are masked from &#39;package:mgcv&#39;: ## ## s, t2 ## The following object is masked from &#39;package:tseries&#39;: ## ## arma ## The following object is masked from &#39;package:e1071&#39;: ## ## rwiener ## The following object is masked from &#39;package:terra&#39;: ## ## autocor ## The following object is masked from &#39;package:pracma&#39;: ## ## bernoulli ## The following object is masked from &#39;package:modeltools&#39;: ## ## prior ## The following object is masked from &#39;package:survival&#39;: ## ## kidney ## The following object is masked from &#39;package:stats&#39;: ## ## ar #S=variance stabilise # Fit model #brm.lag&lt;- brm (COVIDdeathraw ~ 1+Age65raw+Income+sar(NY.nb, type = &quot;lag&quot;), # data = DKnut2_sf2, data2 = list(NY.nb = NY.nb), # chains = 2, cores = 2) 11.5 Machine learning in spatial analysis Up until now we have used frequentist and Bayesian spatial regression methods. Spatial data can also be analysed using machine learning such as random forest. 11.6 Spatio-temporal regression Spatio-temporal regression combines a spatial model with a temporal model. In many cases of low disease incidene in each region it may not be possible to identify any temporal trend at a regional level. "],["app.html", "Chapter 12 App 12.1 Brief introduction to Shiny app", " Chapter 12 App There are several platforms for writing and deploying apps. Rstudio has created a library Shiny to write app. This app can be deployed by creating an account at https://shiny.rstudio.com/. 12.1 Brief introduction to Shiny app Shiny divides the app to 2 component: ui or user interface web page and server or the engine for running the app. This app is used to assist if a stroke code should be activated. It uses the reactive call within server to switch between activating and deactivating code stroke. library(shiny) # Define UI for application ui &lt;- bootstrapPage( titlePanel(&quot;ST6-code stroke prototype app&quot;), wellPanel( helpText(&quot;This app helps you decide if you should call a code stroke. it is designed for use at a teaching hospital and may not apply for your specific need. Please check with your local hospital.&quot;), #stroke textInput(&#39;stroke&#39;,&quot;Is it a stroke-Please enter Yes or No&quot;,&quot;No&quot;), #walk textInput(&#39;walk&#39;,&quot;Prior to stroke, can the patient walk from one room to another without the help from another person? Walking aids such as four wheelframe and walking stick are allowed? -Please enter Yes or No&quot;,&quot;Yes&quot;), #home textInput(&#39;nursing&#39;,&quot;Is the patient from nursing home-Please enter Yes or No&quot;,&quot;Yes&quot;) ), #partition screen to 3 columns- sum to 12 column(9, tabPanel( textOutput(&quot;Hospital&quot;), textOutput(&quot;Walk&quot;)) ) ) # Define server logic server &lt;- function(input, output, session) { #stroke answer&lt;-reactive({ if(input$stroke==&quot;No&quot;) {return (&quot;Your answer is NO to the patient having signs of stroke, please do NOT call code stroke&quot;) } else { return(&quot;Please call code stroke&quot;) } }) output$Hospital&lt;-renderText({return(answer())}) #walk answer1&lt;-reactive({ if(input$stroke==&quot;No&quot;) {return (&quot;Your answer is NO to the patient having signs of stroke, please do NOT call code stroke&quot;) } else { if(input$walk==&quot;No&quot;) { return(&quot;Your answer is No, indicating that premorbidly the patient is unable to walk unaided, please do NOT call code stroke&quot;) } else { if(input$nursing==&quot;Yes&quot;) { return(&quot;Your answer is Yes to the patient living in nursing home, please do NOT call code stroke&quot;) } } return(&quot;Please call code stroke&quot;) } }) output$Walk&lt;-renderText({return(answer1())}) } # Run the application shinyApp(ui = ui, server = server) ## ## Listening on http://127.0.0.1:5816 Other available apps can be found at https://gntem3.shinyapps.io/ambmc. It was designed to explore deployment of mobile stroke unit (ambulance equipped with CT scanner for stroke treatment) in Melbourne (Phan, Beare, et al. 2019). The app illustrates the use of slide ruler to enable user interaction. A picture can be inserted into the app by placing the picture inside sub-folder www. A web page can be inserted using html coding. Plotly object can be rendered wihtin shiny using the call renderPlotly. Leaflet map object can be rendered using the call renderLeaflet. References "],["appendix.html", "Chapter 13 Appendix 13.1 Brief introduction to Matrix 13.2 Regression", " Chapter 13 Appendix 13.1 Brief introduction to Matrix Eigenvector (characteristic vector) and eigenvalue (characteristic value) are useful in solving system of linear equations and providing quicker solution to solving task such as finding the nth power of a matrix. The eigenvector v of a n x n square matrix M is defined as a non-zero vector of M such that the product of Mev is equal to the product of the scalar eigenvalue λ and v. Eigenvector analysis can be used to describe pattern. Here, the eigenvector has an interpretation in term of the direction of the data and the eigenvalue provides a scaling measure of the length or change in direction of the vector (when both are multiplied). Using the description above regarding finding the nth power of a matrix M, the eigenvectors remain unchanged but the eigenvalues change in proportion to the nth power of M. In terms of applications, eigen decomposition approach provides a non-bias method for discovering pattern in complex data; this is typically performed by eigen decomposition of the covariance matrix. This approach had been used in describing pattern of infarct in hypoxic ischaemic injury780, facial recognition. In the section on regression, we can show that the eigenvalue can be interpreted in term of the variance of data. Eigenvalue can be described I is the n x n identity matrix In geometric term, the eigenvalue describes the length of its associated eigenvector and in regression analyss, it can be considered as the variance of the data. Eigenvalue is calculated by finding the solution for the root of characteristic polynomial equation. For a 2 x 2 matrix, The characteristic polynomial is given by The Trace(M) is given by The determinant is given by The quadratic formula can be used to solve for λ. In this case, the eigenvalue is -1 and 3. The eigenvector of M is given by The identity matrix consist of ones on the diagnonal and zero elsewhere. For a 2 x 2 matrix, the identity matrix is For the eigenvalue -1, its eigenvector is given by The eigenvectors include (1, 1) . For the eigenvalue 3, its associated eigenctor include (1, -1). The solution for the characteristic equation becomes more complex as the size of the matrix increase. For large matrices, the power method is used to derive the eigenvalue. The determinant is equal to the product if the eigenvalues (-1 x 3= -3). The trace is equal to the sum of the eigenvalues (-1+3=2) or the sum of the diagonal of the matrix (1+1 =2). A matrix is invertible (non-singular) if it satisfies the following The matrix M-1 is the inverse of matrix M. The inverse of M is calculated as follow. w=1, x=0, y=1,z=0 Matrix which cannot be inverted is termed singular. The determinant of a square matrix is zero. The significance of invertible matrix will be seen in the section on collinearity when dealing with regression analsysis. Sparse matrix is a matrix populated mostly by zeros. In a network sense it implies lack of cohesion in the network. Inverting sparse matrix is challenging due to the reduced rank associate with this type of matrix. The solutions require various form of penalisation (penalised regression is discuss next under Regression). The null space of a matrix can be considered as the solutions to homogenous system of linear equations. The null space of a matrix is also termed the kernel of that matrix (3, -9, 1). As shown above, the null space of a matrix contains a zero vector. In other word the coefficient matrix is zeros. The augemented matrix is displayed on the left. The null space is a subspace of this matrix. The rank of matrix of a matrix can be considered as a measure of the ‘non-singularness’ of a matrix. For example, a 3 x 4 matrix with 2 independent rows has rank of 2. In other word, it describes the number of independent columns of a matrix or its eigenvector. It also describes the dimension of the image of the linear transformation that is performed on the matrix. Often the expression that the matrix is full rank (contains independent rows and columns of data) is used in regression to infer that the matrix is invertible and the data are non-collinear. An example of zero rank or collinear matrix is shown here Observe that the second column is three times the first column. The determinant of this matrix is 3 x 6 – 2 x 9=0. This matrix is not invertible and rank deficient. The interpretation is that the rank of the augmented matrix is equal or to that for the coefficient matrix, the solution to system of linear equation is stable. If the rank of an augmented matrix is larger than the coefficient matrix, the matrix is rank deficient.mA special form of multivariate regression use the rank of matrix in regression (reduced rank regression). This type of regression is useful in the case of of minority class distribution where the majority of the data are at one end of the spectrum. A positive semi-definite matrix is invertible and has full rank. It is defined as a matrix which can be obtained by the multiplication of a matrix and its transpose (denoted by T in upper case). Such a matrix is symmetrical. Examples include correlation and covariance matrices. 13.2 Regression In the section below, a matrix approach to regression is provided. This step was performed as it explains for issues encountered when doing regression analyses. It is easier to explain collinearity using matrix. Examples of performing regression analyses and steps required to check for errors are given. This section deals with univariable and multivariable analyses. The next chapter will discuss the different multivariate analyses. 13.2.1 Linear regression A matrix description of parameter estimation is given here because it is easier to describe the multiple regression (Smith 1998). \\(Y= \\beta_1X_1+ \\beta_2X_2+...\\beta_jX_j\\) A matrix is an array of data as rows and columns. For the imaging data, the individual voxel is represented on each column and each row refers to another patient. A vector refers to a matrix of one column. In matrix form, the multiple regression equation takes the form \\(Y= \\beta_j X + E\\) where X is the predictor matrix, Y is the dependent matrix, β is the regression coefficient and E is the error term (not the intercept). The X is a \\(j^{th}\\) rows by ith columns matrix and Y and E is a \\(j^{th}\\) column vector. Algebraic manipulation of equation 1, shows that the solution for β is \\((X^TX)^{-1}X^TY\\). \\(X^T\\) is the transpose of X such that the columns of are now written as rows. The correlation matrix of X is given by \\(X^TX\\). The solution for β is possible if the correlation matrix \\(X^TX\\) can be inverted. The inverse of a matrix can be found if it is has a square shape (columns and rows of the matrix are equal) and the determinant of the matrix is not singular or nonzero (Smith 1998). The uniqueness of the matrix inverse is that when a matrix is multiplied by its inverse, the solution is an identity matrix. The diagonal elements of a matrix are ones and the remainders of the square matrix are zeros for an identity matrix. For a rectangular matrix, multiplication of the matrix by its Moore-Penrose pseudo-inverse results in an identity matrix. The terms in the inverse of \\(X^TX\\) are divided by the determinants of \\(X^TX\\) . For simplicity, the determinant of a 2 x 2 matrix is given by \\(ad-bc\\) for a matrix A. \\(A=\\left[\\begin{array}{cc}a &amp; b\\\\c &amp; d\\end{array}\\right]\\) The inverse of this matrix A is given by \\(\\frac{1}{ad-bc}\\left[\\begin{array}{cc}d &amp; -b\\\\-c &amp; a\\end{array}\\right]\\) From this equation, it can be seen that the determinant and the inverse exist if the result is nonzero. For a correlation matrix \\(X^TX\\) of the form \\(\\left[\\begin{array}{cc}n &amp; nX_*\\\\nX_* &amp; nX_*^2\\end{array}\\right]\\) then the determinant is zero. \\(n \\times nX_*^2 -nX_* \\times nX_*\\) Hence, there is no unique solution for this equation. If near collinearity exists and the determinant approach zero, there are infinite possible combinations that can result in a least squares estimate of the parameter. In this example, the matrix is singular then the columns of X are likely to be linearly related to each other (collinearity). In this case, the regression coefficient is unstable with the variance of the regression coefficient large. Further, small changes in the dependent variables lead to fluctuations in the regression solution. 13.2.1.1 Leasts squares The least squares solution for the parameter β refers to the fitting of the line between the intercept and the variables of X such that the Euclidian distance between the observed variables Y and expected or predicted variables are as small as possible. The metric for the fit is the sum of squared errors SSE (or residual mean square error/residual sum of squares) and is given by \\(SSE=\\sum(Y-\\beta X)^T(Y-\\beta X)\\) The variance-covariance matrix of \\(\\beta\\) is given by \\(Var(\\beta)=\\delta^2(X^TX)\\) 13.2.1.2 Collinearity Collinearity or relatedness among the predictors is often forgotten in many analysis. This issue can lead to instability in the regression coefficients. There are several tests for collinearity: variance inflation factor and condition index. The variance inflation factor (VIF) is proportional to \\(VIF = 1/1-R^2\\). In this example, as the predictors become strogly correlated \\(R^2\\) apporaches 1 and VIF will approaches infintity. Collinearity is present if VIF &gt;10(Kleinbaum, Kupper, and Muller 1978). Collinearity can also be assessed by measuring the condition index (Phan, Donnan, Koga, et al. 2006). This can be given as ratio between the largest and the corresponding eignvalue \\(CI_i=\\sqrt(\\frac{\\lamda_{max}{\\lambda_i})\\). Collinearity is present when the condition index is &gt;30(Kleinbaum, Kupper, and Muller 1978). 13.2.1.3 weighted least squares In the section above, it was not stated explicitly but the least squares regression model is appropriate when the variances of the predictor variables are uniform (Smith 1998). In this case the variance of the error matrix is a diagonal matrix with equal diagonal elements. When there are unreliable data or errors in measurement of some of the data, the variance of the error matrix contains unequal diagonal elements. The consequence is that the least squares formula leads to instability of the parameter estimate. Weighted least squares regression is similar to least squares regression except that the variance matrix is weighted \\(w\\) by the variance of the columns of the predictor variables. \\(\\beta=(X^TV^-1)^-1X^TV^-1Y\\).The diagonal matrix \\(V\\) contains weights expressed as 1/w along the diagonal elements. These weights are used to down play the importance of the regions where noise occurs and gives appropriate importance to the true data region. The result is a reduction in the variance of the regression coefficients and hence stability in their estimate. Weighted least squares is introduced here because the weighted PLS is used in the PLS-PLR model. 13.2.2 Logistic regression In logistic regression, there is no algebraic solution to determine the parameter estimate (β coefficient) and a numerical method (trial and error approach) such as maximum likelihood estimate is used to determine the parameter estimate. The General Linear Model \\(Y=\\beta X\\) is modified to the form of the Generalised Linear Model (GLIM) by adding a non-linear link function \\(g(\\mu)\\) . The equation now resembles \\(Y=g(\\beta X)\\). Consider the binary response (1, 0) as proportions of the predictor variables. is the probability of an event and is the probability of an event not occurring. The odds ratio \\(OR\\) is given by \\(OR=\\frac{1}{1-p}\\). A logit transformation take the form \\(\\n=logit_i(p_i)=ln(\\frac{p_i}{1-p_i})=\\sum(X_ij\\beta_i)\\). The logistic equation takes the form \\(p_i=\\frac{e^{X_j\\beta_j}}{1-e^{X_j\\beta_j}}\\) References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
